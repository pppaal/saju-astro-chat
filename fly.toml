# fly.toml app configuration file
# See https://fly.io/docs/reference/configuration/ for documentation

app = "backend-ai"
primary_region = "nrt" # Tokyo - closest to Korea

[build]
  dockerfile = "Dockerfile"

[env]
  PORT = "8080"
  OMP_NUM_THREADS = "2"
  MKL_NUM_THREADS = "2"
  RAG_CPU_THREADS = "2"
  PYTHONUNBUFFERED = "1"
  APP_MODULE = "main:app"
  # Gunicorn settings optimized for AI workloads
  GUNICORN_WORKERS = "1"
  GUNICORN_THREADS = "4"
  GUNICORN_TIMEOUT = "120"
  GUNICORN_GRACEFUL_TIMEOUT = "30"
  GUNICORN_KEEPALIVE = "5"
  GUNICORN_WORKER_CLASS = "gthread"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ["app"]

  # Health check - optimized for AI workload startup
  [[http_service.checks]]
    grace_period = "60s"      # Give app time to start and load models
    interval = "15s"          # Check every 15s (balanced)
    method = "GET"
    timeout = "5s"
    path = "/health"

[[services]]
  protocol = "tcp"
  internal_port = 8080

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]

  # Concurrency settings - requests-based is better for AI workloads
  [services.concurrency]
    type = "requests"
    hard_limit = 50           # Max concurrent requests per machine
    soft_limit = 40           # Start scaling when this is reached

# VM resources - optimized for high-performance AI workloads
[[vm]]
  size = "shared-cpu-2x"      # 2 shared CPUs for true parallelism
  memory = "4096mb"           # 4GB RAM for dual workers + AI models

# Auto-scaling configuration
[deploy]
  strategy = "rolling"        # Zero-downtime deploys

# Metrics endpoint for monitoring
[metrics]
  port = 9091
  path = "/metrics"
