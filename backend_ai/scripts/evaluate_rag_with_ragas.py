#!/usr/bin/env python3
"""
RAGAS í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ ì •ëŸ‰ í‰ê°€

Phase 5: Before/After ë¹„êµ ë²¤ì¹˜ë§ˆí¬

Metrics:
- Faithfulness: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•œ ë‹µë³€ì¸ê°€?
- Answer Relevancy: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‹µë³€ì¸ê°€?
- Context Recall: í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ê²€ìƒ‰í–ˆëŠ”ê°€?
- Context Precision: ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ê´€ë ¨ ìˆëŠ” ë¹„ìœ¨

Usage:
    # Before (legacy PyTorch O(n))
    USE_CHROMADB=0 python -m scripts.evaluate_rag_with_ragas

    # After (ChromaDB + PageRank)
    USE_CHROMADB=1 python -m scripts.evaluate_rag_with_ragas

    # ë°ì´í„°ì…‹ ìƒì„±ë§Œ
    python -m scripts.evaluate_rag_with_ragas --generate-only
"""

import argparse
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
_PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, _PROJECT_ROOT)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

# RAGAS import (optional - ì„¤ì¹˜ë˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ)
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    logger.warning("RAGAS not installed. Install with: pip install ragas datasets")
    RAGAS_AVAILABLE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í‰ê°€ ë°ì´í„°ì…‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EVALUATION_DATASET = [
    {
        "question": "ê°‘ëª© ì¼ê°„ì˜ ì„±ê²©ì€?",
        "ground_truth": "ê°‘ëª©ì€ í° ë‚˜ë¬´ë¥¼ ìƒì§•í•˜ë©°, ê³§ê³  ê°•ì§í•œ ì„±ê²©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ì§€ê°€ ê°•í•˜ê³  ë¦¬ë”ì‹­ì´ ìˆìœ¼ë‚˜, ìœµí†µì„±ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "contexts_required": ["ê°‘ëª©", "ì¼ê°„", "ì„±ê²©", "ëª©"],
    },
    {
        "question": "ì„ìˆ˜ì™€ ê°‘ëª©ì˜ ê¶í•©ì€?",
        "ground_truth": "ì„ìˆ˜ì™€ ê°‘ëª©ì€ ìˆ˜ìƒëª©(æ°´ç”Ÿæœ¨) ìƒìƒ ê´€ê³„ë¡œ ë§¤ìš° ì¢‹ì€ ê¶í•©ì…ë‹ˆë‹¤. ì„ìˆ˜ê°€ ê°‘ëª©ì„ í‚¤ì›Œì£¼ëŠ” ê´€ê³„ì…ë‹ˆë‹¤.",
        "contexts_required": ["ì„ìˆ˜", "ê°‘ëª©", "ìˆ˜ìƒëª©", "ìƒìƒ", "ê¶í•©"],
    },
    {
        "question": "ë³‘í™”ê°€ ê°•í•œ ì‚¬ëŒì˜ íŠ¹ì§•ì€?",
        "ground_truth": "ë³‘í™”ëŠ” íƒœì–‘ì„ ìƒì§•í•˜ë©°, ë°ê³  í™œë™ì ì´ë©° ì™¸í–¥ì ì¸ ì„±ê²©ì„ ê°€ì§‘ë‹ˆë‹¤. ë¦¬ë”ì‹­ì´ ê°•í•˜ê³  ì°½ì˜ì ì´ë‚˜, ì¶©ë™ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "contexts_required": ["ë³‘í™”", "íƒœì–‘", "ì„±ê²©", "í™”"],
    },
    {
        "question": "ëª©ì„±ì´ ì‚¬ìˆ˜ìë¦¬ì— ìˆìœ¼ë©´?",
        "ground_truth": "ëª©ì„±ì´ ì‚¬ìˆ˜ìë¦¬ì— ìœ„ì¹˜í•˜ë©´ í™•ì¥, ì„±ì¥, ì² í•™ì  íƒêµ¬ê°€ ê°•ì¡°ë©ë‹ˆë‹¤. í•´ì™¸ ì—¬í–‰, ê³ ë“± êµìœ¡, ì¢…êµ/ì² í•™ì— ê´€ì‹¬ì´ ë§ìŠµë‹ˆë‹¤.",
        "contexts_required": ["ëª©ì„±", "ì‚¬ìˆ˜ìë¦¬", "í™•ì¥", "ì² í•™"],
    },
    {
        "question": "ì‹ì‹ ì´ ë§ìœ¼ë©´ ì–´ë–¤ ì„±ê²©ì¸ê°€?",
        "ground_truth": "ì‹ì‹ ì´ ë§ìœ¼ë©´ í‘œí˜„ë ¥ì´ í’ë¶€í•˜ê³  ì°½ì˜ì ì´ë©°, ì—¬ìœ ë¡­ê³  ë‚™ì²œì ì¸ ì„±ê²©ì„ ê°€ì§‘ë‹ˆë‹¤. ì˜ˆìˆ ì´ë‚˜ ìš”ë¦¬ì— ì¬ëŠ¥ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "contexts_required": ["ì‹ì‹ ", "ì„±ê²©", "ì°½ì˜", "í‘œí˜„"],
    },
    {
        "question": "ê¸ˆìˆ˜ìƒìƒì˜ ì˜ë¯¸ëŠ”?",
        "ground_truth": "ê¸ˆìƒìˆ˜(é‡‘ç”Ÿæ°´)ëŠ” ê¸ˆ(é‡‘)ì´ ìˆ˜(æ°´)ë¥¼ ìƒí•˜ëŠ” ìƒìƒ ê´€ê³„ì…ë‹ˆë‹¤. ê¸ˆì†ì´ ë…¹ì•„ ë¬¼ì´ ë˜ë“¯, ê¸ˆì´ ìˆ˜ë¥¼ í‚¤ì›Œì¤ë‹ˆë‹¤.",
        "contexts_required": ["ê¸ˆ", "ìˆ˜", "ìƒìƒ", "ê¸ˆìƒìˆ˜", "ì˜¤í–‰"],
    },
    {
        "question": "íƒœì–‘ê³¼ ë‹¬ì˜ ì¡°í™”ëŠ”?",
        "ground_truth": "íƒœì–‘(Sun)ì€ ìì•„ì™€ ì˜ì‹, ë‹¬(Moon)ì€ ê°ì •ê³¼ ë¬´ì˜ì‹ì„ ìƒì§•í•©ë‹ˆë‹¤. ë‘ í–‰ì„±ì˜ ì¡°í™”ëŠ” ì´ì„±ê³¼ ê°ì„±ì˜ ê· í˜•ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
        "contexts_required": ["íƒœì–‘", "ë‹¬", "sun", "moon", "ì¡°í™”"],
    },
    {
        "question": "í¸ê´€ì´ ê°•í•˜ë©´?",
        "ground_truth": "í¸ê´€ì´ ê°•í•˜ë©´ ì±…ì„ê°ê³¼ í†µì†”ë ¥ì´ ê°•í•˜ë‚˜, ê¶Œìœ„ì ì´ê³  ë…ë‹¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ì¥ ìƒí™œì—ì„œ ë¦¬ë” ì—­í• ì„ ë§¡ê²Œ ë©ë‹ˆë‹¤.",
        "contexts_required": ["í¸ê´€", "ì„±ê²©", "ë¦¬ë”ì‹­", "ì‹­ì„±"],
    },
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG ì‹œìŠ¤í…œ ì¿¼ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def query_rag_system(question: str, use_chromadb: bool = False) -> Dict:
    """RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ + ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜."""
    try:
        from app.saju_astro_rag import GraphRAG

        graph_rag = GraphRAG()

        # ì§ˆë¬¸ì„ facts dictë¡œ ë³€í™˜
        facts = {"query": question, "type": "general"}

        result = graph_rag.query(facts, top_k=8, domain_priority="saju")

        contexts = result.get("matched_nodes", [])
        context_text = result.get("context_text", "")

        # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ)
        answer = f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:\n{context_text[:500]}"

        return {
            "answer": answer,
            "contexts": contexts,
            "backend": result.get("stats", {}).get("backend", "legacy"),
        }

    except Exception as e:
        logger.error(f"RAG ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
        return {"answer": "", "contexts": [], "backend": "error"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_evaluation_data(use_chromadb: bool = False) -> List[Dict]:
    """í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± (RAG ì‹œìŠ¤í…œ ì‹¤ì œ ì¿¼ë¦¬)."""
    logger.info("=" * 60)
    logger.info(f"í‰ê°€ ë°ì´í„° ìƒì„± ì‹œì‘ (USE_CHROMADB={use_chromadb})")
    logger.info("=" * 60)

    results = []

    for i, item in enumerate(EVALUATION_DATASET, 1):
        logger.info(f"[{i}/{len(EVALUATION_DATASET)}] {item['question']}")

        start = time.time()
        rag_result = query_rag_system(item["question"], use_chromadb)
        elapsed = time.time() - start

        results.append({
            "question": item["question"],
            "answer": rag_result["answer"],
            "contexts": rag_result["contexts"],
            "ground_truth": item["ground_truth"],
            "backend": rag_result["backend"],
            "latency_ms": int(elapsed * 1000),
        })

        logger.info(f"  âœ“ {len(rag_result['contexts'])} contexts, {elapsed*1000:.0f}ms, backend={rag_result['backend']}")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAGAS í‰ê°€ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_ragas_evaluation(eval_data: List[Dict]) -> Dict:
    """RAGAS í”„ë ˆì„ì›Œí¬ë¡œ í‰ê°€ ì‹¤í–‰."""
    if not RAGAS_AVAILABLE:
        logger.error("RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ragas datasets")
        return {}

    logger.info("=" * 60)
    logger.info("RAGAS í‰ê°€ ì‹œì‘")
    logger.info("=" * 60)

    # RAGAS Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    dataset_dict = {
        "question": [d["question"] for d in eval_data],
        "answer": [d["answer"] for d in eval_data],
        "contexts": [d["contexts"] for d in eval_data],
        "ground_truth": [d["ground_truth"] for d in eval_data],
    }

    dataset = Dataset.from_dict(dataset_dict)

    # í‰ê°€ ì‹¤í–‰
    result = evaluate(
        dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_recall,
            context_precision,
        ],
    )

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_comparison_report(before_data: List[Dict], after_data: List[Dict]):
    """Before/After ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±."""
    logger.info("=" * 60)
    logger.info("Before/After ë¹„êµ ë¦¬í¬íŠ¸")
    logger.info("=" * 60)

    # ë ˆì´í„´ì‹œ ë¹„êµ
    before_latency = sum(d["latency_ms"] for d in before_data) / len(before_data)
    after_latency = sum(d["latency_ms"] for d in after_data) / len(after_data)
    latency_improvement = ((before_latency - after_latency) / before_latency) * 100

    logger.info(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    logger.info(f"  Before (PyTorch O(n))  : {before_latency:.0f}ms")
    logger.info(f"  After (ChromaDB+PR)    : {after_latency:.0f}ms")
    logger.info(f"  ê°œì„ ìœ¨                 : {latency_improvement:+.1f}%")

    # ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ë¹„êµ
    before_ctx_avg = sum(len(d["contexts"]) for d in before_data) / len(before_data)
    after_ctx_avg = sum(len(d["contexts"]) for d in after_data) / len(after_data)

    logger.info(f"\nğŸ“š ê²€ìƒ‰ ê²°ê³¼:")
    logger.info(f"  Before í‰ê·  contexts  : {before_ctx_avg:.1f}")
    logger.info(f"  After í‰ê·  contexts   : {after_ctx_avg:.1f}")

    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    report_path = os.path.join(_PROJECT_ROOT, "data", "ragas_comparison_report.json")
    os.makedirs(os.path.dirname(report_path), exist_ok=True)

    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "before": {
            "latency_ms": before_latency,
            "contexts_avg": before_ctx_avg,
            "data": before_data,
        },
        "after": {
            "latency_ms": after_latency,
            "contexts_avg": after_ctx_avg,
            "data": after_data,
        },
        "improvement": {
            "latency_pct": latency_improvement,
        },
    }

    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="RAGAS RAG í‰ê°€")
    parser.add_argument("--generate-only", action="store_true", help="ë°ì´í„°ì…‹ ìƒì„±ë§Œ")
    parser.add_argument("--compare", action="store_true", help="Before/After ë¹„êµ")
    args = parser.parse_args()

    use_chromadb = os.environ.get("USE_CHROMADB", "0") == "1"

    logger.info(f"USE_CHROMADB={use_chromadb}")

    if args.compare:
        # Before/After ë¹„êµ ëª¨ë“œ
        logger.info("Before í‰ê°€ (USE_CHROMADB=0)")
        os.environ["USE_CHROMADB"] = "0"
        before_data = generate_evaluation_data(use_chromadb=False)

        logger.info("\nAfter í‰ê°€ (USE_CHROMADB=1)")
        os.environ["USE_CHROMADB"] = "1"
        after_data = generate_evaluation_data(use_chromadb=True)

        generate_comparison_report(before_data, after_data)

    else:
        # ë‹¨ì¼ í‰ê°€
        eval_data = generate_evaluation_data(use_chromadb)

        if not args.generate_only and RAGAS_AVAILABLE:
            result = run_ragas_evaluation(eval_data)
            logger.info(f"\nğŸ“Š RAGAS í‰ê°€ ê²°ê³¼:")
            logger.info(result)

        # ê²°ê³¼ ì €ì¥
        output_path = os.path.join(
            _PROJECT_ROOT,
            "data",
            f"ragas_eval_{'chromadb' if use_chromadb else 'legacy'}.json"
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(eval_data, f, ensure_ascii=False, indent=2)
        logger.info(f"\nâœ… í‰ê°€ ë°ì´í„° ì €ì¥: {output_path}")


if __name__ == "__main__":
    main()
