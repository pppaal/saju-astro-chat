"""
GraphRAG Utilities (Batch Optimized CPU Version)
SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
- Hugging Face Í≥µÍ∞ú Î™®Îç∏ (Î°úÍ∑∏Ïù∏ Î∂àÌïÑÏöî)
- UTF-8 + BOM ÏßÄÏõê
- CPU/GPU ÏûêÎèô Í∞êÏßÄ
- CSV + JSON ÎèôÏãú ÏßÄÏõê
- Î∞∞Ïπò Ïù∏ÏΩîÎî©ÏúºÎ°ú ÏÜçÎèÑ 5~7Î∞∞ Ìñ•ÏÉÅ
"""

import os
import json
import csv
import torch
import hashlib
from functools import lru_cache
from sentence_transformers import SentenceTransformer, util

# ===============================================================
# üîß MODEL INITIALIZATION
# ===============================================================
_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
_MODEL = None


def get_model():
    """SentenceTransformer Î™®Îç∏ Î°úÎìú (Lazy load + ÏßÑÌñâ Î°úÍ∑∏)."""
    global _MODEL
    if _MODEL is None:
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"[GraphUtils] üß† Loading embedding model: {_MODEL_NAME}")
            print(f"[GraphUtils] Using device: {device}")
            print("[GraphUtils] ‚è≥ Î™®Îç∏ Îã§Ïö¥Î°úÎìú Î∞è Î°úÎî© Ï§ë... (Ï≤òÏùå 1ÌöåÎßå Ïò§Îûò Í±∏Î¶º)")

            _MODEL = SentenceTransformer(_MODEL_NAME, device=device)
            print("[GraphUtils] ‚úÖ SentenceTransformer Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")

        except Exception as e:
            raise RuntimeError(f"‚ùå SentenceTransformer load failed: {e}")
    return _MODEL


# ===============================================================
# üóÇÔ∏è GRAPH DATA LOADING (CSV + JSON ÎèôÏãú ÏßÄÏõê)
# ===============================================================
def _load_graph_nodes(graph_root: str) -> list[dict]:
    """
    CSV + JSON ÌÜµÌï© Î°úÎî©
    - astro_database, cross_analysis, saju ‚Üí CSV
    - rules, fusion ‚Üí JSON
    """
    all_nodes = []
    targets = ["astro_database", "cross_analysis", "saju", "rules", "fusion"]
    print(f"[GraphLoader] üìÇ Scanning base: {graph_root}")

    for sub in targets:
        folder = os.path.join(graph_root, sub)
        if not os.path.isdir(folder):
            print(f"[GraphLoader] ‚ö†Ô∏è Missing folder ‚Üí {folder}")
            continue

        for root, _, files in os.walk(folder):
            for f in files:
                path = os.path.join(root, f)
                # ============================= CSV =============================
                if f.endswith(".csv"):
                    try:
                        with open(path, newline="", encoding="utf-8-sig") as fr:
                            reader = csv.DictReader(fr)
                            for row in reader:
                                desc = (
                                    row.get("description")
                                    or row.get("ÏÑ§Î™Ö")
                                    or row.get("content")
                                    or ""
                                )
                                label = (
                                    row.get("label")
                                    or row.get("name")
                                    or row.get("Ïù¥Î¶Ñ")
                                    or ""
                                )
                                if desc.strip():
                                    all_nodes.append(
                                        {
                                            "label": label.strip(),
                                            "description": desc.strip(),
                                            "type": "csv_node",
                                            "source": sub,
                                        }
                                    )
                    except Exception as e:
                        print(f"[GraphLoader] ‚ùå CSV load failed: {path} | {e}")
                        continue

                # ============================= JSON =============================
                elif f.endswith(".json"):
                    try:
                        with open(path, "r", encoding="utf-8-sig") as fr:
                            data = json.load(fr)

                        if isinstance(data, list):
                            nodes = data
                        elif isinstance(data, dict) and "nodes" in data:
                            nodes = data["nodes"]
                        elif isinstance(data, dict):
                            nodes = [
                                {"label": k, "description": v, "type": "json_rule"}
                                for k, v in data.items()
                            ]
                        else:
                            nodes = [data]

                        for node in nodes:
                            if isinstance(node, dict) and node.get("description"):
                                node.setdefault("source", sub)
                                all_nodes.append(node)

                    except Exception as e:
                        print(f"[GraphLoader] ‚ùå JSON load failed: {path} | {e}")
                        continue

    print(f"[GraphLoader] ‚úÖ Loaded {len(all_nodes)} nodes total.")
    return all_nodes


# ===============================================================
# üß† EMBEDDING + CACHE
# ===============================================================
_CACHE_EMBEDS = {}
_NODES_CACHE = None
_TEXTS_CACHE = None
_CORPUS_EMBEDS_CACHE = None
_CORPUS_EMBEDS_PATH = None
_GRAPH_MTIME = None


def _latest_mtime(folder: str) -> float:
    latest = 0.0
    for root, _, files in os.walk(folder):
        for f in files:
            try:
                ts = os.path.getmtime(os.path.join(root, f))
                if ts > latest:
                    latest = ts
            except OSError:
                continue
    return latest
_NODES_CACHE = None
_TEXTS_CACHE = None
_CORPUS_EMBEDS_CACHE = None
_CORPUS_EMBEDS_PATH = None


def _hash_text(text: str) -> str:
    """ÌÖçÏä§Ìä∏ Ìï¥Ïãú ‚Üí Ï∫êÏãú ÌÇ§."""
    return hashlib.sha1(text.encode("utf-8")).hexdigest()[:16]


@lru_cache(maxsize=2048)
def embed_text(text: str):
    """Îã®Ïùº Î¨∏Ïû•ÏùÑ ÏûÑÎ≤†Îî©."""
    model = get_model()
    return model.encode(
        text,
        convert_to_tensor=True,
        normalize_embeddings=True,
        show_progress_bar=False,
    )


def embed_batch(texts: list[str], batch_size: int = 64):
    """Ïó¨Îü¨ Î¨∏Ïû•ÏùÑ Î∞∞ÏπòÎ°ú ÏûÑÎ≤†Îî© (ÏÜçÎèÑ Ìñ•ÏÉÅ)."""
    model = get_model()
    print(f"[GraphUtils] ‚öôÔ∏è Encoding {len(texts)} texts (batch={batch_size})...")
    embeds = model.encode(
        texts,
        batch_size=batch_size,
        convert_to_tensor=True,
        normalize_embeddings=True,
        show_progress_bar=True,  # ‚úÖ ÏßÑÌñâÎ∞î ÌëúÏãú
    )
    print(f"[GraphUtils] ‚úÖ Batch embedding complete: {embeds.shape}")
    return embeds


# ===============================================================
# üîé SEMANTIC GRAPH SEARCH
# ===============================================================
def search_graphs(query: str, top_k: int = 5, graph_root=None):
    """
    Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞ ÎÇ¥ description ÌïÑÎìúÎ•º ÏûÑÎ≤†Îî© Í∏∞Î∞òÏúºÎ°ú Í≤ÄÏÉâ.
    :param query: ÏûêÏó∞Ïñ¥ Í≤ÄÏÉâÏñ¥
    :param top_k: Î∞òÌôò Í∞úÏàò
    :param graph_root: Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞ Î£®Ìä∏ (Í∏∞Î≥∏: ÌòÑÏû¨ ÌååÏùº ÏúÑÏπò)
    :return: [{"label": str, "description": str, "score": float, ...}, ...]
    """
    if graph_root is None:
        graph_root = os.path.dirname(__file__)

    global _NODES_CACHE, _TEXTS_CACHE, _CORPUS_EMBEDS_CACHE, _CORPUS_EMBEDS_PATH, _GRAPH_MTIME

    current_mtime = _latest_mtime(graph_root)
    graph_changed = _GRAPH_MTIME is None or current_mtime > (_GRAPH_MTIME or 0)

    if _NODES_CACHE is None or graph_changed:
        _NODES_CACHE = _load_graph_nodes(graph_root)
        _TEXTS_CACHE = [n["description"] for n in _NODES_CACHE if n.get("description")]
        _CORPUS_EMBEDS_PATH = os.path.join(graph_root, "corpus_embeds.pt")
        _CORPUS_EMBEDS_CACHE = None  # force reload/recompute
        _GRAPH_MTIME = current_mtime

    if not _NODES_CACHE or not _TEXTS_CACHE:
        print("[GraphSearch] ‚ö†Ô∏è No graph nodes found.")
        return []

    print(f"[GraphSearch] üîé Embedding & searching in corpus ({len(_NODES_CACHE)} nodes)...")
    cache_key = _hash_text(query)

    # ‚úÖ Ï∫êÏãú Ïû¨ÏÇ¨Ïö©
    q_emb = _CACHE_EMBEDS.get(cache_key) or embed_text(query)
    _CACHE_EMBEDS[cache_key] = q_emb

    # ‚ö° ÌîÑÎ¶¨Ïª¥Ìì®Ìä∏ ÏûÑÎ≤†Îî© Î°úÎìú ÎòêÎäî ÏÉùÏÑ±
    if _CORPUS_EMBEDS_CACHE is None:
        if _CORPUS_EMBEDS_PATH and os.path.exists(_CORPUS_EMBEDS_PATH):
            print(f"[GraphUtils] ‚ö° Loading cached embeds: {_CORPUS_EMBEDS_PATH}")
            _CORPUS_EMBEDS_CACHE = torch.load(_CORPUS_EMBEDS_PATH, map_location="cpu")
        else:
            _CORPUS_EMBEDS_CACHE = embed_batch(_TEXTS_CACHE, batch_size=64)
            try:
                if _CORPUS_EMBEDS_PATH:
                    torch.save(_CORPUS_EMBEDS_CACHE, _CORPUS_EMBEDS_PATH)
                    print(f"[GraphUtils] üíæ Saved embeds ‚Üí {_CORPUS_EMBEDS_PATH}")
            except Exception as e:
                print(f"[GraphUtils] ‚ö†Ô∏è Failed to save embeds: {e}")

    corpus_embeds = _CORPUS_EMBEDS_CACHE
    scores = util.cos_sim(q_emb, corpus_embeds)[0]

    best_indices = torch.topk(scores, k=min(top_k, len(_NODES_CACHE)))
    results = []
    for idx, score in zip(best_indices.indices, best_indices.values):
        node = dict(_NODES_CACHE[int(idx)])
        node["score"] = round(float(score), 4)
        results.append(node)

    print(f"[GraphSearch] ‚úÖ Top-{top_k} results ready.")
    return results


# ===============================================================
# üß™ LOCAL TEST
# ===============================================================
if __name__ == "__main__":
    query = "Î¶¨ÎçîÏã≠Í≥º Ï∂îÏßÑÎ†•Ïù¥ Ï°∞ÌôîÎ•º Ïù¥Î£®Îäî ÏãúÍ∏∞"
    print(f"[GraphSearch] üîç Query: {query}")
    results = search_graphs(query, top_k=5)
    print(json.dumps(results, ensure_ascii=False, indent=2))
