import sys
import os
import json
import calendar
import re

# Load environment variables from backend_ai/.env file (explicit path with override)
from dotenv import load_dotenv
_backend_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
load_dotenv(os.path.join(_backend_root, ".env"), override=True)

RAG_DISABLED = os.getenv("RAG_DISABLE") == "1"

# Add project root to Python path for standalone execution
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

import logging
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Optional, Tuple
from uuid import uuid4

from flask import Flask, jsonify, g, request, Response, stream_with_context
from flask_cors import CORS
from flask_compress import Compress

from backend_ai.app.astro_parser import calculate_astrology_data
from backend_ai.app.fusion_logic import interpret_with_ai
from backend_ai.app.saju_parser import calculate_saju_data
from backend_ai.app.dream_logic import interpret_dream
from backend_ai.app.redis_cache import get_cache
from backend_ai.app.sanitizer import (
    sanitize_user_input,
    sanitize_dream_text,
    sanitize_name,
    validate_birth_data,
    is_suspicious_input,
)
# Lazy import fusion_generate to avoid loading SentenceTransformer on startup
# This prevents OOM on Railway free tier (512MB limit)
_fusion_generate_module = None

def _get_fusion_generate():
    """Lazy load fusion_generate module to save memory."""
    global _fusion_generate_module
    if _fusion_generate_module is None:
        from backend_ai.model import fusion_generate as _fg
        _fusion_generate_module = _fg
    return _fusion_generate_module

def _generate_with_gpt4(*args, **kwargs):
    """Lazy wrapper for _generate_with_gpt4."""
    return _get_fusion_generate()._generate_with_gpt4(*args, **kwargs)

def refine_with_gpt5mini(*args, **kwargs):
    """Lazy wrapper for refine_with_gpt5mini."""
    return _get_fusion_generate().refine_with_gpt5mini(*args, **kwargs)

from backend_ai.app.performance_optimizer import (
    track_performance,
    get_performance_stats,
    get_cache_health,
    suggest_optimizations,
)

# Gemini-level features
try:
    from backend_ai.app.realtime_astro import get_current_transits, get_transit_interpretation
    HAS_REALTIME = True
except ImportError:
    HAS_REALTIME = False

try:
    from backend_ai.app.chart_generator import (
        generate_saju_table_svg,
        generate_natal_chart_svg,
        generate_full_chart_html,
        svg_to_base64,
    )
    HAS_CHARTS = True
except ImportError:
    HAS_CHARTS = False

try:
    from backend_ai.app.user_memory import get_user_memory, generate_user_id
    HAS_USER_MEMORY = True
except ImportError:
    HAS_USER_MEMORY = False

# I-Ching RAG - Lazy loaded to avoid OOM (uses saju_astro_rag -> SentenceTransformer)
HAS_ICHING = True  # Assume available, will fail gracefully if not
_iching_rag_module = None

def _get_iching_rag():
    """Lazy load iching_rag module."""
    global _iching_rag_module, HAS_ICHING
    if _iching_rag_module is None:
        try:
            from backend_ai.app import iching_rag as _ir
            _iching_rag_module = _ir
        except ImportError:
            HAS_ICHING = False
            print("[app.py] I-Ching RAG not available (lazy load)")
            return None
    return _iching_rag_module

def cast_hexagram(*args, **kwargs):
    m = _get_iching_rag()
    return m.cast_hexagram(*args, **kwargs) if m else None

def get_hexagram_interpretation(*args, **kwargs):
    m = _get_iching_rag()
    return m.get_hexagram_interpretation(*args, **kwargs) if m else None

def perform_iching_reading(*args, **kwargs):
    m = _get_iching_rag()
    return m.perform_iching_reading(*args, **kwargs) if m else None

def search_iching_wisdom(*args, **kwargs):
    m = _get_iching_rag()
    return m.search_iching_wisdom(*args, **kwargs) if m else None

def get_all_hexagrams_summary(*args, **kwargs):
    m = _get_iching_rag()
    return m.get_all_hexagrams_summary(*args, **kwargs) if m else None

# Persona Embeddings - Lazy loaded (uses SentenceTransformer)
HAS_PERSONA_EMBED = not RAG_DISABLED  # Assume available unless disabled
_persona_embed_module = None

def _get_persona_embed_module():
    global _persona_embed_module, HAS_PERSONA_EMBED
    if _persona_embed_module is None:
        try:
            from backend_ai.app import persona_embeddings as _pe
            _persona_embed_module = _pe
        except ImportError:
            HAS_PERSONA_EMBED = False
            return None
    return _persona_embed_module

def get_persona_embed_rag(*args, **kwargs):
    m = _get_persona_embed_module()
    return m.get_persona_embed_rag(*args, **kwargs) if m else None

try:
    # This import is safe (no SentenceTransformer dependency at module level)
    pass  # Placeholder - persona_embeddings now lazy loaded above
    if not RAG_DISABLED:
        HAS_PERSONA_EMBED = True  # Already set above
except ImportError:
    HAS_PERSONA_EMBED = False

# Tarot Hybrid RAG - Lazy loaded (uses tarot_rag -> SentenceTransformer)
HAS_TAROT = True  # Assume available
_tarot_hybrid_rag_module = None

def _get_tarot_hybrid_rag_module():
    global _tarot_hybrid_rag_module, HAS_TAROT
    if _tarot_hybrid_rag_module is None:
        try:
            from backend_ai.app import tarot_hybrid_rag as _thr
            _tarot_hybrid_rag_module = _thr
        except ImportError:
            HAS_TAROT = False
            return None
    return _tarot_hybrid_rag_module

def get_tarot_hybrid_rag(*args, **kwargs):
    m = _get_tarot_hybrid_rag_module()
    return m.get_tarot_hybrid_rag(*args, **kwargs) if m else None

# RLHF Feedback Learning System
try:
    from backend_ai.app.feedback_learning import get_feedback_learning
    HAS_RLHF = True
except ImportError:
    HAS_RLHF = False

# Badge System
try:
    from backend_ai.app.badge_system import get_badge_system, get_midjourney_prompts
    HAS_BADGES = True
except ImportError:
    HAS_BADGES = False

# Domain RAG - Lazy loaded (uses SentenceTransformer)
HAS_DOMAIN_RAG = not RAG_DISABLED  # Assume available unless disabled
DOMAIN_RAG_DOMAINS = []  # Will be populated on first access
_domain_rag_module = None

def _get_domain_rag_module():
    global _domain_rag_module, HAS_DOMAIN_RAG, DOMAIN_RAG_DOMAINS
    if _domain_rag_module is None:
        try:
            from backend_ai.app import domain_rag as _dr
            _domain_rag_module = _dr
            DOMAIN_RAG_DOMAINS = _dr.DOMAINS
        except ImportError:
            HAS_DOMAIN_RAG = False
            print("[app.py] DomainRAG not available (lazy load)")
            return None
    return _domain_rag_module

def get_domain_rag(*args, **kwargs):
    m = _get_domain_rag_module()
    return m.get_domain_rag(*args, **kwargs) if m else None

# Compatibility (Saju + Astrology fusion) - Lazy loaded (uses saju_astro_rag)
HAS_COMPATIBILITY = True  # Assume available
_compatibility_logic_module = None

def _get_compatibility_logic():
    global _compatibility_logic_module, HAS_COMPATIBILITY
    if _compatibility_logic_module is None:
        try:
            from backend_ai.app import compatibility_logic as _cl
            _compatibility_logic_module = _cl
        except ImportError:
            HAS_COMPATIBILITY = False
            print("[app.py] Compatibility logic not available (lazy load)")
            return None
    return _compatibility_logic_module

def interpret_compatibility(*args, **kwargs):
    m = _get_compatibility_logic()
    return m.interpret_compatibility(*args, **kwargs) if m else None

def interpret_compatibility_group(*args, **kwargs):
    m = _get_compatibility_logic()
    return m.interpret_compatibility_group(*args, **kwargs) if m else None

# Hybrid RAG (Vector + BM25 + Graph + rerank) - Lazy loaded
HAS_HYBRID_RAG = True  # Assume available
_hybrid_rag_module = None

def _get_hybrid_rag_module():
    global _hybrid_rag_module, HAS_HYBRID_RAG
    if _hybrid_rag_module is None:
        try:
            from backend_ai.app import hybrid_rag as _hr
            _hybrid_rag_module = _hr
        except ImportError:
            HAS_HYBRID_RAG = False
            print("[app.py] Hybrid RAG not available (lazy load)")
            return None
    return _hybrid_rag_module

def hybrid_search(*args, **kwargs):
    m = _get_hybrid_rag_module()
    return m.hybrid_search(*args, **kwargs) if m else None

def build_rag_context(*args, **kwargs):
    m = _get_hybrid_rag_module()
    return m.build_rag_context(*args, **kwargs) if m else None

# Agentic RAG System (Next Level Features) - Lazy loaded to avoid OOM
# Import deferred to first use to prevent loading SentenceTransformer on startup
HAS_AGENTIC = True  # Assume available, will fail gracefully if not
_agentic_rag_module = None

def _get_agentic_rag():
    """Lazy load agentic_rag module."""
    global _agentic_rag_module, HAS_AGENTIC
    if _agentic_rag_module is None:
        try:
            from backend_ai.app import agentic_rag as _ar
            _agentic_rag_module = _ar
        except ImportError:
            HAS_AGENTIC = False
            print("[app.py] Agentic RAG not available (lazy load)")
            return None
    return _agentic_rag_module

def agentic_query(*args, **kwargs):
    """Lazy wrapper for agentic_query."""
    m = _get_agentic_rag()
    return m.agentic_query(*args, **kwargs) if m else None

def get_agent_orchestrator(*args, **kwargs):
    """Lazy wrapper for get_agent_orchestrator."""
    m = _get_agentic_rag()
    return m.get_agent_orchestrator(*args, **kwargs) if m else None

def get_entity_extractor(*args, **kwargs):
    """Lazy wrapper for get_entity_extractor."""
    m = _get_agentic_rag()
    return m.get_entity_extractor(*args, **kwargs) if m else None

def get_deep_traversal(*args, **kwargs):
    """Lazy wrapper for get_deep_traversal."""
    m = _get_agentic_rag()
    return m.get_deep_traversal(*args, **kwargs) if m else None

# Classes are accessed as properties
EntityExtractor = property(lambda self: _get_agentic_rag().EntityExtractor if _get_agentic_rag() else None)
DeepGraphTraversal = property(lambda self: _get_agentic_rag().DeepGraphTraversal if _get_agentic_rag() else None)
AgentOrchestrator = property(lambda self: _get_agentic_rag().AgentOrchestrator if _get_agentic_rag() else None)

# Jungian Counseling Engine - Lazy loaded (uses SentenceTransformer)
HAS_COUNSELING = True  # Assume available
_counseling_engine_module = None

def _get_counseling_engine_module():
    global _counseling_engine_module, HAS_COUNSELING
    if _counseling_engine_module is None:
        try:
            from backend_ai.app import counseling_engine as _ce
            _counseling_engine_module = _ce
        except ImportError:
            HAS_COUNSELING = False
            print("[app.py] Counseling engine not available (lazy load)")
            return None
    return _counseling_engine_module

def get_counseling_engine(*args, **kwargs):
    m = _get_counseling_engine_module()
    return m.get_counseling_engine(*args, **kwargs) if m else None

def _get_crisis_detector():
    """Get CrisisDetector class from counseling engine module."""
    m = _get_counseling_engine_module()
    return m.CrisisDetector if m else None

# Proxy class that forwards calls to the actual CrisisDetector
class _CrisisDetectorProxy:
    @staticmethod
    def detect_crisis(text):
        detector = _get_crisis_detector()
        if detector:
            return detector.detect_crisis(text)
        return {"is_crisis": False, "max_severity": "none", "detections": [], "requires_immediate_action": False}

    @staticmethod
    def get_crisis_response(severity, locale="ko"):
        detector = _get_crisis_detector()
        if detector:
            return detector.get_crisis_response(severity, locale)
        return {"immediate_message": "", "follow_up": "", "closing": ""}

CrisisDetector = _CrisisDetectorProxy

# Prediction Engine (v5.0)
if os.getenv("PREDICTION_DISABLE") == "1":
    HAS_PREDICTION = False
    print("[app.py] Prediction engine disabled by PREDICTION_DISABLE")
else:
    try:
        from backend_ai.app.prediction_engine import (
            get_prediction_engine,
            predict_luck,
            find_best_date,
            get_full_forecast,
            EventType,
        )
        HAS_PREDICTION = True
    except ImportError:
        HAS_PREDICTION = False
        print("[app.py] Prediction engine not available")

# Theme Cross-Reference Filter (v5.1)
try:
    from backend_ai.app.theme_cross_filter import (
        get_theme_filter,
        filter_data_by_theme,
        get_theme_prompt_context,
    )
    HAS_THEME_FILTER = True
except ImportError:
    HAS_THEME_FILTER = False
    print("[app.py] Theme cross filter not available")

# Fortune Score Engine (v1.0) - Real-time saju+astrology scoring
try:
    from backend_ai.app.fortune_score_engine import (
        get_fortune_score_engine,
        calculate_fortune_score,
    )
    HAS_FORTUNE_SCORE = True
except ImportError:
    HAS_FORTUNE_SCORE = False
    print("[app.py] Fortune score engine not available")

# GraphRAG System - Lazy loaded (uses SentenceTransformer)
HAS_GRAPH_RAG = not RAG_DISABLED  # Assume available unless disabled
_saju_astro_rag_module = None

def _get_saju_astro_rag_module():
    global _saju_astro_rag_module, HAS_GRAPH_RAG
    if _saju_astro_rag_module is None:
        try:
            from backend_ai.app import saju_astro_rag as _sar
            _saju_astro_rag_module = _sar
        except ImportError:
            HAS_GRAPH_RAG = False
            print("[app.py] GraphRAG not available (lazy load)")
            return None
    return _saju_astro_rag_module

def get_graph_rag(*args, **kwargs):
    m = _get_saju_astro_rag_module()
    return m.get_graph_rag(*args, **kwargs) if m else None

def get_model(*args, **kwargs):
    m = _get_saju_astro_rag_module()
    return m.get_model(*args, **kwargs) if m else None

# OpenAI Client for streaming endpoints
try:
    from openai import OpenAI
    import httpx
    _openai_key = os.getenv("OPENAI_API_KEY")
    if not _openai_key:
        print(f"[app.py] OPENAI_API_KEY not found in environment. Available env vars: {[k for k in os.environ.keys() if 'OPENAI' in k.upper() or 'API' in k.upper()]}")
        raise ValueError("OPENAI_API_KEY environment variable is not set")
    openai_client = OpenAI(
        api_key=_openai_key,
        timeout=httpx.Timeout(60.0, connect=10.0)  # 60s total, 10s connect
    )
    OPENAI_AVAILABLE = True
    print(f"[app.py] OpenAI client initialized successfully (key length: {len(_openai_key)})")
except Exception as e:
    openai_client = None
    OPENAI_AVAILABLE = False
    print(f"[app.py] OpenAI client not available: {e}")

# CorpusRAG System - Lazy loaded (uses SentenceTransformer)
HAS_CORPUS_RAG = not RAG_DISABLED  # Assume available unless disabled
_corpus_rag_module = None

def _get_corpus_rag_module():
    global _corpus_rag_module, HAS_CORPUS_RAG
    if _corpus_rag_module is None:
        try:
            from backend_ai.app import corpus_rag as _cr
            _corpus_rag_module = _cr
        except ImportError:
            HAS_CORPUS_RAG = False
            print("[app.py] CorpusRAG not available (lazy load)")
            return None
    return _corpus_rag_module

def get_corpus_rag(*args, **kwargs):
    m = _get_corpus_rag_module()
    return m.get_corpus_rag(*args, **kwargs) if m else None

# Numerology System
try:
    from backend_ai.app.numerology_logic import (
        analyze_numerology,
        analyze_numerology_compatibility,
        calculate_full_numerology,
    )
    HAS_NUMEROLOGY = True
except ImportError:
    HAS_NUMEROLOGY = False
    print("[app.py] Numerology not available")

# ICP (Interpersonal Circumplex) System
try:
    from backend_ai.app.icp_logic import (
        analyze_icp_style,
        analyze_icp_compatibility,
        get_icp_questions,
        ICPAnalyzer,
    )
    HAS_ICP = True
except ImportError:
    HAS_ICP = False
    print("[app.py] ICP not available")

# Flask Application
app = Flask(__name__)

# Gzip compression - reduces response size by 30-50%
Compress(app)
app.config['COMPRESS_MIMETYPES'] = [
    'text/html', 'text/css', 'text/xml', 'text/plain',
    'application/json', 'application/javascript', 'application/xml'
]
app.config['COMPRESS_LEVEL'] = 6  # Balance between compression ratio and CPU usage
app.config['COMPRESS_MIN_SIZE'] = 500  # Only compress responses > 500 bytes

# CORS configuration - restrict to specific origins for security
CORS_ORIGINS = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "https://destinypal.com",
    "https://www.destinypal.com",
]
# Allow custom origins from environment variable
if os.getenv("CORS_ALLOWED_ORIGINS"):
    CORS_ORIGINS.extend(os.getenv("CORS_ALLOWED_ORIGINS").split(","))

CORS(
    app,
    origins=CORS_ORIGINS,
    allow_headers=["Content-Type", "Authorization", "X-API-KEY", "X-Request-ID"],
    methods=["GET", "POST", "OPTIONS"],
    supports_credentials=True,
    max_age=3600,
)

# Basic logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("backend_ai")

# Optional Sentry (no-op if DSN missing)
try:
    import sentry_sdk

    if os.getenv("SENTRY_DSN"):
        sentry_sdk.init(dsn=os.getenv("SENTRY_DSN"))
        logger.info("Sentry initialized for Flask backend.")
except Exception as e:  # pragma: no cover
    logger.warning(f"Sentry init skipped: {e}")


# ===============================================================
# ğŸ”Œ REGISTER MODULAR BLUEPRINTS
# ===============================================================
# Blueprints are registered first, so they take priority over legacy routes below.
# Legacy routes are kept for backwards compatibility but will be shadowed.
try:
    from backend_ai.app.routers import register_all_blueprints
    register_all_blueprints(app)
    logger.info("âœ… Modular blueprints registered successfully")
except ImportError as e:
    logger.warning(f"Could not import routers: {e}")
except Exception as e:
    logger.error(f"Failed to register blueprints: {e}")


# ===============================================================
# ğŸ›¡ï¸ INPUT SANITIZATION HELPERS
# ===============================================================

def sanitize_messages(messages: list, max_content_length: int = 2000) -> list:
    """Sanitize a list of chat messages."""
    if not messages or not isinstance(messages, list):
        return []
    sanitized = []
    for msg in messages:
        if not isinstance(msg, dict):
            continue
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if isinstance(content, str) and content:
            # Check for suspicious patterns
            if is_suspicious_input(content):
                logger.warning(f"[SANITIZE] Suspicious content in {role} message")
            content = sanitize_user_input(content, max_length=max_content_length, allow_newlines=True)
        sanitized.append({"role": role, "content": content})
    return sanitized


def mask_sensitive_data(text: str) -> str:
    """Mask potentially sensitive data in logs."""
    import re
    # Mask email addresses
    text = re.sub(r'[\w\.-]+@[\w\.-]+\.\w+', '[EMAIL]', text)
    # Mask phone numbers (various formats)
    text = re.sub(r'\b\d{3}[-.\s]?\d{3,4}[-.\s]?\d{4}\b', '[PHONE]', text)
    # Mask credit card numbers
    text = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', '[CARD]', text)
    return text


# ===============================================================
# ğŸš€ CROSS-ANALYSIS CACHE - Pre-loaded for instant lookups
# ===============================================================
_CROSS_ANALYSIS_CACHE = {}

# ===============================================================
# ğŸ”— INTEGRATION ENGINE CACHE - Multimodal analysis data
# ===============================================================
_INTEGRATION_DATA_CACHE = {
    "multimodal_engine": None,
    "career_mapping": None,
    "numerology_core": None,
    "numerology_compatibility": None,
    "numerology_saju": None,
    "numerology_astro": None,
    "numerology_therapeutic": None,
}


def _load_integration_data():
    """Load integration engine and numerology data."""
    global _INTEGRATION_DATA_CACHE

    if _INTEGRATION_DATA_CACHE.get("multimodal_engine") is not None:
        return _INTEGRATION_DATA_CACHE

    base_dir = os.path.dirname(os.path.dirname(__file__))

    # Integration engine files
    integration_dir = os.path.join(base_dir, "data", "graph", "rules", "integration")
    integration_files = {
        "multimodal_engine": "multimodal_integration_engine.json",
        "career_mapping": "modern_career_mapping.json",
    }

    for key, filename in integration_files.items():
        filepath = os.path.join(integration_dir, filename)
        try:
            if os.path.exists(filepath):
                with open(filepath, "r", encoding="utf-8") as f:
                    _INTEGRATION_DATA_CACHE[key] = json.load(f)
                    logger.info(f"  âœ… Loaded integration: {filename}")
        except Exception as e:
            logger.warning(f"  âš ï¸ Failed to load {filename}: {e}")
            _INTEGRATION_DATA_CACHE[key] = {}

    # Numerology files
    numerology_dir = os.path.join(base_dir, "data", "graph", "rules", "numerology")
    numerology_files = {
        "numerology_core": "numerology_core_rules.json",
        "numerology_compatibility": "numerology_compatibility_rules.json",
        "numerology_saju": "numerology_saju_mapping.json",
        "numerology_astro": "numerology_astro_mapping.json",
        "numerology_therapeutic": "numerology_therapeutic_questions.json",
    }

    for key, filename in numerology_files.items():
        filepath = os.path.join(numerology_dir, filename)
        try:
            if os.path.exists(filepath):
                with open(filepath, "r", encoding="utf-8") as f:
                    _INTEGRATION_DATA_CACHE[key] = json.load(f)
                    logger.info(f"  âœ… Loaded numerology: {filename}")
        except Exception as e:
            logger.warning(f"  âš ï¸ Failed to load {filename}: {e}")
            _INTEGRATION_DATA_CACHE[key] = {}

    loaded_count = sum(1 for v in _INTEGRATION_DATA_CACHE.values() if v)
    logger.info(f"[INTEGRATION-CACHE] Loaded {loaded_count}/7 integration/numerology files")
    return _INTEGRATION_DATA_CACHE


def get_integration_context(theme: str = "life") -> Dict:
    """Get theme-specific integration context for multimodal analysis."""
    data = _load_integration_data()
    engine = data.get("multimodal_engine", {})

    result = {
        "correlation_matrix": engine.get("correlation_matrix", {}),
        "theme_focus": {},
    }

    # Get theme-specific focus areas
    question_router = engine.get("question_router", {})
    if theme in question_router:
        result["theme_focus"] = question_router[theme]

    return result


# ===============================================================
# ğŸ§  JUNG PSYCHOLOGY CACHE - Enhanced therapeutic data
# ===============================================================
_JUNG_DATA_CACHE = {
    "active_imagination": None,
    "lifespan_individuation": None,
    "crisis_intervention": None,
    "archetypes": None,
    "therapeutic": None,
    "cross_analysis": None,
    "psychological_types": None,
    "alchemy": None,
    "counseling_scenarios": None,
    "integrated_counseling": None,
    "counseling_prompts": None,
    "personality_integration": None,
    "expanded_counseling": None,
}


def _load_jung_data():
    """Load extended Jung psychology data for deeper therapeutic sessions."""
    global _JUNG_DATA_CACHE

    # Return cached data if already loaded
    if _JUNG_DATA_CACHE.get("active_imagination") is not None:
        return _JUNG_DATA_CACHE

    jung_dir = os.path.join(os.path.dirname(__file__), "..", "data", "graph", "rules", "jung")
    jung_dir = os.path.abspath(jung_dir)

    files_to_load = {
        "active_imagination": "jung_active_imagination.json",
        "lifespan_individuation": "jung_lifespan_individuation.json",
        "crisis_intervention": "jung_crisis_intervention.json",
        "archetypes": "jung_archetypes.json",
        "therapeutic": "jung_therapeutic.json",
        "cross_analysis": "jung_cross_analysis.json",
        "psychological_types": "jung_psychological_types.json",
        "alchemy": "jung_alchemy.json",
        "counseling_scenarios": "jung_counseling_scenarios.json",
        "integrated_counseling": "jung_integrated_counseling.json",
        "counseling_prompts": "jung_counseling_prompts.json",
        "personality_integration": "jung_personality_integration.json",
        "expanded_counseling": "jung_expanded_counseling.json",
    }

    for key, filename in files_to_load.items():
        filepath = os.path.join(jung_dir, filename)
        try:
            if os.path.exists(filepath):
                with open(filepath, "r", encoding="utf-8") as f:
                    _JUNG_DATA_CACHE[key] = json.load(f)
                    logger.info(f"  âœ… Loaded Jung data: {filename}")
        except Exception as e:
            logger.warning(f"  âš ï¸ Failed to load {filename}: {e}")
            _JUNG_DATA_CACHE[key] = {}

    logger.info(f"[JUNG-CACHE] Loaded {sum(1 for v in _JUNG_DATA_CACHE.values() if v)} Jung psychology files")
    return _JUNG_DATA_CACHE


def get_lifespan_guidance(birth_year: int) -> dict:
    """Get age-appropriate psychological guidance based on Jung's lifespan individuation."""
    jung_data = _load_jung_data()
    lifespan = jung_data.get("lifespan_individuation", {})

    if not lifespan:
        return {}

    from datetime import datetime
    current_year = datetime.now().year
    age = current_year - birth_year

    life_stages = lifespan.get("life_stages", {})

    # Determine life stage
    if age <= 12:
        stage = "childhood"
    elif age <= 22:
        stage = "adolescence"
    elif age <= 35:
        stage = "early_adulthood"
    elif age <= 55:
        stage = "midlife"
    elif age <= 70:
        stage = "mature_adulthood"
    else:
        stage = "elder"

    stage_data = life_stages.get(stage, {})

    return {
        "age": age,
        "stage_name": stage_data.get("name_ko", stage),
        "psychological_tasks": stage_data.get("psychological_tasks", []),
        "archetypal_themes": stage_data.get("archetypal_themes", {}),
        "developmental_crises": stage_data.get("developmental_crises", []),
        "shadow_challenges": stage_data.get("shadow_challenges", stage_data.get("shadow_manifestations", [])),
        "saju_parallel": stage_data.get("saju_parallel", {}),
        "astro_parallel": stage_data.get("astro_parallel", {}),
        "guidance": stage_data.get("guidance", stage_data.get("saturn_return_guidance", stage_data.get("uranus_opposition_guidance", {}))),
    }


def get_active_imagination_prompts(context: str) -> list:
    """Get appropriate active imagination exercise prompts based on context."""
    jung_data = _load_jung_data()
    ai_data = jung_data.get("active_imagination", {})

    if not ai_data:
        return []

    prompts = []
    facilitation = ai_data.get("ai_facilitation_guide", {})

    # Get opening prompts based on context
    context_lower = context.lower()

    if any(k in context_lower for k in ["ê¿ˆ", "ì•…ëª½", "ê¿ˆì—ì„œ"]):
        prompts = facilitation.get("opening_prompts", {}).get("after_dream_sharing", [])
    elif any(k in context_lower for k in ["ì‚¬ì£¼", "ìš´ì„¸", "ì¼ê°„"]):
        prompts = facilitation.get("opening_prompts", {}).get("after_saju_analysis", [])
    elif any(k in context_lower for k in ["ì ì„±", "ë³„ìë¦¬", "í•˜ìš°ìŠ¤"]):
        prompts = facilitation.get("opening_prompts", {}).get("after_astro_analysis", [])
    else:
        prompts = facilitation.get("opening_prompts", {}).get("general", [])

    # Add deepening and integration prompts
    deepening = facilitation.get("deepening_prompts", [])
    integration = facilitation.get("integration_prompts", [])

    return {
        "opening": prompts[:2],
        "deepening": deepening[:3],
        "integration": integration[:2],
    }


def get_crisis_resources(locale: str = "ko") -> dict:
    """Get crisis intervention resources and scripts."""
    jung_data = _load_jung_data()
    crisis = jung_data.get("crisis_intervention", {})

    if not crisis:
        return {}

    resources = crisis.get("response_protocols", {}).get("suicidal_ideation", {}).get("resources_korea", {})
    limitations = crisis.get("ai_limitations_and_boundaries", {})
    deescalation = crisis.get("de_escalation_techniques", {})

    return {
        "resources": resources,
        "limitations": limitations,
        "deescalation": deescalation,
    }

def _load_cross_analysis_cache():
    """Load cross-analysis JSON files for instant lookups (no embedding search)."""
    global _CROSS_ANALYSIS_CACHE
    if _CROSS_ANALYSIS_CACHE:
        return _CROSS_ANALYSIS_CACHE

    import json
    fusion_dir = os.path.join(os.path.dirname(__file__), "..", "data", "graph", "fusion")
    fusion_dir = os.path.abspath(fusion_dir)

    if not os.path.exists(fusion_dir):
        logger.warning(f"[CROSS-CACHE] Fusion dir not found: {fusion_dir}")
        return {}

    for fname in os.listdir(fusion_dir):
        if fname.endswith(".json") and "cross" in fname.lower():
            try:
                with open(os.path.join(fusion_dir, fname), "r", encoding="utf-8") as f:
                    data = json.load(f)
                    key = fname.replace(".json", "")
                    _CROSS_ANALYSIS_CACHE[key] = data
                    logger.info(f"  âœ… Loaded cross-analysis: {fname}")
            except Exception as e:
                logger.warning(f"  âš ï¸ Failed to load {fname}: {e}")

    logger.info(f"[CROSS-CACHE] Loaded {len(_CROSS_ANALYSIS_CACHE)} cross-analysis files")
    return _CROSS_ANALYSIS_CACHE


def normalize_day_master(saju_data: dict) -> dict:
    """
    Normalize dayMaster to flat structure { name, element }.
    Handles:
    - String: "åºš" -> { name: "åºš", heavenlyStem: "åºš", element: "ê¸ˆ" }
    - Nested: { heavenlyStem: { name: "åºš", element: "ê¸ˆ" }, element: "..." }
    - Flat: { name: "åºš", element: "ê¸ˆ" } or { heavenlyStem: "åºš", element: "ê¸ˆ" }
    Returns normalized saju_data with flat dayMaster.
    """
    if not saju_data or not saju_data.get("dayMaster"):
        return saju_data

    dm = saju_data.get("dayMaster", {})

    # Map stem to element
    stem_to_element = {
        "ç”²": "ëª©", "ä¹™": "ëª©", "ä¸™": "í™”", "ä¸": "í™”", "æˆŠ": "í† ",
        "å·±": "í† ", "åºš": "ê¸ˆ", "è¾›": "ê¸ˆ", "å£¬": "ìˆ˜", "ç™¸": "ìˆ˜",
        "ê°‘": "ëª©", "ì„": "ëª©", "ë³‘": "í™”", "ì •": "í™”", "ë¬´": "í† ",
        "ê¸°": "í† ", "ê²½": "ê¸ˆ", "ì‹ ": "ê¸ˆ", "ì„": "ìˆ˜", "ê³„": "ìˆ˜",
    }

    # Handle dayMaster as string (e.g., "åºš" or "ê²½")
    if isinstance(dm, str):
        element = stem_to_element.get(dm, "")
        normalized_dm = {
            "name": dm,
            "heavenlyStem": dm,
            "element": element,
        }
        saju_data = dict(saju_data)
        saju_data["dayMaster"] = normalized_dm
        logger.debug(f"[NORMALIZE] dayMaster: string -> dict: {normalized_dm}")
        return saju_data

    if not isinstance(dm, dict):
        return saju_data

    # Check if heavenlyStem is a nested object
    hs = dm.get("heavenlyStem")
    if isinstance(hs, dict):
        # Nested structure: { heavenlyStem: { name, element } }
        normalized_dm = {
            "name": hs.get("name", ""),
            "heavenlyStem": hs.get("name", ""),
            "element": hs.get("element") or dm.get("element", ""),
        }
        saju_data = dict(saju_data)  # Copy to avoid mutation
        saju_data["dayMaster"] = normalized_dm
        logger.debug(f"[NORMALIZE] dayMaster: nested -> flat: {normalized_dm}")
    elif isinstance(hs, str):
        # Already flat but with heavenlyStem as string
        normalized_dm = {
            "name": hs,
            "heavenlyStem": hs,
            "element": dm.get("element", ""),
        }
        saju_data = dict(saju_data)
        saju_data["dayMaster"] = normalized_dm
    # else: already in { name, element } format or empty

    return saju_data


def _normalize_birth_date(value: object) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        value = str(int(value))
    if not isinstance(value, str):
        return None
    text = value.strip()
    if not text:
        return None
    text = text.replace(".", "-").replace("/", "-")
    if re.fullmatch(r"\d{8}", text):
        year, month, day = text[:4], text[4:6], text[6:8]
    else:
        parts = [p for p in text.split("-") if p]
        if len(parts) != 3:
            return None
        year, month, day = parts
        if not (year.isdigit() and month.isdigit() and day.isdigit()):
            return None
        if len(year) != 4:
            return None
        month = month.zfill(2)
        day = day.zfill(2)
    try:
        datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
    except ValueError:
        return None
    return f"{year}-{month}-{day}"


def _normalize_birth_time(value: object) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        value = str(value)
    if not isinstance(value, str):
        return None
    text = value.strip()
    if not text:
        return None
    text = text.replace(".", ":")
    if re.fullmatch(r"\d{1,2}:\d{2}(:\d{2})?", text):
        parts = text.split(":")
        hour = int(parts[0])
        minute = int(parts[1])
        second = int(parts[2]) if len(parts) > 2 else None
        if hour > 23 or minute > 59 or (second is not None and second > 59):
            return None
        if second is None:
            return f"{hour:02d}:{minute:02d}"
        return f"{hour:02d}:{minute:02d}:{second:02d}"
    if re.fullmatch(r"\d{3,4}", text):
        padded = text.zfill(4)
        hour = int(padded[:2])
        minute = int(padded[2:])
        if hour > 23 or minute > 59:
            return None
        return f"{hour:02d}:{minute:02d}"
    return None


def _normalize_birth_payload(data: dict) -> dict:
    """Normalize birth payload from nested or legacy fields."""
    if not isinstance(data, dict):
        return {}

    birth = data.get("birth")
    birth_data = birth if isinstance(birth, dict) else {}
    normalized = dict(birth_data)

    def _pick(source: dict, keys: list) -> Optional[object]:
        for key in keys:
            value = source.get(key)
            if value not in (None, ""):
                return value
        return None

    def _coerce_float(value: object) -> Optional[float]:
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, str):
            value = value.strip()
            if not value:
                return None
            try:
                return float(value)
            except ValueError:
                return None
        return None

    date_raw = _pick(birth_data, ["date"]) or _pick(data, ["birthdate", "birth_date", "birthDate"])
    time_raw = _pick(birth_data, ["time"]) or _pick(data, ["birthtime", "birth_time", "birthTime"])
    gender = _pick(birth_data, ["gender"]) or _pick(data, ["gender", "sex"])
    city = _pick(birth_data, ["city", "place"]) or _pick(
        data, ["birthplace", "birth_place", "birthPlace", "city", "place", "location"]
    )
    lat_val = _pick(birth_data, ["lat", "latitude"]) or _pick(data, ["lat", "latitude"])
    lon_val = _pick(birth_data, ["lon", "longitude"]) or _pick(data, ["lon", "longitude", "lng", "long"])

    date = _normalize_birth_date(date_raw)
    if date:
        normalized["date"] = date
    elif date_raw:
        normalized["date"] = str(date_raw).strip()

    time_val = _normalize_birth_time(time_raw)
    if time_val:
        normalized["time"] = time_val
    elif time_raw:
        normalized["time"] = str(time_raw).strip()
    if gender:
        normalized["gender"] = gender
    if city:
        normalized["city"] = city

    lat = _coerce_float(lat_val)
    lon = _coerce_float(lon_val)
    if lat is not None:
        normalized["lat"] = lat
        if "latitude" not in normalized:
            normalized["latitude"] = lat
    if lon is not None:
        normalized["lon"] = lon
        if "longitude" not in normalized:
            normalized["longitude"] = lon

    return normalized


def get_cross_analysis_for_chart(saju_data: dict, astro_data: dict, theme: str = "chat", locale: str = "ko") -> str:
    """
    Get detailed cross-analysis based on user's chart data.
    Enhanced v3: Uses ALL fusion rules with:
    - Planet + House combinations with timing/advice
    - Saju Ten Gods (ì‹­ì‹ ) analysis
    - Element cross-matching (ì‚¬ì£¼ ì˜¤í–‰ Ã— ì ì„± ì›ì†Œ)
    - Health, Wealth, Family, Life Path analysis
    - Actionable insights with specific timing
    - Supports both new (text_ko/advice) and legacy (text only) rule formats
    """
    cache = _load_cross_analysis_cache()
    results = []
    detailed_insights = []

    # Get chart elements (support both "heavenlyStem" and "name" for dayMaster)
    dm_data = saju_data.get("dayMaster", {})
    if isinstance(dm_data, str):
        daymaster = dm_data
        dm_element = ""
    else:
        daymaster = dm_data.get("heavenlyStem") or dm_data.get("name", "") if isinstance(dm_data, dict) else ""
        dm_element = dm_data.get("element", "") if isinstance(dm_data, dict) else ""

    # Safely get astro signs (handle both dict and non-dict cases)
    sun_data = astro_data.get("sun", {})
    sun_sign = sun_data.get("sign", "") if isinstance(sun_data, dict) else ""
    moon_data = astro_data.get("moon", {})
    moon_sign = moon_data.get("sign", "") if isinstance(moon_data, dict) else ""
    dominant = saju_data.get("dominantElement", "")

    # Extract Ten Gods (ì‹­ì‹ ) from saju data
    ten_gods = saju_data.get("tenGods", {})
    if not isinstance(ten_gods, dict):
        ten_gods = {}
    dominant_god = ten_gods.get("dominant", "")  # e.g., "ì •ê´€", "í¸ê´€", "ì •ì¬", "ìƒê´€"
    # Ensure dominant_god is a string (not dict)
    if isinstance(dominant_god, dict):
        dominant_god = dominant_god.get("name", "") or dominant_god.get("ko", "") or ""
    elif not isinstance(dominant_god, str):
        dominant_god = str(dominant_god) if dominant_god else ""

    # Get element counts for imbalance detection
    element_counts = saju_data.get("elementCounts", {})

    # Map Korean sign names to English and element
    sign_map = {
        "ì–‘ìë¦¬": "Aries", "í™©ì†Œìë¦¬": "Taurus", "ìŒë‘¥ì´ìë¦¬": "Gemini",
        "ê²Œìë¦¬": "Cancer", "ì‚¬ììë¦¬": "Leo", "ì²˜ë…€ìë¦¬": "Virgo",
        "ì²œì¹­ìë¦¬": "Libra", "ì „ê°ˆìë¦¬": "Scorpio", "ê¶ìˆ˜ìë¦¬": "Sagittarius",
        "ì—¼ì†Œìë¦¬": "Capricorn", "ë¬¼ë³‘ìë¦¬": "Aquarius", "ë¬¼ê³ ê¸°ìë¦¬": "Pisces",
    }
    sign_element_map = {
        "Aries": "fire", "Leo": "fire", "Sagittarius": "fire",
        "Taurus": "earth", "Virgo": "earth", "Capricorn": "earth",
        "Gemini": "air", "Libra": "air", "Aquarius": "air",
        "Cancer": "water", "Scorpio": "water", "Pisces": "water",
    }
    # Map saju elements to flags for health rules
    element_to_flag = {
        "æœ¨": "wood", "ç«": "fire", "åœŸ": "earth", "é‡‘": "metal", "æ°´": "water"
    }

    sun_sign_en = sign_map.get(sun_sign, sun_sign)
    moon_sign_en = sign_map.get(moon_sign, moon_sign)
    sun_element = sign_element_map.get(sun_sign_en, "")

    # Planet-house combinations to check (used in multiple sections)
    planet_house_checks = []
    for planet in ["sun", "moon", "mercury", "venus", "mars", "jupiter", "saturn"]:
        p_data = astro_data.get(planet, {})
        house = p_data.get("house")
        if house:
            planet_house_checks.append((planet, str(house)))

    # Load ALL fusion rules (not just career/love)
    fusion_rules = {}
    try:
        rules_dir = Path(__file__).parent.parent / "data" / "graph" / "rules" / "fusion"
        # Load all theme-specific fusion rules
        all_rule_files = [
            "career.json", "love.json", "health.json", "wealth.json",
            "family.json", "life_path.json", "daily.json", "monthly.json",
            "compatibility.json", "new_year.json", "next_year.json"
        ]
        for rule_file in all_rule_files:
            rule_path = rules_dir / rule_file
            if rule_path.exists():
                with open(rule_path, "r", encoding="utf-8") as f:
                    rules = json.load(f)
                    fusion_rules[rule_file.replace(".json", "")] = rules
        logger.debug(f"[CROSS-ANALYSIS] Loaded {len(fusion_rules)} fusion rule sets")
    except Exception as e:
        logger.warning(f"[CROSS-ANALYSIS] Failed to load fusion rules: {e}")

    # 1. Cross-analysis cache lookup (daymaster Ã— sun sign) - INSTANT
    if cache:
        advanced_cross = cache.get("saju_astro_advanced_cross", {})
        dm_data = advanced_cross.get("daymaster_sun_complete", {}).get(daymaster, {})
        if dm_data:
            sun_combo = dm_data.get("sun_signs", {}).get(sun_sign_en, {})
            if sun_combo:
                results.append(f"[{daymaster}+{sun_sign_en}] {sun_combo.get('insight', '')} | ì¶”ì²œ: {', '.join(sun_combo.get('best_for', []))} | ì£¼ì˜: {sun_combo.get('caution', '')}")

            # Moon cross-analysis
            if moon_sign_en and moon_sign_en != sun_sign_en:
                moon_combo = dm_data.get("sun_signs", {}).get(moon_sign_en, {})
                if moon_combo:
                    results.append(f"[{daymaster}+ë‹¬:{moon_sign_en}] ê°ì •: {moon_combo.get('insight', '')[:80]}")

        # 1-2. ì‹­ì‹ -í–‰ì„± êµì°¨ ë¶„ì„ (cross_sipsin_planets.json)
        sipsin_planets = cache.get("cross_sipsin_planets", {})
        if sipsin_planets and dominant_god:
            sipsin_mapping = sipsin_planets.get("sipsin_planet_mapping", {})
            if dominant_god in sipsin_mapping:
                sp_data = sipsin_mapping[dominant_god]
                planet = sp_data.get("planet", "")
                life_areas = sp_data.get("life_areas", {})
                psych = sp_data.get("psychological_theme", "")
                # Theme-specific insight from sipsin-planet mapping
                area_text = ""
                if theme in ["focus_career", "career"] and life_areas.get("career"):
                    area_text = life_areas["career"]
                elif theme in ["focus_love", "love"] and life_areas.get("relationship"):
                    area_text = life_areas["relationship"]
                elif theme in ["focus_wealth", "wealth"] and life_areas.get("wealth"):
                    area_text = life_areas["wealth"]
                elif theme in ["focus_health", "health"] and life_areas.get("health"):
                    area_text = life_areas["health"]
                if area_text:
                    detailed_insights.append((7, f"ğŸ”— ì‹­ì‹ Ã—í–‰ì„± [{dominant_god}â†”{planet}]: {area_text} ({psych})"))

        # 1-3. ì§€ì§€-í•˜ìš°ìŠ¤ êµì°¨ ë¶„ì„ (cross_branch_house.json)
        branch_house = cache.get("cross_branch_house", {})
        if branch_house:
            branch_mapping = branch_house.get("branch_house_mapping", {})
            # Check year, month, day, hour branches
            for pillar_key in ["yearPillar", "monthPillar", "dayPillar", "hourPillar"]:
                pillar = saju_data.get(pillar_key, {})
                if not isinstance(pillar, dict):
                    continue
                branch = pillar.get("earthlyBranch", "")
                if not isinstance(branch, str):
                    branch = str(branch) if branch else ""
                branch_ko = {"å­": "ì", "ä¸‘": "ì¶•", "å¯…": "ì¸", "å¯": "ë¬˜", "è¾°": "ì§„", "å·³": "ì‚¬",
                             "åˆ": "ì˜¤", "æœª": "ë¯¸", "ç”³": "ì‹ ", "é…‰": "ìœ ", "æˆŒ": "ìˆ ", "äº¥": "í•´"}.get(branch, "")
                if branch_ko and branch_ko in branch_mapping:
                    bh_data = branch_mapping[branch_ko]
                    primary_house = bh_data.get("primary_house")
                    # Check if user has a planet in this house
                    for planet, house in planet_house_checks:
                        if str(primary_house) == house:
                            life_themes = bh_data.get("life_themes", {})
                            shared = bh_data.get("shared_energy", "")
                            pillar_names = {"yearPillar": "ë…„ì§€", "monthPillar": "ì›”ì§€", "dayPillar": "ì¼ì§€", "hourPillar": "ì‹œì§€"}
                            detailed_insights.append((6, f"âš¡ ì§€ì§€Ã—í•˜ìš°ìŠ¤ [{pillar_names[pillar_key]} {branch}â†”{planet} {house}H]: {shared}"))
                            break

        # 1-4. ì²œê°„í•©/ì§€ì§€í•© ë¶„ì„ (cross_relations_aspects.json)
        relations_aspects = cache.get("cross_relations_aspects", {})
        if relations_aspects:
            major_aspects = relations_aspects.get("major_aspects", {})
            conj = major_aspects.get("conjunction_0", {})
            # Check for ì²œê°„í•© in user's chart
            cheongan_hap = conj.get("cheongan_hap_details", {})
            year_pillar = saju_data.get("yearPillar", {})
            day_pillar = saju_data.get("dayPillar", {})
            year_stem = year_pillar.get("heavenlyStem", "") if isinstance(year_pillar, dict) else ""
            day_stem = day_pillar.get("heavenlyStem", "") if isinstance(day_pillar, dict) else ""
            # Ensure stems are strings
            if not isinstance(year_stem, str):
                year_stem = str(year_stem) if year_stem else ""
            if not isinstance(day_stem, str):
                day_stem = str(day_stem) if day_stem else ""
            # Common í•© combinations
            hap_pairs = {"ê°‘": "ê¸°", "ì„": "ê²½", "ë³‘": "ì‹ ", "ì •": "ì„", "ë¬´": "ê³„",
                         "ê¸°": "ê°‘", "ê²½": "ì„", "ì‹ ": "ë³‘", "ì„": "ì •", "ê³„": "ë¬´"}
            for stem in [year_stem, day_stem]:
                if stem and isinstance(stem, str) and stem in hap_pairs:
                    hap_key = f"{stem}{hap_pairs[stem]}í•©"
                    if hap_key in cheongan_hap:
                        hap_info = cheongan_hap[hap_key]
                        detailed_insights.append((5, f"â˜¯ï¸ ì²œê°„í•© [{hap_key}]: {hap_info.get('meaning', '')} â†’ {hap_info.get('result', '')}ê¸°ìš´ í˜•ì„±"))

        # 1-5. ì‹ ì‚´Ã—ì†Œí–‰ì„± ë§¤í•‘ (cross_shinsal_asteroids.json)
        shinsal_asteroids = cache.get("cross_shinsal_asteroids", {})
        if shinsal_asteroids:
            shinsal_mapping = shinsal_asteroids.get("major_shinsal_mapping", {})
            # Check user's shinsal from saju_data - handle various data structures
            raw_shinsals = saju_data.get("sinsal", []) or saju_data.get("shinsals", []) or saju_data.get("shinsalList", []) or []
            user_shinsals = []
            if isinstance(raw_shinsals, dict):
                # Handle {"luckyList": [{"name": "ì²œì„ê·€ì¸"}], "unluckyList": [...]} structure
                for key in ["luckyList", "unluckyList", "twelveAll", "hits"]:
                    sublist = raw_shinsals.get(key, [])
                    if isinstance(sublist, list):
                        for item in sublist:
                            if isinstance(item, dict):
                                name = item.get("name", "") or item.get("kind", "")
                                if name:
                                    user_shinsals.append(name)
                            elif isinstance(item, str):
                                user_shinsals.append(item)
                # Also try dict keys as fallback
                if not user_shinsals:
                    user_shinsals = [k for k in raw_shinsals.keys() if not k.startswith("$")]
            elif isinstance(raw_shinsals, list):
                # Handle list of dicts or list of strings
                for item in raw_shinsals:
                    if isinstance(item, dict):
                        name = item.get("name", "") or item.get("kind", "")
                        if name:
                            user_shinsals.append(name)
                    elif isinstance(item, str):
                        user_shinsals.append(item)
            for shinsal_name in user_shinsals[:3]:  # Top 3 shinsals
                if not isinstance(shinsal_name, str):
                    continue
                if shinsal_name in shinsal_mapping:
                    ss_data = shinsal_mapping[shinsal_name]
                    astro_par = ss_data.get("astro_parallel", {})
                    primary = astro_par.get("primary", "")
                    effect = ss_data.get("effect", "")
                    house_act = ss_data.get("house_activation", [])
                    # Check if user has matching planet in activated house
                    if primary and effect:
                        detailed_insights.append((6, f"â­ ì‹ ì‚´Ã—ì ì„± [{shinsal_name}â†”{primary}]: {effect}"))

        # 1-6. ê²©êµ­Ã—í•˜ìš°ìŠ¤ íŒ¨í„´ (cross_geokguk_house.json)
        geokguk_house = cache.get("cross_geokguk_house", {})
        if geokguk_house:
            junggyeok = geokguk_house.get("junggyeok_8types", {})
            user_geokguk = saju_data.get("geokguk", "") or saju_data.get("gyeokguk", "")
            if user_geokguk and user_geokguk in junggyeok:
                gk_data = junggyeok[user_geokguk]
                astro_par = gk_data.get("astro_parallel", {})
                chart_sig = gk_data.get("chart_signature", "")
                life_exp = gk_data.get("life_expression", "")
                primary = astro_par.get("primary", "")
                if primary and chart_sig:
                    detailed_insights.append((7, f"ğŸ›ï¸ ê²©êµ­Ã—ì°¨íŠ¸ [{user_geokguk}â†”{primary}]: {chart_sig}"))
                    if life_exp and theme in ["career", "focus_career", "life"]:
                        detailed_insights.append((6, f"   â†’ ì ì„±: {life_exp}"))

        # 1-7. ëŒ€ìš´Ã—í”„ë¡œê·¸ë ˆì…˜ (cross_luck_progression.json)
        luck_prog = cache.get("cross_luck_progression", {})
        if luck_prog:
            daeun_mapping = luck_prog.get("daeun_progression_mapping", {})
            sipsin_daeun = daeun_mapping.get("sipsin_daeun_astro", {})
            # Get current daeun sipsin
            current_daeun = saju_data.get("currentDaeun", {}) or saju_data.get("daeWoon", {})
            if isinstance(current_daeun, list) and len(current_daeun) > 0:
                current_daeun = current_daeun[0]
            daeun_sipsin = ""
            if isinstance(current_daeun, dict):
                daeun_sipsin = current_daeun.get("sipsin", "") or current_daeun.get("heavenlyGod", "")
                if daeun_sipsin and "ìš´" not in daeun_sipsin:
                    daeun_sipsin = daeun_sipsin + "ìš´"
            if daeun_sipsin and daeun_sipsin in sipsin_daeun:
                ld_data = sipsin_daeun[daeun_sipsin]
                saju_theme = ld_data.get("saju_theme", "")
                astro_par = ld_data.get("astro_parallel", "")
                if saju_theme:
                    detailed_insights.append((5, f"ğŸ“… ëŒ€ìš´Ã—í”„ë¡œê·¸ë ˆì…˜ [{daeun_sipsin}]: {saju_theme}"))

        # 1-8. 60ê°‘ìÃ—í•˜ëª¨ë‹‰/ë‚©ìŒ (cross_60ganji_harmonic.json)
        ganji_harmonic = cache.get("cross_60ganji_harmonic", {})
        if ganji_harmonic:
            naeum_types = ganji_harmonic.get("naeum_30_types", {})
            # Get user's day pillar naeum
            day_pillar = saju_data.get("dayPillar", {})
            day_ganji = day_pillar.get("fullStem", "") or f"{day_pillar.get('heavenlyStem', '')}{day_pillar.get('earthlyBranch', '')}"
            for naeum_name, naeum_data in naeum_types.items():
                if not isinstance(naeum_data, dict):
                    continue
                ganji_list = naeum_data.get("ganji", [])
                if any(day_ganji in g for g in ganji_list):
                    harmonic = naeum_data.get("harmonic_parallel", {})
                    personality = naeum_data.get("personality", "")
                    life_theme = naeum_data.get("life_theme", "")
                    h_primary = harmonic.get("primary", "")
                    if personality:
                        detailed_insights.append((6, f"ğŸµ ë‚©ìŒÃ—í•˜ëª¨ë‹‰ [{naeum_name}â†”{h_primary}]: {personality}"))
                    if life_theme:
                        detailed_insights.append((5, f"   â†’ ì‚¶ì˜ í…Œë§ˆ: {life_theme}"))
                    break

        # 1-9. ê³µë§Ã—ë“œë¼ì½”ë‹‰ ì¹´ë¥´ë§ˆ (cross_draconic_karma.json)
        draconic_karma = cache.get("cross_draconic_karma", {})
        if draconic_karma:
            gongmang_sn = draconic_karma.get("gongmang_south_node", {})
            branch_void = gongmang_sn.get("cross_mapping", {}).get("branch_house_void", {})
            user_gongmang = saju_data.get("gongmang", []) or saju_data.get("kongmang", [])
            if isinstance(user_gongmang, str):
                user_gongmang = [user_gongmang]
            for gm in user_gongmang[:2]:
                if not isinstance(gm, str):
                    continue
                gm_key = f"{gm}_ê³µë§"
                if gm_key in branch_void:
                    gm_data = branch_void[gm_key]
                    gm_theme = gm_data.get("theme", "")  # Use gm_theme to avoid shadowing function param
                    draconic = gm_data.get("draconic", "")
                    if gm_theme:
                        detailed_insights.append((5, f"ğŸŒ™ ê³µë§Ã—ë“œë¼ì½”ë‹‰ [{gm} ê³µë§]: {gm_theme}"))

    # 2. Planet-House detailed analysis from ALL fusion rules
    is_ko = locale == "ko"
    text_key = "text_ko" if is_ko else "text_en"

    # Determine which domains to use based on theme (expanded for all themes)
    theme_to_domain = {
        # General chat uses multiple domains
        "chat": ["career", "love", "health", "wealth"],
        "life": ["career", "love", "life_path", "wealth"],
        "life_path": ["life_path", "career", "love"],
        # Focus themes use specific domains
        "focus_career": ["career"], "career": ["career"],
        "focus_love": ["love"], "love": ["love"],
        "focus_health": ["health"], "health": ["health"],
        "focus_wealth": ["wealth"], "wealth": ["wealth"],
        "focus_family": ["family"], "family": ["family"],
        # Time-based themes
        "daily": ["daily", "health"],
        "monthly": ["monthly", "career", "love"],
        "new_year": ["new_year", "career", "love", "health"],
        "next_year": ["next_year", "career", "wealth"],
        # Compatibility
        "compatibility": ["compatibility", "love"],
    }
    domains = theme_to_domain.get(theme, ["career", "love", "health"])

    # Helper to extract text from rule (supports both new and legacy formats)
    def get_rule_text(rule: dict, prefer_ko: bool = True) -> str:
        """Extract text from rule, supporting both new (text_ko/text_en) and legacy (text) formats."""
        if prefer_ko:
            return rule.get("text_ko", rule.get("text", rule.get("text_en", "")))
        else:
            return rule.get("text_en", rule.get("text", rule.get("text_ko", "")))

    # Theme-specific emoji mapping
    domain_emoji = {
        "career": "ğŸ’¼", "love": "ğŸ’•", "health": "ğŸ¥", "wealth": "ğŸ’°",
        "family": "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦", "life_path": "ğŸŒŸ", "daily": "ğŸ“…", "monthly": "ğŸ“†",
        "compatibility": "ğŸ’‘", "new_year": "ğŸŠ", "next_year": "ğŸ”®"
    }

    # Apply detailed fusion rules for ALL domains
    for domain in domains:
        if domain not in fusion_rules:
            continue
        rules = fusion_rules[domain]
        emoji = domain_emoji.get(domain, "âœ¨")

        # A. Check planet-house rules (e.g., rule_sun_10, rule_venus_7)
        # Also check legacy format: rule_1, rule_2, etc. with "when" arrays
        for planet, house in planet_house_checks:
            # New format: rule_{planet}_{house}
            rule_key = f"rule_{planet}_{house}"
            if rule_key in rules:
                rule = rules[rule_key]
                text = get_rule_text(rule, is_ko)
                advice = rule.get("advice_ko", "")
                timing = rule.get("timing", "")
                saju_link = rule.get("saju_link", "")
                weight = rule.get("weight", 5)

                if text:
                    insight = f"{emoji} {text[:150]}"
                    if timing and is_ko:
                        insight += f"\nâ° ì‹œê¸°: {timing}"
                    if advice and is_ko:
                        insight += f"\nğŸ’¡ ì¡°ì–¸: {advice}"
                    if saju_link and dominant_god and dominant_god in saju_link:
                        insight += f"\nğŸ”— ì‚¬ì£¼ì—°ê²°: {saju_link}"
                    detailed_insights.append((weight, insight))

            # Legacy format: search for rules with "when" arrays containing planet and house
            for rule_key, rule in rules.items():
                if not isinstance(rule, dict):
                    continue
                when = rule.get("when", [])
                if isinstance(when, list) and planet in when and house in when:
                    text = get_rule_text(rule, is_ko)
                    weight = rule.get("weight", 4)
                    if text and len(text) > 10:
                        # Translate common English patterns to Korean if needed
                        if is_ko and text.startswith(planet.capitalize()):
                            text = f"{planet.capitalize()} {house}í•˜ìš°ìŠ¤: {text.split(':', 1)[-1].strip() if ':' in text else text}"
                        insight = f"{emoji} {text[:120]}"
                        detailed_insights.append((weight, insight))
                        break  # Only one match per planet-house combo per domain

        # B. Health-specific: Element imbalance analysis
        if domain == "health" and element_counts:
            for elem_ko, elem_en in element_to_flag.items():
                count = element_counts.get(elem_ko, 0)
                # Check for depleted elements (0 count)
                if count == 0:
                    flag_key = f"{elem_en}_zero"
                    for rule_key, rule in rules.items():
                        when = rule.get("when", [])
                        if isinstance(when, list) and flag_key in when:
                            text = get_rule_text(rule, is_ko)
                            weight = rule.get("weight", 4)
                            if text:
                                ko_elem_names = {"wood": "ëª©(æœ¨)", "fire": "í™”(ç«)", "earth": "í† (åœŸ)", "metal": "ê¸ˆ(é‡‘)", "water": "ìˆ˜(æ°´)"}
                                elem_name = ko_elem_names.get(elem_en, elem_en)
                                insight = f"ğŸ¥ {elem_name} ë¶€ì¡±: {text[:100]}" if is_ko else f"ğŸ¥ {elem_en} depleted: {text[:100]}"
                                detailed_insights.append((weight, insight))
                                break
                # Check for excess elements (high count, e.g., >= 3)
                elif count >= 3:
                    flag_key = f"{elem_en}_high"
                    for rule_key, rule in rules.items():
                        when = rule.get("when", [])
                        if isinstance(when, list) and flag_key in when:
                            text = get_rule_text(rule, is_ko)
                            weight = rule.get("weight", 3)
                            if text:
                                ko_elem_names = {"wood": "ëª©(æœ¨)", "fire": "í™”(ç«)", "earth": "í† (åœŸ)", "metal": "ê¸ˆ(é‡‘)", "water": "ìˆ˜(æ°´)"}
                                elem_name = ko_elem_names.get(elem_en, elem_en)
                                insight = f"ğŸ¥ {elem_name} ê³¼ë‹¤: {text[:100]}" if is_ko else f"ğŸ¥ {elem_en} excess: {text[:100]}"
                                detailed_insights.append((weight, insight))
                                break

        # C. Wealth-specific: Money house analysis (2, 8, 10, 11)
        if domain == "wealth":
            money_houses = ["2", "8", "10", "11"]
            for planet, house in planet_house_checks:
                if house in money_houses:
                    for rule_key, rule in rules.items():
                        when = rule.get("when", [])
                        if isinstance(when, list) and planet in when and house in when:
                            text = get_rule_text(rule, is_ko)
                            weight = rule.get("weight", 5)
                            if text:
                                insight = f"ğŸ’° {text[:120]}"
                                detailed_insights.append((weight, insight))
                                break

        # D. Check Ten Gods rules (ì‹­ì‹  ê¸°ë°˜ ë¶„ì„) - for career/love domains
        if dominant_god and domain in ["career", "love"]:
            god_mapping = {
                "ì •ê´€": "jeonggwan", "í¸ê´€": "pyeongwan",
                "ì •ì¬": "jeongje", "í¸ì¬": "pyeonje",
                "ìƒê´€": "sangwan", "ì‹ì‹ ": "sikshin",
                "ì •ì¸": "jeongin", "í¸ì¸": "pyeonin",
                "ë¹„ê²¬": "bigyeon", "ê²ì¬": "geopje",
            }
            mapped_god = god_mapping.get(dominant_god, "")
            if mapped_god:
                for rule_key, rule in rules.items():
                    if mapped_god in rule_key.lower():
                        text = get_rule_text(rule, is_ko)
                        advice = rule.get("advice_ko", "")
                        weight = rule.get("weight", 5)
                        if text:
                            insight = f"ğŸ“Š ì‹­ì‹ ë¶„ì„ [{dominant_god}]: {text[:120]}"
                            if advice:
                                insight += f"\nğŸ’¡ {advice}"
                            detailed_insights.append((weight, insight))
                            break

        # E. Check element cross-rules (ì‚¬ì£¼ ì˜¤í–‰ Ã— ì ì„± ì›ì†Œ)
        if dm_element and sun_element:
            for rule_key, rule in rules.items():
                if "cross" in rule_key and dm_element in rule_key and sun_element in rule_key:
                    text = get_rule_text(rule, is_ko)
                    advice = rule.get("advice_ko", "")
                    weight = rule.get("weight", 6)
                    if text:
                        insight = f"ğŸ”® ìœµí•©ë¶„ì„ [{dm_element}+{sun_element}]: {text[:120]}"
                        if advice:
                            insight += f"\nğŸ’¡ {advice}"
                        detailed_insights.append((weight, insight))
                        break

    # 3. GraphRAG theme rules (keyword match, no embedding) - INSTANT
    if HAS_GRAPH_RAG:
        try:
            graph_rag = get_graph_rag()

            # Build facts string for rule matching
            facts_parts = [theme, daymaster, dm_element, dominant]
            if sun_sign_en:
                facts_parts.append(sun_sign_en.lower())
            if moon_sign_en:
                facts_parts.append(moon_sign_en.lower())

            # Add planets in houses
            for planet, house in planet_house_checks:
                facts_parts.extend([planet, house, f"{planet} {house}"])

            facts_str = " ".join(filter(None, facts_parts))

            # Apply rules from each domain (instant keyword match)
            for domain in domains:
                if hasattr(graph_rag, '_apply_rules'):
                    matched = graph_rag._apply_rules(domain, facts_str)
                    if matched:
                        results.extend(matched[:2])

        except Exception as e:
            logger.warning(f"[CROSS-ANALYSIS] GraphRAG rules failed: {e}")

    # Sort detailed insights by weight (highest first) and deduplicate
    detailed_insights.sort(key=lambda x: -x[0])
    seen_texts = set()
    unique_insights = []
    for weight, insight in detailed_insights:
        # Use first 50 chars as dedup key
        key = insight[:50]
        if key not in seen_texts:
            seen_texts.add(key)
            unique_insights.append(insight)
        if len(unique_insights) >= 5:  # Take top 5 unique insights
            break

    # Combine all results: basic cross-analysis + detailed insights
    all_results = results + unique_insights

    # Log summary
    logger.info(f"[CROSS-ANALYSIS] Generated {len(all_results)} insights for theme={theme}, domains={domains}")

    return "\n\n".join(all_results[:8]) if all_results else ""


# ===============================================================
# ğŸ¯ THEME-SPECIFIC FUSION RULES - Daily/Monthly/Yearly guidance
# ===============================================================

def get_theme_fusion_rules(saju_data: dict, astro_data: dict, theme: str, locale: str = "ko", birth_year: int = None) -> str:
    """
    Get theme-specific fusion rules based on counselor theme.
    Applies rules from daily.json, monthly.json, new_year.json, next_year.json, family.json, life_path.json.

    Returns actionable insights tailored to the specific counseling theme.
    """
    from pathlib import Path
    from datetime import datetime

    results = []
    is_ko = locale == "ko"
    now = datetime.now()

    # Theme to rule file mapping
    theme_file_map = {
        "focus_overall": ["daily", "monthly", "life_path", "new_year"],
        "focus_career": ["daily", "monthly", "career"],
        "focus_love": ["daily", "monthly", "love", "family"],
        "focus_health": ["daily", "monthly", "health"],
        "focus_wealth": ["daily", "monthly", "wealth"],
        "focus_family": ["daily", "monthly", "family"],
        "focus_2025": ["new_year", "next_year", "monthly"],
        "focus_compatibility": ["compatibility", "love", "family"],
        "chat": ["daily", "life_path"],
    }

    rule_files = theme_file_map.get(theme, ["daily", "life_path"])

    # Load fusion rules
    rules_dir = Path(__file__).parent.parent / "data" / "graph" / "rules" / "fusion"
    loaded_rules = {}
    for rf in rule_files:
        rule_path = rules_dir / f"{rf}.json"
        if rule_path.exists():
            try:
                with open(rule_path, "r", encoding="utf-8") as f:
                    loaded_rules[rf] = json.load(f)
            except Exception as e:
                logger.warning(f"[THEME-FUSION] Failed to load {rf}.json: {e}")

    # Extract chart data
    dm_data = saju_data.get("dayMaster", {})
    if not isinstance(dm_data, dict):
        dm_data = {}
    daymaster = dm_data.get("heavenlyStem") or dm_data.get("name", "")
    dm_element = dm_data.get("element", "")
    ten_gods = saju_data.get("tenGods", {})
    if not isinstance(ten_gods, dict):
        ten_gods = {}
    dominant_god = ten_gods.get("dominant", "")
    # Ensure dominant_god is a string (not dict)
    if isinstance(dominant_god, dict):
        dominant_god = dominant_god.get("name", "") or dominant_god.get("ko", "") or ""
    elif not isinstance(dominant_god, str):
        dominant_god = str(dominant_god) if dominant_god else ""

    # Astrology data - safely handle non-dict values
    sun_data = astro_data.get("sun", {})
    sun_sign = sun_data.get("sign", "") if isinstance(sun_data, dict) else ""
    moon_data = astro_data.get("moon", {})
    moon_sign = moon_data.get("sign", "") if isinstance(moon_data, dict) else ""

    # Calculate age if birth_year provided
    current_age = now.year - birth_year if birth_year else None

    # Helper to get localized text
    def get_text(rule):
        if is_ko:
            return rule.get("text_ko", rule.get("text", ""))
        return rule.get("text_en", rule.get("text", ""))

    def get_advice(rule):
        return rule.get("advice_ko", "") if is_ko else rule.get("advice_en", "")

    # ===============================================================
    # 1. DAILY RULES - Moon phases, planetary transits, day energy
    # ===============================================================
    if "daily" in loaded_rules:
        daily_rules = loaded_rules["daily"]

        # Check moon phase (simplified - use current day of lunar month)
        lunar_day = now.day % 30
        if lunar_day <= 3:  # New moon period
            rule = daily_rules.get("rule_new_moon_day")
            if rule:
                results.append(f"ğŸŒ‘ {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")
        elif 13 <= lunar_day <= 17:  # Full moon period
            rule = daily_rules.get("rule_full_moon_day")
            if rule:
                results.append(f"ğŸŒ• {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check daily Ten God energy (ilgan)
        if dominant_god:
            god_category = ""
            if dominant_god in ["ë¹„ê²¬", "ê²ì¬"]:
                god_category = "bigyeop"
            elif dominant_god in ["ì‹ì‹ ", "ìƒê´€"]:
                god_category = "siksang"
            elif dominant_god in ["ì •ì¬", "í¸ì¬"]:
                god_category = "jaesung"
            elif dominant_god in ["ì •ê´€", "í¸ê´€"]:
                god_category = "gwansung"
            elif dominant_god in ["ì •ì¸", "í¸ì¸"]:
                god_category = "insung"

            if god_category:
                rule_key = f"rule_ilgan_{god_category}"
                rule = daily_rules.get(rule_key)
                if rule:
                    results.append(f"ğŸ“… ì˜¤ëŠ˜ì˜ ê¸°ìš´ [{dominant_god}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 2. MONTHLY RULES - Seasonal energy, monthly transits
    # ===============================================================
    if "monthly" in loaded_rules:
        monthly_rules = loaded_rules["monthly"]

        # Check monthly Ten God energy (wolgon)
        if dominant_god:
            god_category = ""
            if dominant_god in ["ë¹„ê²¬", "ê²ì¬"]:
                god_category = "bigyeop"
            elif dominant_god in ["ì‹ì‹ ", "ìƒê´€"]:
                god_category = "siksang"
            elif dominant_god in ["ì •ì¬", "í¸ì¬"]:
                god_category = "jaesung"
            elif dominant_god in ["ì •ê´€", "í¸ê´€"]:
                god_category = "gwansung"
            elif dominant_god in ["ì •ì¸", "í¸ì¸"]:
                god_category = "insung"

            if god_category:
                rule_key = f"rule_wolgon_{god_category}"
                rule = monthly_rules.get(rule_key)
                if rule:
                    results.append(f"ğŸ“† ì´ë²ˆ ë‹¬ ì—ë„ˆì§€ [{dominant_god}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check for eclipse month (simple approximation - eclipse seasons)
        if now.month in [3, 4, 9, 10]:  # Approximate eclipse seasons
            rule = monthly_rules.get("rule_eclipse_month")
            if rule and rule.get("weight", 0) >= 8:
                results.append(f"ğŸŒ“ {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 3. NEW YEAR / 2025 RULES - Annual themes, daeun
    # ===============================================================
    if "new_year" in loaded_rules and theme in ["focus_2025", "focus_overall"]:
        new_year_rules = loaded_rules["new_year"]

        # Check daeun (10-year luck cycle) based on dominant god
        if dominant_god:
            god_category = ""
            if dominant_god in ["ë¹„ê²¬", "ê²ì¬"]:
                god_category = "bigyeop"
            elif dominant_god in ["ì‹ì‹ ", "ìƒê´€"]:
                god_category = "siksang"
            elif dominant_god in ["ì •ì¬", "í¸ì¬"]:
                god_category = "jaesung"
            elif dominant_god in ["ì •ê´€", "í¸ê´€"]:
                god_category = "gwansung"
            elif dominant_god in ["ì •ì¸", "í¸ì¸"]:
                god_category = "insung"

            if god_category:
                rule_key = f"rule_daeun_{god_category}"
                rule = new_year_rules.get(rule_key)
                if rule:
                    results.append(f"ğŸŠ 2025ë…„ ëŒ€ìš´ [{dominant_god}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check year pillar harmony/clash (simplified)
        # 2025 is ì„ì‚¬ë…„ (ä¹™å·³å¹´) - Wood Snake
        year_snake_compatible = ["ì", "ì¶•", "ì‹ ", "ìœ "]  # Generally harmonious
        year_snake_clash = ["í•´"]  # ì‚¬í•´ì¶©

        day_pillar_data = saju_data.get("dayPillar", {})
        day_branch = day_pillar_data.get("earthlyBranch", "") if isinstance(day_pillar_data, dict) else ""
        if not isinstance(day_branch, str):
            day_branch = str(day_branch) if day_branch else ""
        branch_ko = {"å­": "ì", "ä¸‘": "ì¶•", "å¯…": "ì¸", "å¯": "ë¬˜", "è¾°": "ì§„", "å·³": "ì‚¬",
                     "åˆ": "ì˜¤", "æœª": "ë¯¸", "ç”³": "ì‹ ", "é…‰": "ìœ ", "æˆŒ": "ìˆ ", "äº¥": "í•´"}.get(day_branch, "")

        if branch_ko in year_snake_compatible:
            rule = new_year_rules.get("rule_year_pillar_match")
            if rule:
                results.append(f"âœ¨ 2025ë…„ ìš´ì„¸ ì¡°í™”: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")
        elif branch_ko in year_snake_clash:
            rule = new_year_rules.get("rule_year_pillar_clash")
            if rule:
                results.append(f"âš¡ 2025ë…„ ë³€í™”ì˜ í•´: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 4. NEXT YEAR RULES - Future planning
    # ===============================================================
    if "next_year" in loaded_rules and theme in ["focus_2025"]:
        next_year_rules = loaded_rules["next_year"]

        # Seun (yearly luck) based on dominant god
        if dominant_god:
            god_category = ""
            if dominant_god in ["ë¹„ê²¬", "ê²ì¬"]:
                god_category = "bigyeop"
            elif dominant_god in ["ì‹ì‹ ", "ìƒê´€"]:
                god_category = "siksang"
            elif dominant_god in ["ì •ì¬", "í¸ì¬"]:
                god_category = "jaesung"
            elif dominant_god in ["ì •ê´€", "í¸ê´€"]:
                god_category = "gwansung"
            elif dominant_god in ["ì •ì¸", "í¸ì¸"]:
                god_category = "insung"

            if god_category:
                rule_key = f"rule_seun_{god_category}"
                rule = next_year_rules.get(rule_key)
                if rule:
                    results.append(f"ğŸ”® 2026ë…„ ì„¸ìš´ ì „ë§ [{dominant_god}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 5. FAMILY RULES - Relationship dynamics
    # ===============================================================
    if "family" in loaded_rules and theme in ["focus_love", "focus_family", "focus_compatibility"]:
        family_rules = loaded_rules["family"]

        # Check moon house position for family dynamics
        moon_house = astro_data.get("moon", {}).get("house")
        if moon_house:
            house_num = str(moon_house).replace("H", "")
            rule_key = f"rule_moon_{house_num}"
            rule = family_rules.get(rule_key)
            if rule:
                results.append(f"ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ê°€ì¡± ê´€ê³„ [ë‹¬ {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check venus for relationships
        venus_house = astro_data.get("venus", {}).get("house")
        if venus_house:
            house_num = str(venus_house).replace("H", "")
            rule_key = f"rule_venus_{house_num}"
            rule = family_rules.get(rule_key)
            if rule:
                results.append(f"ğŸ’• ê´€ê³„ ì—ë„ˆì§€ [ê¸ˆì„± {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 5-1. HEALTH RULES - Element balance, 6th/12th house
    # ===============================================================
    if "health" in loaded_rules and theme == "focus_health":
        health_rules = loaded_rules["health"]

        # Check element deficiencies
        element_counts = saju_data.get("elementCounts", {})
        if not isinstance(element_counts, dict):
            element_counts = {}
        element_map = {"æœ¨": "wood", "ç«": "fire", "åœŸ": "earth", "é‡‘": "metal", "æ°´": "water"}

        for elem_ko, elem_en in element_map.items():
            count_val = element_counts.get(elem_ko, 0)
            count = count_val if isinstance(count_val, (int, float)) else 0
            if count == 0:
                rule_key = f"rule_{elem_en}_zero"
                rule = health_rules.get(rule_key)
                if rule:
                    results.append(f"âš•ï¸ ì˜¤í–‰ ë¶€ì¡± [{elem_ko}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")
            elif count >= 3:
                rule_key = f"rule_{elem_en}_high"
                rule = health_rules.get(rule_key)
                if rule:
                    results.append(f"âš•ï¸ ì˜¤í–‰ ê³¼ë‹¤ [{elem_ko}]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check health houses (6, 12)
        for planet in ["mars", "saturn", "moon", "neptune", "jupiter", "pluto"]:
            planet_data = astro_data.get(planet, {})
            if not isinstance(planet_data, dict):
                continue
            house = planet_data.get("house")
            if house:
                house_num = str(house).replace("H", "")
                if house_num in ["6", "12", "1"]:
                    rule_key = f"rule_{planet}_{house_num}"
                    rule = health_rules.get(rule_key)
                    if rule:
                        results.append(f"ğŸ¥ ê±´ê°• ê´€ë¦¬ [{planet} {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 5-2. WEALTH RULES - Money houses, financial potential
    # ===============================================================
    if "wealth" in loaded_rules and theme == "focus_wealth":
        wealth_rules = loaded_rules["wealth"]

        # Check money houses (2, 8, 10, 11)
        for planet in ["jupiter", "venus", "saturn", "uranus", "pluto", "moon", "mars", "mercury", "sun"]:
            planet_data = astro_data.get(planet, {})
            if not isinstance(planet_data, dict):
                continue
            house = planet_data.get("house")
            if house:
                house_num = str(house).replace("H", "")
                if house_num in ["2", "8", "10", "11"]:
                    rule_key = f"rule_{planet}_{house_num}"
                    rule = wealth_rules.get(rule_key)
                    if rule:
                        results.append(f"ğŸ’° ì¬ë¬¼ìš´ [{planet} {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check jaesung (ì¬ì„±) strength
        ten_gods_count = saju_data.get("tenGodsCount", {})
        if not isinstance(ten_gods_count, dict):
            ten_gods_count = {}
        jeongjae_val = ten_gods_count.get("ì •ì¬", 0)
        pyeonjae_val = ten_gods_count.get("í¸ì¬", 0)
        # Ensure values are numeric
        jeongjae_count = jeongjae_val if isinstance(jeongjae_val, (int, float)) else 0
        pyeonjae_count = pyeonjae_val if isinstance(pyeonjae_val, (int, float)) else 0
        jaesung_count = jeongjae_count + pyeonjae_count
        if jaesung_count >= 2:
            rule = wealth_rules.get("rule_jaesung_strong")
            if rule:
                results.append(f"ğŸ’ ì¬ì„± ë¶„ì„: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")
        elif jaesung_count == 0:
            rule = wealth_rules.get("rule_jaesung_weak")
            if rule:
                results.append(f"ğŸ’ ì¬ì„± ë¶„ì„: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # ===============================================================
    # 6. LIFE PATH RULES - Soul purpose, individuation
    # ===============================================================
    if "life_path" in loaded_rules and theme in ["focus_overall", "chat"]:
        life_path_rules = loaded_rules["life_path"]

        # Check sun house for life purpose
        sun_data = astro_data.get("sun", {})
        sun_house = sun_data.get("house") if isinstance(sun_data, dict) else None
        if sun_house:
            house_num = str(sun_house).replace("H", "")
            rule_key = f"rule_sun_{house_num}"
            rule = life_path_rules.get(rule_key)
            if rule:
                results.append(f"ğŸŒŸ ì¸ìƒ ë°©í–¥ [íƒœì–‘ {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

        # Check north node for karmic direction
        north_node = astro_data.get("northNode", {}) or astro_data.get("north_node", {})
        if not isinstance(north_node, dict):
            north_node = {}
        nn_house = north_node.get("house")
        if nn_house:
            house_num = str(nn_house).replace("H", "")
            rule_key = f"rule_north_node_{house_num}"
            rule = life_path_rules.get(rule_key)
            if rule:
                results.append(f"ğŸ§­ ì˜í˜¼ì˜ ì„±ì¥ ë°©í–¥ [ë¶êµì  {house_num}í•˜ìš°ìŠ¤]: {get_text(rule)}\nğŸ’¡ {get_advice(rule)}")

    # Limit results and format
    if results:
        logger.info(f"[THEME-FUSION] Generated {len(results)} theme-specific insights for {theme}")
        return "\n\n".join(results[:5])  # Top 5 insights

    return ""


# ===============================================================
# ğŸš€ SESSION RAG CACHE - Pre-computed RAG data for fast chat
# ===============================================================
import threading
from datetime import datetime, timedelta

# In-memory session cache: session_id -> {data, created_at, last_accessed}
_SESSION_RAG_CACHE = {}
_SESSION_CACHE_LOCK = threading.Lock()
SESSION_CACHE_TTL_MINUTES = 60  # Session data expires after 60 minutes
SESSION_CACHE_MAX_SIZE = 50  # Max number of sessions to cache (LRU eviction)


def _cleanup_expired_sessions():
    """Remove expired session data."""
    now = datetime.now()
    expired = []
    with _SESSION_CACHE_LOCK:
        for sid, data in _SESSION_RAG_CACHE.items():
            if now - data.get("created_at", now) > timedelta(minutes=SESSION_CACHE_TTL_MINUTES):
                expired.append(sid)
        for sid in expired:
            del _SESSION_RAG_CACHE[sid]
    if expired:
        logger.info(f"[SESSION-CACHE] Cleaned up {len(expired)} expired sessions")


def _evict_lru_sessions(keep_count: int = SESSION_CACHE_MAX_SIZE):
    """Evict least recently used sessions to maintain cache size."""
    with _SESSION_CACHE_LOCK:
        if len(_SESSION_RAG_CACHE) <= keep_count:
            return
        # Sort by last_accessed time (oldest first)
        sorted_sessions = sorted(
            _SESSION_RAG_CACHE.items(),
            key=lambda x: x[1].get("last_accessed", x[1].get("created_at", datetime.min))
        )
        # Evict oldest sessions until we're under the limit
        evict_count = len(_SESSION_RAG_CACHE) - keep_count
        for sid, _ in sorted_sessions[:evict_count]:
            del _SESSION_RAG_CACHE[sid]
        logger.info(f"[SESSION-CACHE] LRU evicted {evict_count} sessions, {len(_SESSION_RAG_CACHE)} remaining")


def prefetch_all_rag_data(saju_data: dict, astro_data: dict, theme: str = "chat", locale: str = "ko") -> dict:
    """
    Pre-fetch relevant data from ALL RAG systems for a user's chart.
    Uses parallel execution for ~2-3x speedup.

    Returns:
        Dict with all pre-fetched RAG data
    """
    start_time = time.time()
    result = {
        "graph_nodes": [],
        "graph_context": "",
        "corpus_quotes": [],
        "persona_context": {},
        "cross_analysis": "",
    }

    # Build query from chart data (support both "heavenlyStem" and "name" for dayMaster)
    dm_data = saju_data.get("dayMaster", {})
    daymaster = dm_data.get("heavenlyStem") or dm_data.get("name", "")
    dm_element = dm_data.get("element", "")
    sun_sign = astro_data.get("sun", {}).get("sign", "")
    moon_sign = astro_data.get("moon", {}).get("sign", "")
    dominant = saju_data.get("dominantElement", "")

    # Build comprehensive query for embedding search
    query_parts = [theme, daymaster, dm_element, dominant]
    if sun_sign:
        query_parts.append(sun_sign)
    if moon_sign:
        query_parts.append(moon_sign)

    # Add planets and houses
    for planet in ["sun", "moon", "mercury", "venus", "mars", "jupiter", "saturn"]:
        p_data = astro_data.get(planet, {})
        if p_data.get("sign"):
            query_parts.append(p_data["sign"])
        if p_data.get("house"):
            query_parts.append(f"{planet} {p_data['house']}í•˜ìš°ìŠ¤")

    query = " ".join(filter(None, query_parts))
    logger.info(f"[PREFETCH] Query: {query[:100]}...")

    # Build facts dict for graph query (shared across tasks)
    facts = {
        "daymaster": daymaster,
        "element": dm_element,
        "sun_sign": sun_sign,
        "moon_sign": moon_sign,
        "theme": theme,
    }

    # Theme concepts for Jung quotes - ENHANCED with more keywords
    theme_concepts = {
        "career": "vocation calling work purpose self-realization individuation hero journey ì†Œëª… ì§ì—… ìì•„ì‹¤í˜„ ì˜ì›… ì—¬ì • ì‚¬ëª…",
        "love": "anima animus relationship shadow projection intimacy attachment ì•„ë‹ˆë§ˆ ì•„ë‹ˆë¬´ìŠ¤ ê·¸ë¦¼ì íˆ¬ì‚¬ ì¹œë°€ê° ê´€ê³„ ì‚¬ë‘",
        "health": "psyche wholeness integration healing body-mind ì¹˜ìœ  í†µí•© ì „ì²´ì„± ì‹¬ì‹  íšŒë³µ",
        "life_path": "individuation self persona shadow meaning transformation ê°œì„±í™” ìì•„ í˜ë¥´ì†Œë‚˜ ì˜ë¯¸ ë³€í™˜ ì„±ì¥",
        "wealth": "abundance value meaning purpose security prosperity ê°€ì¹˜ ì˜ë¯¸ ëª©ì  ì•ˆì • í’ìš”",
        "family": "complex archetype mother father inner child ì½¤í”Œë ‰ìŠ¤ ì›í˜• ë¶€ëª¨ ë‚´ë©´ì•„ì´ ê°€ì¡±",
        "chat": "self-discovery meaning crisis growth ìê¸°ë°œê²¬ ì˜ë¯¸ ìœ„ê¸° ì„±ì¥",
        "focus_career": "vocation calling work purpose self-realization ì†Œëª… ì§ì—… ìì•„ì‹¤í˜„ ì§„ë¡œ",
    }

    # --- Pre-load RAG instances (thread-safe) ---
    # SentenceTransformer encode() is NOT thread-safe, so we must load
    # instances in main thread and run queries SEQUENTIALLY
    _graph_rag_inst = get_graph_rag() if HAS_GRAPH_RAG else None
    _corpus_rag_inst = get_corpus_rag() if HAS_CORPUS_RAG else None
    _persona_rag_inst = get_persona_embed_rag() if HAS_PERSONA_EMBED else None
    _domain_rag_inst = get_domain_rag() if HAS_DOMAIN_RAG else None

    # --- Execute RAG fetches SEQUENTIALLY (thread-safe) ---
    # GraphRAG
    try:
        if _graph_rag_inst:
            graph_result = _graph_rag_inst.query(
                facts, top_k=20,
                domain_priority=theme if theme in _graph_rag_inst.rules else "career"
            )
            result["graph_nodes"] = graph_result.get("matched_nodes", [])[:15]
            result["graph_context"] = graph_result.get("context_text", "")[:2000]
            if graph_result.get("rule_summary"):
                result["graph_rules"] = graph_result.get("rule_summary", [])[:5]
            logger.info(f"[PREFETCH] GraphRAG: {len(result['graph_nodes'])} nodes")
    except Exception as e:
        logger.warning(f"[PREFETCH] GraphRAG failed: {e}")

    # CorpusRAG (Jung quotes) - ENHANCED: fetch more quotes with diverse concepts
    try:
        if _corpus_rag_inst:
            jung_query_parts = [theme_concepts.get(theme, theme), query[:100]]
            jung_query = " ".join(jung_query_parts)
            # Primary theme-based quotes
            quotes = _corpus_rag_inst.search(jung_query, top_k=6, min_score=0.12)

            # Also fetch general wisdom quotes for variety
            general_queries = ["individuation growth ê°œì„±í™” ì„±ì¥", "shadow integration ê·¸ë¦¼ì í†µí•©"]
            for gq in general_queries:
                try:
                    extra_quotes = _corpus_rag_inst.search(gq, top_k=2, min_score=0.15)
                    quotes.extend(extra_quotes)
                except Exception:
                    pass

            # Deduplicate and limit
            seen = set()
            unique_quotes = []
            for q in quotes:
                key = q.get("quote_kr", "") or q.get("quote_en", "")
                if key and key not in seen:
                    seen.add(key)
                    unique_quotes.append(q)
                if len(unique_quotes) >= 8:
                    break

            result["corpus_quotes"] = [
                {
                    "text_ko": q.get("quote_kr", ""),
                    "text_en": q.get("quote_en", ""),
                    "source": q.get("source", ""),
                    "concept": q.get("concept", ""),
                    "score": q.get("score", 0)
                }
                for q in unique_quotes
            ]
            logger.info(f"[PREFETCH] CorpusRAG: {len(result['corpus_quotes'])} quotes (enhanced)")
    except Exception as e:
        logger.warning(f"[PREFETCH] CorpusRAG failed: {e}")

    # PersonaEmbedRAG
    try:
        if _persona_rag_inst:
            persona_result = _persona_rag_inst.get_persona_context(query, top_k=5)
            result["persona_context"] = {
                "jung": persona_result.get("jung_insights", [])[:5],
                "stoic": persona_result.get("stoic_insights", [])[:5],
            }
            logger.info(f"[PREFETCH] PersonaEmbedRAG: {persona_result.get('total_matched', 0)} matches")
    except Exception as e:
        logger.warning(f"[PREFETCH] PersonaEmbedRAG failed: {e}")

    # Cross-analysis (no ML, thread-safe) - pass locale for proper language
    try:
        result["cross_analysis"] = get_cross_analysis_for_chart(saju_data, astro_data, theme, locale)
    except Exception as e:
        logger.warning(f"[PREFETCH] Cross-analysis failed: {e}")

    # DomainRAG - ë„ë©”ì¸ë³„ ì „ë¬¸ ì§€ì‹ (ì‚¬ì£¼/ì ì„± í•´ì„ ì›ì¹™ ë“±)
    try:
        if _domain_rag_inst:
            # í…Œë§ˆì— ë§ëŠ” ë„ë©”ì¸ ê²€ìƒ‰
            domain_map = {
                "career": "career", "love": "love", "health": "health",
                "wealth": "wealth", "family": "family", "life_path": "life",
                "focus_career": "career", "focus_love": "love",
            }
            domain = domain_map.get(theme, "life")
            domain_results = _domain_rag_inst.search(domain, query[:200], top_k=5)
            result["domain_knowledge"] = domain_results[:5] if domain_results else []
            logger.info(f"[PREFETCH] DomainRAG: {len(result.get('domain_knowledge', []))} results")
    except Exception as e:
        logger.warning(f"[PREFETCH] DomainRAG failed: {e}")

    elapsed = time.time() - start_time
    logger.info(f"[PREFETCH] All RAG data prefetched in {elapsed:.2f}s (sequential)")
    result["prefetch_time_ms"] = int(elapsed * 1000)

    return result


def get_session_rag_cache(session_id: str) -> dict:
    """Get cached RAG data for a session. Updates last_accessed for LRU."""
    with _SESSION_CACHE_LOCK:
        cache_entry = _SESSION_RAG_CACHE.get(session_id)
        if cache_entry:
            # Check if expired
            if datetime.now() - cache_entry.get("created_at", datetime.now()) > timedelta(minutes=SESSION_CACHE_TTL_MINUTES):
                del _SESSION_RAG_CACHE[session_id]
                return None
            # Update last_accessed for LRU tracking
            cache_entry["last_accessed"] = datetime.now()
            return cache_entry.get("data")
    return None


def set_session_rag_cache(session_id: str, data: dict):
    """Store RAG data in session cache with LRU eviction."""
    now = datetime.now()
    with _SESSION_CACHE_LOCK:
        _SESSION_RAG_CACHE[session_id] = {
            "data": data,
            "created_at": now,
            "last_accessed": now,
        }
    # LRU eviction if cache is too large
    if len(_SESSION_RAG_CACHE) > SESSION_CACHE_MAX_SIZE:
        _cleanup_expired_sessions()  # First remove expired
        _evict_lru_sessions()  # Then evict LRU if still over limit


# ===============================================================
# ğŸš€ MODEL WARMUP - Preload models on startup for faster first request
# ===============================================================
def warmup_models():
    """Preload all singleton models and caches on startup."""
    logger.info("ğŸ”¥ Starting model warmup...")
    start = time.time()

    try:
        # 0. Cross-analysis cache (instant, no ML)
        _load_cross_analysis_cache()

        # 1. SentenceTransformer model + GraphRAG
        if HAS_GRAPH_RAG:
            model = get_model()
            logger.info(f"  âœ… SentenceTransformer loaded: {model.get_sentence_embedding_dimension()}d")

            # 2. GraphRAG with embeddings
            rag = get_graph_rag()
            logger.info(f"  âœ… GraphRAG loaded: {len(rag.graph.nodes())} nodes")

        # 3. Corpus RAG (Jung quotes)
        if HAS_CORPUS_RAG:
            corpus = get_corpus_rag()
            logger.info(f"  âœ… CorpusRAG loaded")

        # 4. Persona embeddings (if available)
        if HAS_PERSONA_EMBED:
            persona = get_persona_embed_rag()
            logger.info(f"  âœ… PersonaEmbedRAG loaded")

        # 5. Tarot RAG (if available)
        if HAS_TAROT:
            tarot = get_tarot_hybrid_rag()
            logger.info(f"  âœ… TarotHybridRAG loaded")

        # 6. Dream RAG (for faster dream interpretation)
        try:
            from backend_ai.app.dream_logic import get_dream_embed_rag
            dream_rag = get_dream_embed_rag()
            # Warmup query to pre-compute any lazy embeddings
            _ = dream_rag.search("ê¿ˆ í•´ì„ í…ŒìŠ¤íŠ¸", top_k=1)
            logger.info(f"  âœ… DreamEmbedRAG loaded and warmed up")
        except Exception as dream_err:
            logger.warning(f"  âš ï¸ DreamEmbedRAG warmup failed: {dream_err}")

        # 7. Redis cache connection
        cache = get_cache()
        logger.info(f"  âœ… Redis cache: {'connected' if cache.enabled else 'memory fallback'}")

        elapsed = time.time() - start
        logger.info(f"ğŸ”¥ Model warmup completed in {elapsed:.2f}s")

    except Exception as e:
        logger.warning(f"âš ï¸ Warmup error (non-fatal): {e}")


# Auto-warmup on import if WARMUP_ON_START is set (for Gunicorn/production)
if os.getenv("WARMUP_ON_START", "").lower() in ("1", "true", "yes"):
    warmup_models()

# Simple token gate + rate limiting
ADMIN_TOKEN = os.getenv("ADMIN_API_TOKEN")
RATE_LIMIT = int(os.getenv("API_RATE_PER_MIN", "60"))
RATE_WINDOW_SECONDS = 60
_rate_state = defaultdict(list)  # ip -> timestamps
UNPROTECTED_PATHS = {"/", "/health", "/health/full", "/counselor/init", "/api/destiny-story/generate-stream"}


def _client_id() -> str:
    return (
        (request.headers.get("X-Forwarded-For") or "").split(",")[0].strip()
        or request.remote_addr
        or "unknown"
    )


_rate_cleanup_counter = 0

def _check_rate() -> Tuple[bool, Optional[float]]:
    global _rate_cleanup_counter
    now = time.time()
    client = _client_id()
    window = [t for t in _rate_state[client] if now - t < RATE_WINDOW_SECONDS]
    _rate_state[client] = window

    # Periodic cleanup of stale clients (every 100 requests)
    _rate_cleanup_counter += 1
    if _rate_cleanup_counter >= 100:
        _rate_cleanup_counter = 0
        stale_clients = [
            c for c, ts in _rate_state.items()
            if not ts or (now - max(ts)) > RATE_WINDOW_SECONDS * 2
        ]
        for c in stale_clients:
            del _rate_state[c]
        if stale_clients:
            logger.debug(f"[RATE] Cleaned up {len(stale_clients)} stale clients")

    if len(window) >= RATE_LIMIT:
        retry_after = max(0, RATE_WINDOW_SECONDS - (now - window[0]))
        return False, retry_after
    window.append(now)
    _rate_state[client] = window
    return True, None


def _require_auth() -> Optional[Tuple[dict, int]]:
    # Read token at request time to handle dotenv race conditions
    admin_token = os.getenv("ADMIN_API_TOKEN") or ADMIN_TOKEN
    if not admin_token:
        return None
    # Allow unauthenticated access to health endpoints for load balancers
    if request.path in UNPROTECTED_PATHS or request.method == "OPTIONS":
        return None
    auth_header = request.headers.get("Authorization", "")
    token = None
    if auth_header.lower().startswith("bearer "):
        token = auth_header[7:].strip()
    token = token or request.headers.get("X-API-KEY") or request.args.get("token")
    if token != admin_token:
        return {"status": "error", "message": "unauthorized"}, 401
    return None


@app.before_request
def before_request():
    g.request_id = str(uuid4())
    g._start_time = time.time()

    ok, retry_after = _check_rate()
    if not ok:
        logger.warning(
            f"[RATE_LIMIT] client={_client_id()} path={request.path} retry_after={retry_after}"
        )
        return (
            jsonify(
                {
                    "status": "error",
                    "message": "rate limit exceeded",
                    "retry_after": retry_after,
                }
            ),
            429,
        )

    auth_error = _require_auth()
    if auth_error:
        logger.warning(f"[AUTH] blocked client={_client_id()} path={request.path}")
        body, code = auth_error
        return jsonify(body), code


@app.after_request
def after_request(response):
    response.headers["X-Request-ID"] = getattr(g, "request_id", "")
    try:
        duration = time.time() - getattr(g, "_start_time", time.time())
        logger.info(
            f"[REQ] id={getattr(g, 'request_id', '')} path={request.path} "
            f"status={response.status_code} dur_ms={int(duration*1000)}"
        )
    except Exception:
        pass
    return response


# ===============================================================
# GLOBAL ERROR HANDLERS - Consistent error responses
# ===============================================================

@app.errorhandler(400)
def bad_request(e):
    return jsonify({
        "status": "error",
        "code": 400,
        "message": "Bad request",
        "request_id": getattr(g, "request_id", None)
    }), 400


@app.errorhandler(404)
def not_found(e):
    return jsonify({
        "status": "error",
        "code": 404,
        "message": "Endpoint not found",
        "request_id": getattr(g, "request_id", None)
    }), 404


@app.errorhandler(405)
def method_not_allowed(e):
    return jsonify({
        "status": "error",
        "code": 405,
        "message": "Method not allowed",
        "request_id": getattr(g, "request_id", None)
    }), 405


@app.errorhandler(500)
def internal_error(e):
    logger.exception(f"[ERROR] Unhandled exception: {e}")
    return jsonify({
        "status": "error",
        "code": 500,
        "message": "Internal server error",
        "request_id": getattr(g, "request_id", None)
    }), 500


@app.errorhandler(Exception)
def handle_exception(e):
    """Catch-all for unhandled exceptions."""
    logger.exception(f"[ERROR] Unhandled exception: {e}")
    return jsonify({
        "status": "error",
        "code": 500,
        "message": "An unexpected error occurred",
        "request_id": getattr(g, "request_id", None)
    }), 500


# Helper functions for building chart context
def _build_saju_summary(saju_data: dict) -> str:
    """Build concise saju summary for chat context."""
    if not saju_data:
        return ""
    parts = []
    if saju_data.get("dayMaster"):
        dm = saju_data["dayMaster"]
        dm_stem = dm.get('heavenlyStem') or dm.get('name', '')
        parts.append(f"Day Master: {dm_stem} ({dm.get('element', '')})")
    if saju_data.get("yearPillar"):
        yp = saju_data["yearPillar"]
        parts.append(f"Year: {yp.get('heavenlyStem', '')}{yp.get('earthlyBranch', '')}")
    if saju_data.get("monthPillar"):
        mp = saju_data["monthPillar"]
        parts.append(f"Month: {mp.get('heavenlyStem', '')}{mp.get('earthlyBranch', '')}")
    if saju_data.get("dominantElement"):
        parts.append(f"Dominant: {saju_data['dominantElement']}")
    return "SAJU: " + " | ".join(parts) if parts else ""


def _pick_astro_planet(astro_data: dict, name: str):
    """Select a planet payload from multiple possible shapes."""
    if not astro_data or not name:
        return None

    key = name.lower()
    direct = astro_data.get(key) or astro_data.get(name)
    if isinstance(direct, dict):
        return direct

    facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}
    fact_hit = facts.get(key) or facts.get(name)
    if isinstance(fact_hit, dict):
        return fact_hit

    planets = astro_data.get("planets")
    if isinstance(planets, list):
        for p in planets:
            if isinstance(p, dict) and str(p.get("name", "")).lower() == key:
                return p
    return None


def _pick_ascendant(astro_data: dict):
    """Select ascendant payload from multiple possible shapes."""
    if not astro_data:
        return None
    asc = astro_data.get("ascendant") or astro_data.get("asc")
    if isinstance(asc, dict):
        return asc
    facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}
    asc = facts.get("ascendant") or facts.get("asc")
    return asc if isinstance(asc, dict) else None


def _pick_astro_aspect(astro_data: dict):
    """Pick a representative aspect entry."""
    if not astro_data:
        return None
    aspects = astro_data.get("aspects")
    if not isinstance(aspects, list):
        facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}
        aspects = facts.get("aspects")
    if not isinstance(aspects, list) or not aspects:
        return None
    sorted_aspects = sorted(
        [a for a in aspects if isinstance(a, dict)],
        key=lambda a: a.get("score", 0),
        reverse=True
    )
    return sorted_aspects[0] if sorted_aspects else None


def _build_astro_summary(astro_data: dict) -> str:
    """Build concise astro summary for chat context."""
    if not astro_data:
        return ""
    parts = []
    sun = _pick_astro_planet(astro_data, "sun")
    if sun:
        parts.append(f"Sun: {sun.get('sign', '')}")
    moon = _pick_astro_planet(astro_data, "moon")
    if moon:
        parts.append(f"Moon: {moon.get('sign', '')}")
    asc = _pick_ascendant(astro_data)
    if asc:
        parts.append(f"Rising: {asc.get('sign', '')}")
    return "ASTRO: " + " | ".join(parts) if parts else ""


def _build_detailed_saju(saju_data: dict) -> str:
    """Build detailed saju context for personalized responses."""
    if not saju_data:
        return "ì‚¬ì£¼ ì •ë³´ ì—†ìŒ"

    lines = []
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    pillars = saju_data.get("pillars") if isinstance(saju_data.get("pillars"), dict) else facts.get("pillars", {})

    def _format_pillar(label: str, pillar: dict | str | None):
        if not pillar:
            return None
        if isinstance(pillar, str):
            return f"{label}: {pillar}"
        if not isinstance(pillar, dict):
            return None
        hs = pillar.get("heavenlyStem") or {}
        eb = pillar.get("earthlyBranch") or {}
        stem = hs.get("name") if isinstance(hs, dict) else hs
        branch = eb.get("name") if isinstance(eb, dict) else eb
        element = pillar.get("element") or (hs.get("element") if isinstance(hs, dict) else None) or (eb.get("element") if isinstance(eb, dict) else None)
        core = f"{stem or ''}{branch or ''}".strip() or pillar.get("name", "")
        return f"{label}: {core}" + (f" ({element})" if element else "")

    # Four Pillars (support facts/pillars shapes)
    year_pillar = saju_data.get("yearPillar") or facts.get("yearPillar") or (pillars.get("year") if isinstance(pillars, dict) else None)
    month_pillar = saju_data.get("monthPillar") or facts.get("monthPillar") or (pillars.get("month") if isinstance(pillars, dict) else None)
    day_pillar = saju_data.get("dayPillar") or facts.get("dayPillar") or (pillars.get("day") if isinstance(pillars, dict) else None)
    hour_pillar = saju_data.get("hourPillar") or facts.get("timePillar") or (pillars.get("time") if isinstance(pillars, dict) else None)

    for label, pillar in [("ë…„ì£¼", year_pillar), ("ì›”ì£¼", month_pillar), ("ì¼ì£¼", day_pillar), ("ì‹œì£¼", hour_pillar)]:
        formatted = _format_pillar(label, pillar)
        if formatted:
            lines.append(formatted)

    # Day Master (most important) - support both "heavenlyStem" and "name"
    dm = saju_data.get("dayMaster") or facts.get("dayMaster")
    if dm:
        dm_stem = dm.get("heavenlyStem") or dm.get("name", "")
        lines.append(f"ì¼ê°„(ë³¸ì¸): {dm_stem} - {dm.get('element', '')}ì˜ ê¸°ìš´")

    # Five Elements balance
    fe = saju_data.get("fiveElements") or facts.get("fiveElements")
    if fe:
        elements = [f"{k}({v})" for k, v in fe.items() if v]
        if elements:
            lines.append(f"ì˜¤í–‰ ë¶„í¬: {', '.join(elements)}")

    # Dominant element
    dominant_element = saju_data.get("dominantElement") or facts.get("dominantElement")
    if dominant_element:
        lines.append(f"ì£¼ìš” ê¸°ìš´: {dominant_element}")

    # Ten Gods (if available)
    tg = saju_data.get("tenGods") or facts.get("tenGods")
    if tg:
        if isinstance(tg, dict):
            gods = [f"{k}: {v}" for k, v in list(tg.items())[:4]]
            if gods:
                lines.append(f"ì‹­ì‹ : {', '.join(gods)}")

    return "\n".join(lines) if lines else "ì‚¬ì£¼ ì •ë³´ ë¶€ì¡±"


def _build_detailed_astro(astro_data: dict) -> str:
    """Build detailed astrology context for personalized responses."""
    if not astro_data:
        return "ì ì„±ìˆ  ì •ë³´ ì—†ìŒ"

    lines = []
    from datetime import datetime
    now = datetime.now()
    facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}

    # Big Three - ESSENTIAL
    sun_sign = ""
    moon_sign = ""
    sun = _pick_astro_planet(astro_data, "sun")
    if sun:
        sun_sign = sun.get("sign", "")
        house = sun.get("house", "")
        lines.append(f"â˜€ï¸ íƒœì–‘(ìì•„): {sun_sign} {sun.get('degree', '')}Â°" + (f" - {house}í•˜ìš°ìŠ¤" if house else ""))
    moon = _pick_astro_planet(astro_data, "moon")
    if moon:
        moon_sign = moon.get("sign", "")
        house = moon.get("house", "")
        lines.append(f"ğŸŒ™ ë‹¬(ê°ì •): {moon_sign} {moon.get('degree', '')}Â°" + (f" - {house}í•˜ìš°ìŠ¤" if house else ""))
    asc = _pick_ascendant(astro_data)
    if asc:
        lines.append(f"â¬†ï¸ ìƒìŠ¹(ì™¸ì ): {asc.get('sign', '')} {asc.get('degree', '')}Â°")

    # Key planets with houses
    for planet, info in [("mercury", "ìˆ˜ì„±(ì†Œí†µ)"), ("venus", "ê¸ˆì„±(ì‚¬ë‘/ê´€ê³„)"),
                         ("mars", "í™”ì„±(ì—ë„ˆì§€)"), ("jupiter", "ëª©ì„±(í–‰ìš´/í™•ì¥)"),
                         ("saturn", "í† ì„±(ì‹œë ¨/ì±…ì„)")]:
        p = _pick_astro_planet(astro_data, planet)
        if p:
            house = p.get("house", "")
            lines.append(f"{info}: {p.get('sign', '')}" + (f" - {house}í•˜ìš°ìŠ¤" if house else ""))

    # Houses (if available)
    houses = astro_data.get("houses") or facts.get("houses")
    if houses:
        h = houses
        lines.append("\nğŸ  ì£¼ìš” í•˜ìš°ìŠ¤:")
        # Handle both dict and list formats
        if isinstance(h, dict):
            if h.get("1"):
                lines.append(f"  1í•˜ìš°ìŠ¤(ìì•„): {h['1'].get('sign', '') if isinstance(h['1'], dict) else h['1']}")
            if h.get("7"):
                lines.append(f"  7í•˜ìš°ìŠ¤(íŒŒíŠ¸ë„ˆ): {h['7'].get('sign', '') if isinstance(h['7'], dict) else h['7']}")
            if h.get("10"):
                lines.append(f"  10í•˜ìš°ìŠ¤(ì»¤ë¦¬ì–´): {h['10'].get('sign', '') if isinstance(h['10'], dict) else h['10']}")
        elif isinstance(h, list) and len(h) >= 10:
            # List format: index 0 = 1st house, etc.
            if h[0]:
                sign = h[0].get('sign', '') if isinstance(h[0], dict) else h[0]
                lines.append(f"  1í•˜ìš°ìŠ¤(ìì•„): {sign}")
            if len(h) > 6 and h[6]:
                sign = h[6].get('sign', '') if isinstance(h[6], dict) else h[6]
                lines.append(f"  7í•˜ìš°ìŠ¤(íŒŒíŠ¸ë„ˆ): {sign}")
            if len(h) > 9 and h[9]:
                sign = h[9].get('sign', '') if isinstance(h[9], dict) else h[9]
                lines.append(f"  10í•˜ìš°ìŠ¤(ì»¤ë¦¬ì–´): {sign}")

    # Current transits - ADD TIMING CONTEXT for 2025
    lines.append(f"\nğŸ”® í˜„ì¬ íŠ¸ëœì§“ ({now.year}ë…„ {now.month}ì›”):")
    if now.year == 2025:
        if now.month <= 3:
            lines.append("â€¢ í† ì„± ë¬¼ê³ ê¸°ìë¦¬: ê°ì •ì  ê²½ê³„ í•™ìŠµ, ì˜ì  ì„±ìˆ™")
            lines.append("â€¢ ëª©ì„± ìŒë‘¥ì´ìë¦¬: ì†Œí†µê³¼ í•™ìŠµì˜ í™•ì¥ê¸°")
        elif now.month <= 6:
            lines.append("â€¢ í† ì„± ì–‘ìë¦¬ ì…ì„± (5ì›”): ìƒˆë¡œìš´ ì±…ì„ê³¼ ë„ì „ì˜ ì‹œì‘")
            lines.append("â€¢ ëª©ì„± ìŒë‘¥ì´ìë¦¬ ë§ˆë¬´ë¦¬: ì§€ì‹ í™•ì¥ ì™„ë£Œ")
        else:
            lines.append("â€¢ í† ì„± ì–‘ìë¦¬: ìê¸°ì£¼ë„ì  ì„±ì¥ì˜ ì‹œê¸°")
            lines.append("â€¢ ëª©ì„± ê²Œìë¦¬ (7ì›”~): ê°€ì •/ì •ì„œì  í’ìš”")
        lines.append("â€¢ ëª…ì™•ì„± ë¬¼ë³‘ìë¦¬: ì‚¬íšŒì  ë³€í˜, ê°œì¸ì˜ ë…ë¦½ì„± ê°•ì¡°")
    else:
        lines.append("â€¢ ì£¼ìš” í–‰ì„± íŠ¸ëœì§“ ì°¸ê³ í•˜ì—¬ í•´ì„")

    # Interpretation hints
    if sun_sign or moon_sign:
        lines.append("\nğŸ’¡ í•´ì„ í¬ì¸íŠ¸:")
        if sun_sign:
            lines.append(f"  íƒœì–‘ {sun_sign}: í•µì‹¬ ì •ì²´ì„±, ì‚¶ì˜ ëª©ì ")
        if moon_sign:
            lines.append(f"  ë‹¬ {moon_sign}: ê°ì • íŒ¨í„´, ë‚´ë©´ì˜ ìš•êµ¬")

    return "\n".join(lines) if lines else "ì ì„±ìˆ  ì •ë³´ ë¶€ì¡±"


def _build_advanced_astro_context(advanced_astro: dict) -> str:
    """Build context from advanced astrology features (draconic, harmonics, progressions, etc.)."""
    if not advanced_astro:
        return ""

    lines = []

    # Draconic Chart (soul-level astrology)
    if advanced_astro.get("draconic"):
        draconic = advanced_astro["draconic"]
        if isinstance(draconic, dict):
            lines.append("\nğŸ‰ ë“œë¼ì½”ë‹‰ ì°¨íŠ¸ (ì˜í˜¼ ë ˆë²¨):")
            if draconic.get("sun"):
                lines.append(f"  â€¢ ë“œë¼ì½”ë‹‰ íƒœì–‘: {draconic['sun']}")
            if draconic.get("moon"):
                lines.append(f"  â€¢ ë“œë¼ì½”ë‹‰ ë‹¬: {draconic['moon']}")
            if draconic.get("insights"):
                lines.append(f"  â†’ {draconic['insights'][:200]}")

    # Harmonics (personality layers)
    if advanced_astro.get("harmonics"):
        harmonics = advanced_astro["harmonics"]
        if isinstance(harmonics, dict):
            lines.append("\nğŸµ í•˜ëª¨ë‹‰ ë¶„ì„:")
            for key, value in list(harmonics.items())[:3]:
                if value:
                    lines.append(f"  â€¢ {key}: {value[:100] if isinstance(value, str) else value}")

    # Progressions (life timing)
    if advanced_astro.get("progressions"):
        prog = advanced_astro["progressions"]
        if isinstance(prog, dict):
            lines.append("\nğŸ”„ í”„ë¡œê·¸ë ˆì…˜ (ìƒì•  íƒ€ì´ë°):")
            if prog.get("secondary"):
                lines.append(f"  â€¢ ì„¸ì»¨ë”ë¦¬: {prog['secondary'][:150] if isinstance(prog['secondary'], str) else prog['secondary']}")
            if prog.get("solarArc"):
                lines.append(f"  â€¢ ì†”ë¼ ì•„í¬: {prog['solarArc'][:150] if isinstance(prog['solarArc'], str) else prog['solarArc']}")
            if prog.get("moonPhase"):
                lines.append(f"  â€¢ í˜„ì¬ ë‹¬ ìœ„ìƒ: {prog['moonPhase']}")

    # Solar Return (birthday year ahead)
    if advanced_astro.get("solarReturn"):
        sr = advanced_astro["solarReturn"]
        if isinstance(sr, dict) and sr.get("summary"):
            summary = sr['summary']
            if isinstance(summary, str):
                lines.append("\nğŸ‚ ì†”ë¼ ë¦¬í„´ (ì˜¬í•´ ìƒì¼ ì°¨íŠ¸):")
                lines.append(f"  {summary[:200]}")

    # Lunar Return (monthly energy)
    if advanced_astro.get("lunarReturn"):
        lr = advanced_astro["lunarReturn"]
        if isinstance(lr, dict) and lr.get("summary"):
            summary = lr['summary']
            if isinstance(summary, str):
                lines.append("\nğŸŒ™ ë£¨ë‚˜ ë¦¬í„´ (ì´ë²ˆ ë‹¬ ì—ë„ˆì§€):")
                lines.append(f"  {summary[:200]}")

    # Asteroids (detailed personality)
    if advanced_astro.get("asteroids"):
        asteroids = advanced_astro["asteroids"]
        if isinstance(asteroids, (list, dict)):
            lines.append("\nâ˜„ï¸ ì†Œí–‰ì„± ë¶„ì„:")
            if isinstance(asteroids, list):
                for ast in asteroids[:4]:
                    if isinstance(ast, dict):
                        interp = ast.get('interpretation', '')
                        interp_str = interp[:80] if isinstance(interp, str) else str(interp)[:80]
                        lines.append(f"  â€¢ {ast.get('name', '')}: {ast.get('sign', '')} {interp_str}")
            elif isinstance(asteroids, dict):
                for name, data in list(asteroids.items())[:4]:
                    if isinstance(data, dict):
                        interp = data.get('interpretation', '')
                        interp_str = interp[:80] if isinstance(interp, str) else str(interp)[:80]
                        lines.append(f"  â€¢ {name}: {data.get('sign', '')} {interp_str}")

    # Fixed Stars (fate/destiny points)
    if advanced_astro.get("fixedStars"):
        stars = advanced_astro["fixedStars"]
        if isinstance(stars, list) and stars:
            lines.append("\nâ­ ê³ ì •í•­ì„± (ìš´ëª… í¬ì¸íŠ¸):")
            for star in stars[:3]:
                if isinstance(star, dict):
                    interp = star.get('interpretation', '')
                    interp_str = interp[:100] if isinstance(interp, str) else str(interp)[:100]
                    lines.append(f"  â€¢ {star.get('name', '')}: {interp_str}")

    # Eclipses (transformation points)
    if advanced_astro.get("eclipses"):
        eclipses = advanced_astro["eclipses"]
        if isinstance(eclipses, (list, dict)):
            lines.append("\nğŸŒ‘ ì¼ì‹/ì›”ì‹ ì˜í–¥:")
            if isinstance(eclipses, list):
                for ecl in eclipses[:2]:
                    if isinstance(ecl, dict):
                        interp = ecl.get('interpretation', '')
                        interp_str = interp[:100] if isinstance(interp, str) else str(interp)[:100]
                        lines.append(f"  â€¢ {ecl.get('type', '')}: {ecl.get('date', '')} - {interp_str}")
            elif isinstance(eclipses, dict):
                if eclipses.get("solar"):
                    solar = eclipses['solar']
                    solar_str = solar[:100] if isinstance(solar, str) else str(solar)[:100]
                    lines.append(f"  â€¢ ì¼ì‹: {solar_str}")
                if eclipses.get("lunar"):
                    lunar = eclipses['lunar']
                    lunar_str = lunar[:100] if isinstance(lunar, str) else str(lunar)[:100]
                    lines.append(f"  â€¢ ì›”ì‹: {lunar_str}")

    # Midpoints (relationship dynamics)
    if advanced_astro.get("midpoints"):
        mp = advanced_astro["midpoints"]
        if isinstance(mp, dict):
            lines.append("\nğŸ”— ë¯¸ë“œí¬ì¸íŠ¸ (í•µì‹¬ ì¡°í•©):")
            if mp.get("sunMoon"):
                lines.append(f"  â€¢ íƒœì–‘/ë‹¬: {mp['sunMoon'][:100] if isinstance(mp['sunMoon'], str) else mp['sunMoon']}")
            if mp.get("ascMc"):
                lines.append(f"  â€¢ ìƒìŠ¹/MC: {mp['ascMc'][:100] if isinstance(mp['ascMc'], str) else mp['ascMc']}")

    # Current Transits (personalized)
    if advanced_astro.get("transits"):
        transits = advanced_astro["transits"]
        if isinstance(transits, list) and transits:
            lines.append("\nğŸŒ í˜„ì¬ ê°œì¸ íŠ¸ëœì§“:")
            for transit in transits[:5]:
                if isinstance(transit, dict):
                    lines.append(f"  â€¢ {transit.get('aspect', '')}: {transit.get('interpretation', '')[:100]}")
                elif isinstance(transit, str):
                    lines.append(f"  â€¢ {transit[:100]}")

    # Extra Points (Lilith, Part of Fortune, etc.)
    if advanced_astro.get("extraPoints"):
        extra = advanced_astro["extraPoints"]
        if isinstance(extra, dict):
            lines.append("\nğŸ”® íŠ¹ìˆ˜ í¬ì¸íŠ¸:")
            for name, data in list(extra.items())[:4]:
                if isinstance(data, dict):
                    lines.append(f"  â€¢ {name}: {data.get('sign', '')} {data.get('interpretation', '')[:80]}")
                elif isinstance(data, str):
                    lines.append(f"  â€¢ {name}: {data[:80]}")

    if lines:
        return "\n".join(lines)
    return ""


def _add_months(src_date: date, months: int) -> date:
    """Add months to a date while keeping day within target month range."""
    year = src_date.year + (src_date.month - 1 + months) // 12
    month = (src_date.month - 1 + months) % 12 + 1
    day = min(src_date.day, calendar.monthrange(year, month)[1])
    return date(year, month, day)

def _format_month_name(src_date: date) -> str:
    month_names = [
        "January", "February", "March", "April", "May", "June",
        "July", "August", "September", "October", "November", "December",
    ]
    return month_names[src_date.month - 1]


def _format_date_ymd(src_date: date) -> str:
    return f"{src_date.year:04d}-{src_date.month:02d}-{src_date.day:02d}"


def _count_timing_markers(text: str) -> int:
    if not text:
        return 0
    pattern = re.compile(
        r"(?:\d{1,2}\s*~\s*\d{1,2}\s*ì›”|\d{1,2}\s*ì›”|\d{1,2}\s*ì£¼|\d{1,2}/\d{1,2}|"
        r"ì´ë²ˆ\s*ë‹¬|ë‹¤ìŒ\s*ë‹¬|ë‹¤ë‹¤ìŒ\s*ë‹¬|ì´ë²ˆ\s*ì£¼|ë‹¤ìŒ\s*ì£¼|ìƒë°˜ê¸°|í•˜ë°˜ê¸°)"
    )
    return len({m.group(0) for m in pattern.finditer(text)})


def _has_week_timing(text: str) -> bool:
    if not text:
        return False
    pattern = re.compile(
        r"(?:\d{1,2}\s*ì›”\s*(?:\d{1,2}\s*ì£¼|1~2ì£¼ì°¨|2~3ì£¼ì°¨|3~4ì£¼ì°¨|"
        r"ì²«ì§¸ì£¼|ë‘˜ì§¸ì£¼|ì…‹ì§¸ì£¼|ë„·ì§¸ì£¼|ë‹¤ì„¯ì§¸ì£¼))"
    )
    return bool(pattern.search(text))


def _has_caution(text: str) -> bool:
    if not text:
        return False
    caution_terms = [
        "ì£¼ì˜",
        "ê²½ê³ ",
        "ìœ ì˜",
        "ì¡°ì‹¬",
        "í”¼í•˜",
        "ìœ„í—˜",
        "ê²½ê³„",
    ]
    return any(term in text for term in caution_terms)

def _count_timing_markers_en(text: str) -> int:
    if not text:
        return 0
    pattern = re.compile(
        r"(?:\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\w*|\bq[1-4]\b|"
        r"\b(?:this|next)\s+(?:week|month|quarter)\b|\bweek\s*\d{1,2}\b|"
        r"\b\d{1,2}(?:st|nd|rd|th)?\s+week\b|\b\d{1,2}/\d{1,2}(?:/\d{2,4})?\b)",
        re.IGNORECASE,
    )
    return len({m.group(0).lower() for m in pattern.finditer(text)})

def _has_week_timing_en(text: str) -> bool:
    if not text:
        return False
    pattern = re.compile(
        r"(?:week\s*\d{1,2}|\d{1,2}(?:st|nd|rd|th)?\s+week)",
        re.IGNORECASE,
    )
    return bool(pattern.search(text))

def _has_caution_en(text: str) -> bool:
    if not text:
        return False
    caution_terms = [
        "caution", "avoid", "watch out", "be careful", "risk", "risky",
        "hold off", "delay", "slow down", "conflict", "friction",
    ]
    lower = text.lower()
    return any(term in lower for term in caution_terms)


def _ensure_ko_prefix(text: str, locale: str) -> str:
    if locale != "ko" or not text:
        return text
    trimmed = text.lstrip(" \t\r\n\"'â€œâ€â€˜â€™")
    if trimmed.startswith("ì´ì•¼"):
        return trimmed
    return f"ì´ì•¼, {trimmed}"


def _format_korean_spacing(text: str) -> str:
    if not text:
        return text
    text = re.sub(r"([.!?])(?=[ê°€-í£A-Za-z0-9])", r"\1 ", text)
    text = re.sub(r"([,])(?=[ê°€-í£A-Za-z0-9])", r"\1 ", text)
    text = re.sub(r"([ê°€-í£])(\d)", r"\1 \2", text)
    text = re.sub(r"((?:ASC|MC|IC|DC))(\d)", r"\1 \2", text)

    unit_tokens = ("ë…„", "ì›”", "ì¼", "ì£¼", "ì°¨", "ì‹œ", "ë¶„", "ì´ˆ", "í•˜ìš°ìŠ¤", "ëŒ€", "ì„¸", "ì‚´", "ê°œì›”")

    def _digit_hangul(match: re.Match) -> str:
        digit = match.group(1)
        tail = match.group(2)
        for unit in unit_tokens:
            if tail.startswith(unit):
                return f"{digit}{tail}"
        return f"{digit} {tail}"

    text = re.sub(r"(\d)([ê°€-í£])", _digit_hangul, text)
    text = re.sub(r"(\d)\s*\.\s*(\d)", r"\1.\2", text)
    text = re.sub(r"(\d)\s*,\s*(\d)", r"\1,\2", text)
    text = re.sub(r"[ \t]{2,}", " ", text)
    text = re.sub(r"(\d)\s+(ë…„|ì›”|ì¼|ì£¼|ì°¨|ì‹œ|ë¶„|ì´ˆ|í•˜ìš°ìŠ¤|ëŒ€|ì„¸|ì‚´|ê°œì›”)", r"\1\2", text)
    return text.strip()


def _insert_addendum(text: str, addendum: str) -> str:
    if not addendum:
        return text
    if "\n\n" in text:
        parts = text.split("\n\n")
        insert_idx = max(1, len(parts) - 1)
        parts.insert(insert_idx, addendum)
        return "\n\n".join(parts)
    sentence_ends = [m.end() for m in re.finditer(r"[.!?]", text)]
    if sentence_ends:
        insert_pos = sentence_ends[0] if len(sentence_ends) == 1 else sentence_ends[1]
        prefix = text[:insert_pos]
        suffix = text[insert_pos:].lstrip()
        sep = "" if prefix.endswith((" ", "\n", "\t")) else " "
        return f"{prefix}{sep}{addendum} {suffix}"
    if text:
        # Fallback: insert near the middle so evidence lands in-body.
        mid = max(0, len(text) // 2)
        right = text.find(" ", mid)
        left = text.rfind(" ", 0, mid)
        insert_pos = right if right != -1 else left
        if insert_pos > 0:
            prefix = text[:insert_pos]
            suffix = text[insert_pos:].lstrip()
            sep = "" if prefix.endswith((" ", "\n", "\t")) else " "
            return f"{prefix}{sep}{addendum} {suffix}"
    last_question = text.rfind("?")
    if last_question != -1:
        prefix = text[:last_question]
        suffix = text[last_question:]
        sep = "" if prefix.endswith((" ", "\n", "\t")) else " "
        return f"{prefix}{sep}{addendum} {suffix}"
    last_period = max(text.rfind("."), text.rfind("!"))
    if last_period != -1:
        prefix = text[:last_period + 1]
        suffix = text[last_period + 1:].lstrip()
        sep = "" if prefix.endswith((" ", "\n", "\t")) else " "
        return f"{prefix}{sep}{addendum} {suffix}"
    return f"{text} {addendum}"


def _chunk_text(text: str, chunk_size: int = 200):
    if not text:
        return []
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

def _get_stream_chunk_size() -> int:
    return _get_int_env("ASK_STREAM_CHUNK_SIZE", 200, min_value=80, max_value=800)

def _to_sse_event(text: str) -> str:
    if text is None:
        return ""
    lines = text.splitlines()
    if not lines:
        return "data: \n\n"
    payload = "".join([f"data: {line}\n" for line in lines])
    return payload + "\n"

def _sse_error_response(message: str) -> Response:
    def generate():
        chunk_size = _get_stream_chunk_size()
        for piece in _chunk_text(message or "", chunk_size):
            yield _to_sse_event(piece)
        yield "data: [DONE]\n\n"

    return Response(
        stream_with_context(generate()),
        mimetype="text/event-stream",
        headers={
            "Content-Type": "text/event-stream; charset=utf-8",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


def _has_saju_payload(saju_data: dict) -> bool:
    if not isinstance(saju_data, dict) or not saju_data:
        return False
    if saju_data.get("dayMaster"):
        return True
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    for key in ("pillars", "tenGods", "fiveElements", "dominantElement", "daeun", "unse"):
        if saju_data.get(key) or facts.get(key):
            return True
    return False


def _has_astro_payload(astro_data: dict) -> bool:
    if not isinstance(astro_data, dict) or not astro_data:
        return False
    if astro_data.get("sun") or astro_data.get("moon"):
        return True
    if astro_data.get("planets") or astro_data.get("houses") or astro_data.get("aspects"):
        return True
    if astro_data.get("ascendant") or astro_data.get("asc") or astro_data.get("rising"):
        return True
    facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}
    if facts.get("planets") or facts.get("houses") or facts.get("aspects"):
        return True
    return False


def _build_birth_format_message(locale: str) -> str:
    if locale == "ko":
        return "ìƒë…„ì›”ì¼/ì‹œê°„ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆ: 1995-02-09, 06:40"
    return "Invalid birth date/time format. Example: 1995-02-09, 06:40"


def _build_missing_payload_message(locale: str, missing_saju: bool, missing_astro: bool) -> str:
    if locale == "ko":
        if missing_saju and missing_astro:
            return (
                "ì‚¬ì£¼/ì ì„±í•™ ê³„ì‚° ê²°ê³¼ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ëŸ°íŠ¸ì—ì„œ computeDestinyMap ê²°ê³¼ë¥¼ "
                "`saju`ì™€ `astro`ë¡œ ì „ë‹¬í•´ ì£¼ì„¸ìš”. (ìƒë…„ì›”ì¼/ì‹œê°„ë§Œìœ¼ë¡œëŠ” ì´ APIê°€ ê³ ê¸‰ ì°¨íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
            )
        if missing_saju:
            return (
                "ì‚¬ì£¼ ê³„ì‚° ê²°ê³¼ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. computeDestinyMap ê²°ê³¼ë¥¼ `saju`ë¡œ ì „ë‹¬í•´ ì£¼ì„¸ìš”. "
                "(ìƒë…„ì›”ì¼/ì‹œê°„ë§Œìœ¼ë¡œëŠ” ì´ APIê°€ ê³ ê¸‰ ì°¨íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
            )
        return (
            "ì ì„±í•™ ê³„ì‚° ê²°ê³¼ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. computeDestinyMap ê²°ê³¼ë¥¼ `astro`ë¡œ ì „ë‹¬í•´ ì£¼ì„¸ìš”. "
            "(ìƒë…„ì›”ì¼/ì‹œê°„ë§Œìœ¼ë¡œëŠ” ì´ APIê°€ ê³ ê¸‰ ì°¨íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
        )
    if missing_saju and missing_astro:
        return (
            "Computed saju/astrology payload is missing. Please pass computeDestinyMap results in `saju` and `astro`. "
            "(This API does not compute advanced charts from birth inputs alone.)"
        )
    if missing_saju:
        return (
            "Computed saju payload is missing. Please pass computeDestinyMap results in `saju`. "
            "(This API does not compute advanced charts from birth inputs alone.)"
        )
    return (
        "Computed astrology payload is missing. Please pass computeDestinyMap results in `astro`. "
        "(This API does not compute advanced charts from birth inputs alone.)"
    )

def _summarize_five_elements(saju_data: dict) -> str:
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    five = saju_data.get("fiveElements") or facts.get("fiveElements")
    if not isinstance(five, dict) or not five:
        return ""
    element_map = {
        "wood": "ëª©",
        "fire": "í™”",
        "earth": "í† ",
        "metal": "ê¸ˆ",
        "water": "ìˆ˜",
    }
    normalized = {}
    for key, value in five.items():
        ko = element_map.get(key, key)
        if isinstance(value, (int, float)):
            normalized[ko] = value
    if not normalized:
        return ""
    max_elem = max(normalized, key=normalized.get)
    min_elem = min(normalized, key=normalized.get)
    if normalized[max_elem] == normalized[min_elem]:
        return "ì˜¤í–‰ì€ ë¹„êµì  ê³ ë¥´ê²Œ ë¶„í¬ëœ í¸ì´ì—ìš”"
    return f"ì˜¤í–‰ì€ {max_elem} ê¸°ìš´ì´ ê°•í•˜ê³  {min_elem} ê¸°ìš´ì´ ì•½í•œ í¸ì´ì—ìš”"

def _summarize_five_elements_en(saju_data: dict) -> str:
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    five = saju_data.get("fiveElements") or facts.get("fiveElements")
    if not isinstance(five, dict) or not five:
        return ""
    element_map = {
        "wood": "wood",
        "fire": "fire",
        "earth": "earth",
        "metal": "metal",
        "water": "water",
        "ëª©": "wood",
        "í™”": "fire",
        "í† ": "earth",
        "ê¸ˆ": "metal",
        "ìˆ˜": "water",
        "æœ¨": "wood",
        "ç«": "fire",
        "åœŸ": "earth",
        "é‡‘": "metal",
        "æ°´": "water",
    }
    normalized = {}
    for key, value in five.items():
        mapped = element_map.get(str(key).lower(), element_map.get(str(key), str(key)))
        if isinstance(value, (int, float)):
            normalized[mapped] = value
    if not normalized:
        return ""
    max_elem = max(normalized, key=normalized.get)
    min_elem = min(normalized, key=normalized.get)
    if normalized[max_elem] == normalized[min_elem]:
        return "Five Elements look fairly balanced."
    return f"Five Elements show strong {max_elem} and weaker {min_elem}."


def _pick_sibsin(saju_data: dict) -> str:
    def _pick_from_pillar(pillar: dict) -> str:
        if not isinstance(pillar, dict):
            return ""
        for key in ("heavenlyStem", "earthlyBranch"):
            val = pillar.get(key) if isinstance(pillar.get(key), dict) else {}
            sibsin = val.get("sibsin")
            if sibsin:
                return sibsin
        sibsin = pillar.get("sibsin")
        if isinstance(sibsin, dict):
            for val in sibsin.values():
                if val:
                    return val
        return ""

    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    for root in (facts, saju_data):
        pillars = root.get("pillars") if isinstance(root.get("pillars"), dict) else {}
        for key in ("day", "month", "year", "time"):
            sibsin = _pick_from_pillar(pillars.get(key))
            if sibsin:
                return sibsin
        for key in ("dayPillar", "monthPillar", "yearPillar", "timePillar"):
            sibsin = _pick_from_pillar(root.get(key))
            if sibsin:
                return sibsin
    return ""


def _planet_ko_name(name: str) -> str:
    if not name:
        return ""
    planet_map = {
        "sun": "íƒœì–‘",
        "moon": "ë‹¬",
        "mercury": "ìˆ˜ì„±",
        "venus": "ê¸ˆì„±",
        "mars": "í™”ì„±",
        "jupiter": "ëª©ì„±",
        "saturn": "í† ì„±",
        "uranus": "ì²œì™•ì„±",
        "neptune": "í•´ì™•ì„±",
        "pluto": "ëª…ì™•ì„±",
    }
    return planet_map.get(name.lower(), name)

def _planet_en_name(name: str) -> str:
    if not name:
        return ""
    planet_map = {
        "sun": "Sun",
        "moon": "Moon",
        "mercury": "Mercury",
        "venus": "Venus",
        "mars": "Mars",
        "jupiter": "Jupiter",
        "saturn": "Saturn",
        "uranus": "Uranus",
        "neptune": "Neptune",
        "pluto": "Pluto",
    }
    return planet_map.get(name.lower(), name)


def _pick_any_planet(astro_data: dict):
    for key in ("sun", "moon", "mercury", "venus", "mars", "jupiter", "saturn", "uranus", "neptune", "pluto"):
        hit = _pick_astro_planet(astro_data, key)
        if hit:
            return hit
    facts = astro_data.get("facts") if isinstance(astro_data.get("facts"), dict) else {}
    for source in (astro_data.get("planets"), facts.get("planets")):
        if isinstance(source, list):
            for planet in source:
                if isinstance(planet, dict) and planet.get("name"):
                    return planet
    return None


def _build_saju_evidence_sentence(saju_data: dict) -> str:
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    dm = saju_data.get("dayMaster") or facts.get("dayMaster") or {}
    dm_name = dm.get("heavenlyStem") or dm.get("name")
    dm_element = dm.get("element")

    pillars = saju_data.get("pillars") if isinstance(saju_data.get("pillars"), dict) else facts.get("pillars", {})
    month_pillar = None
    if isinstance(pillars, dict):
        month_pillar = pillars.get("month")
    if isinstance(month_pillar, dict):
        hs = month_pillar.get("heavenlyStem", {})
        eb = month_pillar.get("earthlyBranch", {})
        month_name = f"{hs.get('name', '')}{eb.get('name', '')}".strip()
    else:
        month_name = ""

    dm_text = ""
    if dm_name or dm_element:
        dm_text = f"{dm_name}" if dm_name else ""
        if dm_element:
            dm_text = f"{dm_text}({dm_element})" if dm_text else f"{dm_element}"

    element_summary = _summarize_five_elements(saju_data)
    sibsin = _pick_sibsin(saju_data)

    parts = []
    if dm_text:
        parts.append(f"ì¼ê°„ {dm_text} íë¦„ì´ ìˆê³ ")
    if element_summary:
        parts.append(element_summary)
    else:
        parts.append("ì˜¤í–‰ ê· í˜•ì€ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•´ìš”")
    if sibsin:
        parts.append(f"ì‹­ì„±ì€ {sibsin} ê¸°ìš´ì´ ë„ë“œë¼ì ¸ìš”")
    else:
        parts.append("ì‹­ì„± íë¦„ì€ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•´ìš”")
    if parts:
        return "ì‚¬ì£¼ì—ì„œëŠ” " + ", ".join(parts) + "."
    if month_name:
        return f"ì‚¬ì£¼ë¡œ ë³´ë©´ ì›”ì£¼ {month_name} íë¦„ì´ ìˆì–´ì„œ ì•ˆì •ê³¼ í™•ì¥ì˜ ê· í˜•ì„ ìì£¼ ê³ ë¯¼í•˜ê²Œ ë˜ëŠ” í¸ì´ì—ìš”."
    return ""

def _build_saju_evidence_sentence_en(saju_data: dict) -> str:
    facts = saju_data.get("facts") if isinstance(saju_data.get("facts"), dict) else {}
    dm = saju_data.get("dayMaster") or facts.get("dayMaster") or {}
    dm_name = dm.get("heavenlyStem") or dm.get("name")
    dm_element = dm.get("element")
    element_map = {
        "ëª©": "wood", "í™”": "fire", "í† ": "earth", "ê¸ˆ": "metal", "ìˆ˜": "water",
        "æœ¨": "wood", "ç«": "fire", "åœŸ": "earth", "é‡‘": "metal", "æ°´": "water",
        "wood": "wood", "fire": "fire", "earth": "earth", "metal": "metal", "water": "water",
    }
    dm_element_en = element_map.get(str(dm_element), dm_element) if dm_element else ""
    dm_text = ""
    if dm_name or dm_element_en:
        dm_text = f"{dm_name}" if dm_name else ""
        if dm_element_en:
            dm_text = f"{dm_text} ({dm_element_en})" if dm_text else f"{dm_element_en}"

    element_summary = _summarize_five_elements_en(saju_data)
    sibsin = _pick_sibsin(saju_data)

    parts = []
    if dm_text:
        parts.append(f"your Day Master is {dm_text}")
    if element_summary:
        parts.append(element_summary.rstrip("."))
    else:
        parts.append("Five Elements balance needs a closer check")
    if sibsin:
        parts.append(f"Ten Gods emphasize {sibsin}")
    else:
        parts.append("Ten Gods emphasis needs confirmation")
    return "From your Four Pillars, " + ", ".join(parts) + "."


def _build_astro_evidence_sentence(astro_data: dict) -> str:
    planet = _pick_astro_planet(astro_data, "sun") or _pick_astro_planet(astro_data, "moon") or _pick_any_planet(astro_data)
    asc = _pick_ascendant(astro_data)
    aspect = _pick_astro_aspect(astro_data)

    aspect_text = ""
    if isinstance(aspect, dict):
        aspect_map = {
            "trine": "íŠ¸ë¼ì¸",
            "square": "ìŠ¤í€˜ì–´",
            "conjunction": "ì»¨ì •ì…˜",
            "opposition": "ì˜µí¬ì§€ì…˜",
            "sextile": "ì„¹ìŠ¤íƒ€ì¼",
        }
        from_name = _planet_ko_name(str(aspect.get("from", {}).get("name", "")))
        to_name = _planet_ko_name(str(aspect.get("to", {}).get("name", "")))
        aspect_type = aspect_map.get(str(aspect.get("type", "")).lower(), aspect.get("type", ""))
        if from_name and to_name and aspect_type:
            aspect_text = f"{from_name}-{to_name} {aspect_type} ê°"

    if planet:
        planet_name = _planet_ko_name(str(planet.get("name", ""))) or "ì£¼ìš”"
        sign = planet.get("sign", "")
        house = planet.get("house")
        house_text = f"{house}í•˜ìš°ìŠ¤" if house else "í•˜ìš°ìŠ¤"
        position_text = f"{sign} {house_text}".strip()
        aspect_clause = f", {aspect_text}ì´ ìˆì–´" if aspect_text else ""
        return f"ì ì„±ì—ì„œëŠ” {planet_name}ì´ë¼ëŠ” í–‰ì„±ì´ {position_text}ì— ìˆê³ {aspect_clause} íë¦„ì´ ë³´ì—¬ìš”."
    if asc:
        sign = asc.get("sign", "")
        return f"ì ì„±ì—ì„œëŠ” í–‰ì„± ë°ì´í„°ê°€ ì œí•œì ì´ì§€ë§Œ ìƒìŠ¹ì ì´ {sign}ì´ê³  í•˜ìš°ìŠ¤ ì¶•ì´ ë¶„ëª…í•´ í–‰ë™ ë°©ì‹ì´ ë˜ë ·í•˜ê²Œ ë³´ì´ëŠ” í¸ì´ì—ìš”."
    return ""

def _build_astro_evidence_sentence_en(astro_data: dict) -> str:
    planet = _pick_astro_planet(astro_data, "sun") or _pick_astro_planet(astro_data, "moon") or _pick_any_planet(astro_data)
    asc = _pick_ascendant(astro_data)
    aspect = _pick_astro_aspect(astro_data)

    aspect_text = ""
    if isinstance(aspect, dict):
        aspect_map = {
            "trine": "trine",
            "square": "square",
            "conjunction": "conjunction",
            "opposition": "opposition",
            "sextile": "sextile",
        }
        from_name = _planet_en_name(str(aspect.get("from", {}).get("name", "")))
        to_name = _planet_en_name(str(aspect.get("to", {}).get("name", "")))
        aspect_type = aspect_map.get(str(aspect.get("type", "")).lower(), aspect.get("type", ""))
        if from_name and to_name and aspect_type:
            aspect_text = f"{from_name}-{to_name} {aspect_type}"

    if planet:
        planet_name = _planet_en_name(str(planet.get("name", ""))) or "a key planet"
        sign = planet.get("sign", "")
        house = planet.get("house")
        house_text = f"{house}th house" if house else "a house placement"
        position_text = f"{sign} {house_text}".strip()
        aspect_clause = f", with a {aspect_text} aspect" if aspect_text else ""
        return f"In your chart, {planet_name} in {position_text}{aspect_clause} shows up as a clear influence."
    if asc:
        sign = asc.get("sign", "")
        return f"Your Ascendant in {sign} sets a clear outer persona even when other planetary data is limited."
    return "Astrology data is limited, but keep the Sun/Moon and house axis as anchors for guidance."


def _build_missing_requirements_addendum(
    text: str,
    locale: str,
    saju_data: dict,
    astro_data: dict,
    now_date: date,
    require_saju: bool = True,
    require_astro: bool = True,
    require_timing: bool = True,
    require_caution: bool = True,
) -> str:
    if not text:
        return ""

    if locale == "ko":
        saju_tokens = [
            "ì¼ê°„", "ì˜¤í–‰", "ì‹­ì„±", "ëŒ€ìš´", "ì„¸ìš´", "ì›”ì£¼", "ì¼ì£¼", "ë…„ì£¼", "ì‹œì£¼",
            "ë¹„ê²¬", "ê²ì¬", "ì‹ì‹ ", "ìƒê´€", "í¸ì¬", "ì •ì¬", "í¸ê´€", "ì •ê´€", "í¸ì¸", "ì •ì¸",
        ]
        dm = (saju_data.get("dayMaster") or {}).get("name")
        dm_element = (saju_data.get("dayMaster") or {}).get("element")
        if dm:
            saju_tokens.append(str(dm))
        if dm_element:
            saju_tokens.append(str(dm_element))
        has_saju = any(token and token in text for token in saju_tokens)
        has_saju_required = "ì˜¤í–‰" in text and "ì‹­ì„±" in text

        astro_tokens = ["íƒœì–‘", "ë‹¬", "ASC", "ìƒìŠ¹", "í–‰ì„±", "í•˜ìš°ìŠ¤", "ìˆ˜ì„±", "ê¸ˆì„±", "í™”ì„±", "ëª©ì„±", "í† ì„±", "ì²œì™•ì„±", "í•´ì™•ì„±", "ëª…ì™•ì„±"]
        sun = _pick_astro_planet(astro_data, "sun")
        moon = _pick_astro_planet(astro_data, "moon")
        asc = _pick_ascendant(astro_data)
        for p in (sun, moon, asc):
            if p and p.get("sign"):
                astro_tokens.append(str(p.get("sign")))
        has_astro = any(token and token in text for token in astro_tokens)
        has_astro_required = "í–‰ì„±" in text and "í•˜ìš°ìŠ¤" in text

        timing_count = _count_timing_markers(text)
        has_week_timing = _has_week_timing(text)
        has_caution = _has_caution(text)

        add_parts = []
        if require_saju and (not has_saju or not has_saju_required):
            saju_sentence = _build_saju_evidence_sentence(saju_data)
            if saju_sentence:
                add_parts.append(saju_sentence)
        if require_astro and (not has_astro or not has_astro_required):
            astro_sentence = _build_astro_evidence_sentence(astro_data)
            if astro_sentence:
                add_parts.append(astro_sentence)
        if require_timing and (timing_count < 2 or not has_week_timing):
            m1 = _add_months(now_date, 1)
            m2 = _add_months(now_date, 3)
            m3 = _add_months(now_date, 5)
            timing_sentence = (
                f"íƒ€ì´ë°ì€ {m1.year}ë…„ {m1.month}ì›” 1~2ì£¼ì°¨, "
                f"{m2.year}ë…„ {m2.month}ì›” 2~3ì£¼ì°¨, "
                f"{m3.year}ë…„ {m3.month}ì›” 3~4ì£¼ì°¨ íë¦„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë³´ë©´ ì¢‹ì•„ìš”."
            )
            add_parts.append(timing_sentence)
        if require_caution and not has_caution:
            m2 = _add_months(now_date, 3)
            add_parts.append(
                f"\uC8FC\uC758: {m2.year}\uB144 {m2.month}\uC6D4 2~3\uC8FC\uCC28\uCBE4\uC740 \uC911\uC694\uD55C \uACB0\uC815\uC744 \uBB34\uB9AC\uD558\uAC8C \uBC00\uC5B4\uBD99\uC774\uAE30\uBCF4\uB2E4\uB294 \uD55C \uD15C\uD3EC \uC810\uAC80\uD558\uB294 \uAC8C \uC88B\uC544 \uBCF4\uC5EC\uC694."
            )

        return " ".join([part for part in add_parts if part]).strip()

    lower = text.lower()
    saju_tokens_en = [
        "day master", "five elements", "ten gods", "daeun", "seun",
        "year pillar", "month pillar", "day pillar", "hour pillar", "four pillars",
    ]
    has_saju = any(token in lower for token in saju_tokens_en)
    has_saju_required = "five elements" in lower and "ten gods" in lower

    astro_tokens_en = [
        "sun", "moon", "ascendant", "rising", "house", "planet", "aspect", "transit",
    ]
    has_astro = any(token in lower for token in astro_tokens_en)
    has_astro_required = "planet" in lower and "house" in lower

    timing_count = _count_timing_markers_en(text)
    has_week_timing = _has_week_timing_en(text)
    has_caution = _has_caution_en(text)

    add_parts = []
    if require_saju and (not has_saju or not has_saju_required):
        saju_sentence = _build_saju_evidence_sentence_en(saju_data)
        if saju_sentence:
            add_parts.append(saju_sentence)
    if require_astro and (not has_astro or not has_astro_required):
        astro_sentence = _build_astro_evidence_sentence_en(astro_data)
        if astro_sentence:
            add_parts.append(astro_sentence)
    if require_timing and (timing_count < 2 or not has_week_timing):
        m1 = _add_months(now_date, 1)
        m2 = _add_months(now_date, 3)
        m3 = _add_months(now_date, 5)
        timing_sentence = (
            f"Timing: focus on {_format_month_name(m1)} weeks 1-2, "
            f"{_format_month_name(m2)} weeks 2-3, and "
            f"{_format_month_name(m3)} weeks 3-4 for key moves."
        )
        add_parts.append(timing_sentence)
    if require_caution and not has_caution:
        m2 = _add_months(now_date, 3)
        add_parts.append(
            f"Caution: around {_format_month_name(m2)} weeks 2-3, avoid rushing decisions and double-check details."
        )

    return " ".join([part for part in add_parts if part]).strip()


def _is_truthy(value: object) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return value != 0
    if isinstance(value, str):
        return value.strip().lower() in ("1", "true", "yes", "on")
    return False


def _bool_env(name: str) -> bool:
    return _is_truthy(os.getenv(name, ""))


def _build_rag_debug_addendum(meta: dict, locale: str) -> str:
    if not isinstance(meta, dict) or not meta.get("enabled"):
        return ""

    theme = meta.get("theme", "")
    question = meta.get("question", "")
    graph_nodes = meta.get("graph_nodes", 0)
    corpus_quotes = meta.get("corpus_quotes", 0)
    persona_jung = meta.get("persona_jung", 0)
    persona_stoic = meta.get("persona_stoic", 0)
    cross_analysis = "on" if meta.get("cross_analysis") else "off"
    theme_fusion = "on" if meta.get("theme_fusion") else "off"
    lifespan = "on" if meta.get("lifespan") else "off"
    therapeutic = "on" if meta.get("therapeutic") else "off"

    model = meta.get("model", "")
    temperature = meta.get("temperature", "")
    ab_variant = meta.get("ab_variant", "")

    if locale == "ko":
        return (
            f"[RAG ê·¼ê±° íƒœê·¸] theme={theme} | q=\"{question}\" | graph={graph_nodes} | "
            f"corpus={corpus_quotes} | persona={persona_jung + persona_stoic} | cross={cross_analysis} | fusion={theme_fusion}\n"
            f"[RAG ìš”ì•½] graph_nodes={graph_nodes}; corpus_quotes={corpus_quotes}; "
            f"persona_jung={persona_jung}; persona_stoic={persona_stoic}; "
            f"cross_analysis={cross_analysis}; theme_fusion={theme_fusion}; "
            f"lifespan={lifespan}; therapeutic={therapeutic}; model={model}; temp={temperature}; ab={ab_variant}\n"
        )

    return (
        f"[RAG Evidence Tags] theme={theme} | q=\"{question}\" | graph={graph_nodes} | "
        f"corpus={corpus_quotes} | persona={persona_jung + persona_stoic} | cross={cross_analysis} | fusion={theme_fusion}\n"
        f"[RAG Summary] graph_nodes={graph_nodes}; corpus_quotes={corpus_quotes}; "
        f"persona_jung={persona_jung}; persona_stoic={persona_stoic}; "
        f"cross_analysis={cross_analysis}; theme_fusion={theme_fusion}; "
        f"lifespan={lifespan}; therapeutic={therapeutic}; model={model}; temp={temperature}; ab={ab_variant}\n"
    )


def _coerce_float(value: object, default: Optional[float] = None) -> Optional[float]:
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _coerce_int(value: object, default: Optional[int] = None) -> Optional[int]:
    if value is None:
        return default
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


def _get_int_env(name: str, default: int, min_value: int = 1, max_value: int = 16000) -> int:
    raw = _coerce_int(os.getenv(name), default)
    if raw is None:
        return default
    return max(min_value, min(max_value, raw))


def _clamp_temperature(value: Optional[float], default: float = 0.75) -> float:
    if value is None:
        return default
    return max(0.0, min(2.0, value))


def _select_model_and_temperature(
    data: dict,
    default_model: str,
    default_temp: float,
    session_id: Optional[str],
    request_id: str,
) -> Tuple[str, float, str]:
    model = data.get("model") or data.get("model_name") or default_model
    temperature = _clamp_temperature(_coerce_float(data.get("temperature")), default_temp)

    ab_variant = str(data.get("ab_variant") or "").strip().upper()
    if not ab_variant and _bool_env("RAG_AB_MODE"):
        seed = session_id or request_id or ""
        ab_variant = "A" if (sum(ord(c) for c in seed) % 2 == 0) else "B"

    if ab_variant in ("A", "B"):
        model = os.getenv(f"RAG_AB_MODEL_{ab_variant}") or model
        temperature = _clamp_temperature(
            _coerce_float(os.getenv(f"RAG_AB_TEMP_{ab_variant}")),
            temperature,
        )
    else:
        ab_variant = ""

    return model, temperature, ab_variant


# Health check
def index():
    return jsonify({"status": "ok", "message": "DestinyPal Fusion AI backend is running!"})


# Fusion endpoint with caching and performance optimization
def ask():
    """
    Accepts saju/astro/tarot facts + theme/locale/prompt and runs fusion logic.
    Enhanced with Redis caching and performance monitoring.
    """
    try:
        data = request.get_json(force=True)
        saju_data = data.get("saju") or {}
        astro_data = data.get("astro") or {}
        tarot_data = data.get("tarot") or {}
        theme = data.get("theme", "daily")
        locale = data.get("locale", "en")
        raw_prompt = data.get("prompt") or ""

        # Input validation - check for suspicious patterns
        if is_suspicious_input(raw_prompt):
            logger.warning(f"[ASK] Suspicious input detected: {raw_prompt[:100]}...")

        # Normalize dayMaster structure (nested -> flat)
        saju_data = normalize_day_master(saju_data)

        # Detect structured JSON prompts from frontend (these contain format instructions)
        is_structured_prompt = (
            "You MUST return a valid JSON object" in raw_prompt or
            '"lifeTimeline"' in raw_prompt or
            '"categoryAnalysis"' in raw_prompt
        )
        # Allow full prompt for structured requests, otherwise sanitize and clamp
        prompt = raw_prompt if is_structured_prompt else sanitize_user_input(raw_prompt, max_length=500)
        if is_structured_prompt:
            logger.info(f"[ASK] Detected STRUCTURED JSON prompt (len={len(raw_prompt)})")

        # render_mode: "template" (AI ì—†ì´ ì¦‰ì‹œ) or "gpt" (AI ì‚¬ìš©)
        render_mode = data.get("render_mode", "gpt")
        logger.info(f"[ASK] id={g.request_id} theme={theme} locale={locale} render_mode={render_mode}")

        # DEBUG: Log saju.unse data received from frontend
        unse_data = saju_data.get("unse", {})
        logger.info(f"[ASK] saju.unse received: daeun={len(unse_data.get('daeun', []))}, annual={len(unse_data.get('annual', []))}")

        facts = {
            "theme": theme,
            "saju": saju_data,
            "astro": astro_data,
            "tarot": tarot_data,
            "prompt": prompt,
            "locale": locale,
            "render_mode": render_mode,  # ğŸ”¥ í…œí”Œë¦¿/AI ëª¨ë“œ êµ¬ë¶„
        }

        # Performance monitoring
        start_time = time.time()
        result = interpret_with_ai(facts)
        duration_ms = int((time.time() - start_time) * 1000)

        logger.info(f"[ASK] id={g.request_id} completed in {duration_ms}ms cache_hit={result.get('cached', False)}")

        # Add performance metadata
        if isinstance(result, dict):
            result["performance"] = {
                "duration_ms": duration_ms,
                "cached": result.get("cached", False)
            }

        return jsonify({"status": "success", "data": result})

    except Exception as e:
        logger.exception(f"[ERROR] id={getattr(g, 'request_id', '')} /ask failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def ask_stream():
    """
    Streaming version of /ask for real-time chat responses.
    Uses Server-Sent Events (SSE) for instant first-token response.

    If session_id is provided (from /counselor/init), uses pre-fetched RAG data
    for richer responses with all embedded knowledge.
    """
    try:
        # Ensure UTF-8 encoding for request data (Windows encoding fix)
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        saju_data = data.get("saju") or {}
        astro_data = data.get("astro") or {}
        advanced_astro = data.get("advanced_astro") or {}  # Advanced astrology features
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "chat")
        locale = data.get("locale", "en")
        raw_prompt = data.get("prompt") or ""
        session_id = data.get("session_id")  # Optional: use pre-fetched RAG data
        conversation_history = data.get("history") or []  # Previous messages for context
        user_context = data.get("user_context") or {}  # Premium: persona + session summaries
        cv_text = (data.get("cv_text") or "")[:4000]  # CV/Resume text for career consultations

        # Input validation - sanitize user prompt
        if is_suspicious_input(raw_prompt):
            logger.warning(f"[ASK-STREAM] Suspicious input detected: {raw_prompt[:100]}...")

        # Detect if frontend already sent a fully structured prompt (from chat-stream/route.ts)
        # This includes system prompt, saju/astro data, and advanced analysis
        is_frontend_structured = (
            "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ìš´ëª… ìƒë‹´ì‚¬" in raw_prompt or
            "You are a warm, professional destiny counselor" in raw_prompt or
            "[ì‚¬ì£¼/ì ì„± ê¸°ë³¸ ë°ì´í„°]" in raw_prompt or
            "â˜…â˜…â˜… í•µì‹¬ ê·œì¹™ â˜…â˜…â˜…" in raw_prompt
        )

        prompt = sanitize_user_input(raw_prompt, max_length=8000 if is_frontend_structured else 1500, allow_newlines=True)

        debug_rag = _is_truthy(data.get("debug_rag")) or _bool_env("RAG_DEBUG_RESPONSE")
        debug_log = _is_truthy(data.get("debug_log")) or _bool_env("RAG_DEBUG_LOG") or debug_rag

        current_user_question = ""
        if "ì§ˆë¬¸:" in prompt:
            current_user_question = prompt.split("ì§ˆë¬¸:")[-1].strip()[:500]
        elif "Q:" in prompt:
            current_user_question = prompt.split("Q:")[-1].strip()[:500]
        else:
            current_user_question = prompt[-500:] if prompt else ""

        if is_frontend_structured:
            logger.info(f"[ASK-STREAM] Detected STRUCTURED frontend prompt (len={len(raw_prompt)})")

        # Normalize dayMaster structure (nested -> flat)
        saju_data = normalize_day_master(saju_data)

        logger.info(f"[ASK-STREAM] id={g.request_id} theme={theme} locale={locale} session={session_id or 'none'} history_len={len(conversation_history)} has_user_ctx={bool(user_context)} cv_len={len(cv_text)}")
        logger.info(f"[ASK-STREAM] saju dayMaster: {saju_data.get('dayMaster', {})}")

        # Check for pre-fetched RAG data from session
        session_cache = None
        session_rag_data = {}
        persona_context = {}
        rag_context = ""
        if session_id:
            session_cache = get_session_rag_cache(session_id)
            if session_cache:
                logger.info(f"[ASK-STREAM] Using pre-fetched session data for {session_id}")
                # Use cached saju/astro if not provided in request
                if not saju_data:
                    saju_data = session_cache.get("saju_data", {})
                if not astro_data:
                    astro_data = session_cache.get("astro_data", {})

                # Build rich RAG context from pre-fetched data
                rag_data = session_cache.get("rag_data", {})
                session_rag_data = rag_data

                # GraphRAG context
                if rag_data.get("graph_nodes"):
                    rag_context += "\n[ğŸ“Š ê´€ë ¨ ì§€ì‹ ê·¸ë˜í”„]\n"
                    rag_context += "\n".join(rag_data["graph_nodes"][:8])

                # Jung quotes
                if rag_data.get("corpus_quotes"):
                    rag_context += "\n\n[ğŸ“š ê´€ë ¨ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©]\n"
                    for q in rag_data["corpus_quotes"][:3]:
                        rag_context += f"â€¢ {q.get('text_ko', q.get('text_en', ''))} ({q.get('source', '')})\n"

                # Persona insights
                persona_context = rag_data.get("persona_context", {})
                if persona_context.get("jung"):
                    rag_context += "\n[ğŸ§  ë¶„ì„ê°€ ê´€ì ]\n"
                    rag_context += "\n".join(f"â€¢ {i}" for i in persona_context["jung"][:3])
                if persona_context.get("stoic"):
                    rag_context += "\n\n[âš”ï¸ ìŠ¤í† ì•„ ì² í•™ ê´€ì ]\n"
                    rag_context += "\n".join(f"â€¢ {i}" for i in persona_context["stoic"][:3])

                logger.info(f"[ASK-STREAM] RAG context from session: {len(rag_context)} chars")
            else:
                logger.warning(f"[ASK-STREAM] Session {session_id} not found or expired")

        allow_birth_compute = _bool_env("ALLOW_BIRTH_ONLY")
        if allow_birth_compute and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                saju_data = _calculate_simple_saju(
                    birth_data["date"],
                    birth_data["time"],
                )
                saju_data = normalize_day_master(saju_data)
                logger.info(f"[ASK-STREAM] Computed simple saju from birth: {saju_data.get('dayMaster', {})}")
            except Exception as e:
                logger.warning(f"[ASK-STREAM] Failed to compute simple saju: {e}")

        has_saju_payload = _has_saju_payload(saju_data)
        has_astro_payload = _has_astro_payload(astro_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and (not has_saju_payload or not has_astro_payload):
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[ASK-STREAM] Invalid birth format for missing payload")
                    return _sse_error_response(_build_birth_format_message(locale))
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=not has_saju_payload,
                missing_astro=not has_astro_payload,
            )
            logger.warning("[ASK-STREAM] Missing computed payload(s)")
            return _sse_error_response(missing_message)

        # Build DETAILED chart context (not just summary)
        saju_detail = _build_detailed_saju(saju_data)
        astro_detail = _build_detailed_astro(astro_data)
        advanced_astro_detail = _build_advanced_astro_context(advanced_astro)
        logger.info(f"[ASK-STREAM] saju_detail length: {len(saju_detail)}")
        logger.info(f"[ASK-STREAM] astro_detail length: {len(astro_detail)}")
        if advanced_astro_detail:
            logger.info(f"[ASK-STREAM] advanced_astro_detail length: {len(advanced_astro_detail)}")

        # Get cross-analysis (from session or instant lookup)
        cross_rules = ""
        if session_cache and session_cache.get("rag_data", {}).get("cross_analysis"):
            cross_rules = session_cache["rag_data"]["cross_analysis"]
        else:
            try:
                cross_rules = get_cross_analysis_for_chart(saju_data, astro_data, theme, locale)
                if cross_rules:
                    logger.info(f"[ASK-STREAM] Instant cross-analysis: {len(cross_rules)} chars, theme={theme}")
            except Exception as e:
                logger.warning(f"[ASK-STREAM] Cross-analysis lookup failed: {e}")

        # Get Jung/Stoic insights if not from session (instant lookup)
        instant_quotes = []
        if not rag_context and HAS_CORPUS_RAG:
            try:
                _corpus_rag_inst = get_corpus_rag()
                if _corpus_rag_inst:
                    # Build query from user context
                    theme_concepts = {
                        "career": "vocation calling purpose ì†Œëª… ì§ì—… ìì•„ì‹¤í˜„",
                        "love": "anima animus relationship ê´€ê³„ ì‚¬ë‘ ê·¸ë¦¼ì",
                        "health": "healing wholeness ì¹˜ìœ  í†µí•©",
                        "life_path": "individuation meaning ê°œì„±í™” ì˜ë¯¸ ì„±ì¥",
                        "family": "complex archetype ì½¤í”Œë ‰ìŠ¤ ì›í˜• ê°€ì¡±",
                    }
                    jung_query = f"{theme_concepts.get(theme, theme)} {prompt[:50] if prompt else ''}"
                    quotes = _corpus_rag_inst.search(jung_query, top_k=3, min_score=0.15)
                    if quotes:
                        instant_quotes = quotes
                        rag_context += "\n\n[ğŸ“š ìœµ ì‹¬ë¦¬í•™ í†µì°°]\n"
                        for q in quotes[:2]:
                            quote_text = q.get('quote_kr') or q.get('quote_en', '')
                            if quote_text:
                                rag_context += f"â€¢ \"{quote_text[:150]}...\" â€” ì¹¼ ìœµ, {q.get('source', '')}\n"
                        logger.info(f"[ASK-STREAM] Instant Jung quotes: {len(quotes)} found")
            except Exception as e:
                logger.debug(f"[ASK-STREAM] Instant Jung quotes failed: {e}")

        # Build cross-analysis section
        cross_section = ""
        if cross_rules:
            cross_section = f"\n[ì‚¬ì£¼+ì ì„± êµì°¨ í•´ì„ ê·œì¹™]\n{cross_rules}\n"

        # Current date for time-relevant advice
        now = datetime.now()
        today_date = now.date()
        six_month_date = _add_months(today_date, 6)
        weekdays_ko = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        current_date_str = f"ì˜¤ëŠ˜: {now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekdays_ko[now.weekday()]})"
        timing_window_str = (
            f"íƒ€ì´ë° ê¸°ì¤€: {_format_date_ymd(today_date)} ~ {_format_date_ymd(six_month_date)}"
            if locale == "ko"
            else f"Timing window: {_format_date_ymd(today_date)} to {_format_date_ymd(six_month_date)}"
        )

        # Build user context section for returning users (premium feature)
        user_context_section = ""
        if user_context:
            persona = user_context.get("persona", {})
            recent_sessions = user_context.get("recentSessions", [])
            personality_type = user_context.get("personalityType", {})

            # Nova Personality Type (from personality quiz)
            if personality_type.get("typeCode"):
                type_code = personality_type["typeCode"]
                type_name = personality_type.get("personaName", "")
                user_context_section = f"\n[ğŸ­ ì‚¬ìš©ì ì„±ê²© ìœ í˜•: {type_code}]\n"
                if type_name:
                    user_context_section += f"â€¢ ìœ í˜•ëª…: {type_name}\n"

                # Lookup archetype details for counseling approach
                archetype_hints = {
                    "RVLA": "ì „ëµì  ê´€ì ì—ì„œ ì¡°ì–¸. í° ê·¸ë¦¼ê³¼ ì‹¤í–‰ ê³„íš ì œì‹œ. ì§ì ‘ì  ì†Œí†µ ì„ í˜¸.",
                    "RVLF": "ë‹¤ì–‘í•œ ì˜µì…˜ê³¼ ê°€ëŠ¥ì„± ì œì‹œ. ì‹¤í—˜ì  ì ‘ê·¼ ê¶Œì¥. ì°½ì˜ì  í•´ê²°ì±… íƒìƒ‰.",
                    "RVHA": "ë¹„ì „ê³¼ ì˜ë¯¸ ì—°ê²°. ìŠ¤í† ë¦¬í…”ë§ í™œìš©. ë™ê¸°ë¶€ì—¬ ì¤‘ì‹¬ ì¡°ì–¸.",
                    "RVHF": "ì˜ê°ê³¼ ìƒˆë¡œìš´ ê´€ì  ì œì‹œ. ì‚¬íšŒì  ì—°ê²° ê°•ì¡°. ì—´ì •ì  ì†Œí†µ.",
                    "RSLA": "ëª…í™•í•œ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš ì œì‹œ. ì±…ì„ê³¼ ê²°ê³¼ ê°•ì¡°. íš¨ìœ¨ì  ì¡°ì–¸.",
                    "RSLF": "ì¦‰ê°ì ì´ê³  ì‹¤ìš©ì ì¸ í•´ê²°ì±…. í˜„ì¥ ê°ê° í™œìš©. ìœ„ê¸° ëŒ€ì‘ ê´€ì .",
                    "RSHA": "ê´€ê³„ì™€ ì„±ì¥ ì¤‘ì‹¬. ë”°ëœ»í•˜ë©´ì„œë„ ì²´ê³„ì . ì•ˆì •ì  í™˜ê²½ ê°•ì¡°.",
                    "RSHF": "ì°¸ì—¬ì™€ ì†Œí†µ ì¤‘ì‹¬. ë‹¤ì–‘í•œ ê´€ì  í¬ìš©. í•¨ê»˜í•˜ëŠ” í•´ê²°.",
                    "GVLA": "ê·¼ë³¸ ì›ì¸ ë¶„ì„. ì¥ê¸°ì  ê´€ì . ì²´ê³„ì ì´ê³  ê¹Šì€ í•´ê²°ì±….",
                    "GVLF": "ë°ì´í„°ì™€ ì¦ê±° ê¸°ë°˜ ì ‘ê·¼. ì²´ê³„ì  ë¶„ì„. íŒ¨í„´ í™œìš©.",
                    "GVHA": "ì„±ì¥ê³¼ ë°œì „ ì¤‘ì‹¬. ì¥ê¸°ì  ê´€ê³„. ë©˜í† ë§ ê´€ì .",
                    "GVHF": "ì˜ë¯¸ì™€ ëª©ì  ì—°ê²°. ê¹Šì€ ì§ˆë¬¸. ì§„ì •ì„± ìˆëŠ” ëŒ€í™”.",
                    "GSLA": "ë‹¨ê³„ë³„ ê²€ì¦ëœ ë°©ë²• ê¶Œì¥. ì•ˆì •ì  ì‹¤í–‰ ì§€ì›. ë¦¬ìŠ¤í¬ ê´€ë¦¬.",
                    "GSLF": "ë¬¸ì œë¥¼ ì‘ê²Œ ë¶„í•´. í•˜ë‚˜ì”© ì²´ê³„ì  í•´ê²°. ì •ë°€í•œ ì ‘ê·¼.",
                    "GSHA": "ì•ˆì •ê³¼ ì‹ ë¢° ê¸°ë°˜. ì ì§„ì  ë³€í™” ê¶Œì¥. ê¾¸ì¤€í•œ ì§€ì›.",
                    "GSHF": "ê°ˆë“± í•´ì†Œì™€ ì¡°í™” ì¤‘ì‹¬. ê²½ì²­ê³¼ ì¤‘ì¬. ë¶€ë“œëŸ¬ìš´ ì ‘ê·¼.",
                }
                if type_code in archetype_hints:
                    user_context_section += f"â€¢ ìƒë‹´ ìŠ¤íƒ€ì¼: {archetype_hints[type_code]}\n"

                user_context_section += "\nâ†’ ì´ ì‚¬ìš©ìì˜ ì„±ê²© ìœ í˜•ì— ë§ê²Œ ì†Œí†µ ìŠ¤íƒ€ì¼ì„ ì¡°ì ˆí•˜ì„¸ìš”.\n"
                logger.info(f"[ASK-STREAM] Personality type: {type_code}")

            if persona.get("sessionCount", 0) > 0 or recent_sessions:
                if not user_context_section:
                    user_context_section = "\n[ğŸ”„ ì´ì „ ìƒë‹´ ë§¥ë½]\n"
                else:
                    user_context_section += "\n[ğŸ”„ ì´ì „ ìƒë‹´ ë§¥ë½]\n"

                # Persona memory
                if persona.get("sessionCount"):
                    user_context_section += f"â€¢ ì´ {persona['sessionCount']}íšŒ ìƒë‹´í•œ ì¬ë°©ë¬¸ ê³ ê°\n"
                if persona.get("lastTopics"):
                    topics = persona["lastTopics"][:3] if isinstance(persona["lastTopics"], list) else []
                    if topics:
                        user_context_section += f"â€¢ ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(topics)}\n"
                if persona.get("emotionalTone"):
                    user_context_section += f"â€¢ ê°ì • ìƒíƒœ: {persona['emotionalTone']}\n"
                if persona.get("recurringIssues"):
                    issues = persona["recurringIssues"][:2] if isinstance(persona["recurringIssues"], list) else []
                    if issues:
                        user_context_section += f"â€¢ ë°˜ë³µ ì´ìŠˆ: {', '.join(issues)}\n"

                # Recent session summaries
                if recent_sessions:
                    user_context_section += "\n[ìµœê·¼ ëŒ€í™”]\n"
                    for sess in recent_sessions[:2]:  # Last 2 sessions
                        if sess.get("summary"):
                            user_context_section += f"â€¢ {sess['summary']}\n"
                        elif sess.get("keyTopics"):
                            topics_str = ", ".join(sess["keyTopics"][:3]) if isinstance(sess["keyTopics"], list) else ""
                            if topics_str:
                                user_context_section += f"â€¢ ì£¼ì œ: {topics_str}\n"

                user_context_section += "\nâ†’ ì¬ë°©ë¬¸ ê³ ê°ì´ë‹ˆ 'ë˜ ì˜¤ì…¨ë„¤ìš”' ê°™ì€ ì¹œê·¼í•œ ì¸ì‚¬ë¡œ ì‹œì‘í•˜ê³ , ì´ì „ ìƒë‹´ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì°¸ì¡°í•˜ì„¸ìš”.\n"
                logger.info(f"[ASK-STREAM] User context section: {len(user_context_section)} chars")

        # Build CV/Resume section - use CV whenever available (for career, life_path, chat themes)
        cv_section = ""
        if cv_text:
            cv_section = f"""
[ğŸ“„ ì‚¬ìš©ì ì´ë ¥ì„œ/CV]
{cv_text}

â†’ ìœ„ ì´ë ¥ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ê²½ë ¥, ê¸°ìˆ , ê²½í—˜ì— ë§ëŠ” êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
â†’ ì‚¬ì£¼/ì ì„± í•´ì„ê³¼ ì´ë ¥ì„œ ë‚´ìš©ì„ ì—°ê²°í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.
â†’ ì»¤ë¦¬ì–´, ì§ì—…, ì ì„± ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ì´ë ¥ì„œ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
"""
            logger.info(f"[ASK-STREAM] CV section added: {len(cv_text)} chars, theme={theme}")

        # ======================================================
        # ğŸŒ± LIFESPAN GUIDANCE - Age-appropriate psychological tasks
        # ======================================================
        lifespan_section = ""
        birth_year = None
        try:
            # Extract birth year from birth_data or saju_data
            if birth_data.get("date"):
                birth_year = int(birth_data["date"].split("-")[0])
            elif saju_data.get("birthYear"):
                birth_year = int(saju_data["birthYear"])
        except (ValueError, KeyError, TypeError, AttributeError):
            pass

        if birth_year:
            lifespan_guidance = get_lifespan_guidance(birth_year)
            if lifespan_guidance and lifespan_guidance.get("stage_name"):
                stage = lifespan_guidance
                lifespan_section = f"""
[ğŸŒ± ìƒì• ì£¼ê¸°ë³„ ì‹¬ë¦¬ ê³¼ì œ: {stage['stage_name']} ({stage['age']}ì„¸)]
â€¢ ë°œë‹¬ ê³¼ì œ: {', '.join(stage.get('psychological_tasks', [])[:3])}
â€¢ í•µì‹¬ ì›í˜•: {stage.get('archetypal_themes', {}).get('primary', [''])[0] if isinstance(stage.get('archetypal_themes', {}).get('primary'), list) else ''}
â€¢ í”í•œ ìœ„ê¸°: {', '.join(stage.get('developmental_crises', stage.get('shadow_challenges', []))[:2])}
â€¢ ì‚¬ì£¼ ì—°ê²°: {stage.get('saju_parallel', {}).get('theme', '')}
â€¢ ì ì„± ì—°ê²°: {stage.get('astro_parallel', {}).get('theme', '')}

â†’ ì´ ìƒì•  ë‹¨ê³„ì— ë§ëŠ” ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”. ë‚˜ì´ì— ë§ì§€ ì•ŠëŠ” ì¡°ì–¸(ì˜ˆ: 20ëŒ€ì—ê²Œ 'ì€í‡´ ì¤€ë¹„')ì€ í”¼í•˜ì„¸ìš”.
"""
                logger.info(f"[ASK-STREAM] Lifespan guidance: {stage['stage_name']} (age {stage['age']})")

        # ======================================================
        # ğŸ¯ THEME FUSION RULES - Daily/Monthly/Yearly guidance
        # ======================================================
        theme_fusion_section = ""
        try:
            theme_fusion = get_theme_fusion_rules(saju_data, astro_data, theme, locale, birth_year)
            if theme_fusion:
                theme_fusion_section = f"""
[ğŸ¯ í…Œë§ˆë³„ ìœµí•© í•´ì„]
{theme_fusion}

â†’ ìœ„ í…Œë§ˆë³„ í•´ì„ì„ ìƒë‹´ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì „ë‹¬í•˜ì„¸ìš”.
"""
                logger.info(f"[ASK-STREAM] Theme fusion rules added: {len(theme_fusion)} chars, theme={theme}")
        except Exception as e:
            logger.warning(f"[ASK-STREAM] Theme fusion rules failed: {e}")

        # ======================================================
        # ğŸ¨ ACTIVE IMAGINATION - Deep therapeutic prompts (optional)
        # ======================================================
        imagination_section = ""
        if prompt and any(k in prompt.lower() for k in ["ê¹Šì´", "ë‚´ë©´", "ë¬´ì˜ì‹", "ê·¸ë¦¼ì", "ëª…ìƒ", "ìƒìƒ"]):
            ai_prompts = get_active_imagination_prompts(prompt)
            if ai_prompts:
                imagination_section = f"""
[ğŸ¨ ì ê·¹ì  ìƒìƒ ê¸°ë²• - ì‹¬ì¸µ ì‘ì—…ìš©]
â€¢ ì‹œì‘ ì§ˆë¬¸: {ai_prompts.get('opening', [''])[0] if ai_prompts.get('opening') else ''}
â€¢ ì‹¬í™” ì§ˆë¬¸: {ai_prompts.get('deepening', [''])[0] if ai_prompts.get('deepening') else ''}
â€¢ í†µí•© ì§ˆë¬¸: {ai_prompts.get('integration', [''])[0] if ai_prompts.get('integration') else ''}

â†’ ì‚¬ìš©ìê°€ ê¹Šì€ ë‚´ë©´ ì‘ì—…ì„ ì›í•  ë•Œë§Œ ì´ ì§ˆë¬¸ë“¤ì„ í™œìš©í•˜ì„¸ìš”. ê°•ìš”í•˜ì§€ ë§ˆì„¸ìš”.
"""
                logger.info(f"[ASK-STREAM] Active imagination prompts added")

        # ======================================================
        # ğŸš¨ CRISIS DETECTION - Check for dangerous keywords
        # Only check the CURRENT user question, not history (to avoid false positives)
        # ======================================================
        crisis_response = None
        crisis_check = {"is_crisis": False, "max_severity": "none", "requires_immediate_action": False}
        if HAS_COUNSELING and current_user_question:
            crisis_check = CrisisDetector.detect_crisis(current_user_question)
            if crisis_check["is_crisis"]:
                logger.warning(f"[ASK-STREAM] Crisis detected! severity={crisis_check['max_severity']}")
                crisis_response = CrisisDetector.get_crisis_response(
                    crisis_check["max_severity"],
                    locale=locale
                )
                if crisis_check["requires_immediate_action"]:
                    # Return safety response immediately via SSE
                    def crisis_generator():
                        msg = crisis_response.get("immediate_message", "")
                        if crisis_response.get("follow_up"):
                            msg += "\n\n" + crisis_response["follow_up"]
                        if crisis_response.get("closing"):
                            msg += "\n\n" + crisis_response["closing"]
                        yield f"data: {msg}\n\n"
                        yield "data: [DONE]\n\n"

                    return Response(
                        stream_with_context(crisis_generator()),
                        mimetype="text/event-stream",
                        headers={
                            "Cache-Control": "no-cache",
                            "Connection": "keep-alive",
                            "X-Accel-Buffering": "no",
                        }
                    )

        # Build crisis context for medium/medium_high severity (not immediate, but needs empathetic response)
        crisis_context_section = ""
        if crisis_response and not crisis_check.get("requires_immediate_action"):
            severity = crisis_check.get("max_severity", "")
            if severity == "medium_high":
                crisis_context_section = """
[âš ï¸ ì‚¬ìš©ì ê°ì • ìƒíƒœ: ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤]
- ê³µê°ê³¼ ì•ˆì •ê°ì„ ì£¼ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- ë¨¼ì € ê°ì •ì„ ì¸ì •í•˜ê³  í˜¸í¡/ê·¸ë¼ìš´ë”© ê¸°ë²•ì„ ì•ˆë‚´í•˜ì„¸ìš”
- ì ìˆ  í•´ì„ì€ í¬ë§ì ì¸ ê´€ì ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- í•„ìš”ì‹œ ì „ë¬¸ ìƒë‹´ ê¶Œìœ : ì •ì‹ ê±´ê°•ìœ„ê¸°ìƒë‹´ì „í™” 1577-0199
"""
            elif severity == "medium":
                crisis_context_section = """
[âš ï¸ ì‚¬ìš©ì ê°ì • ìƒíƒœ: í¬ë§ ì €í•˜]
- ê³µê°ê³¼ ë”°ëœ»í•¨ì„ ë‹´ì•„ ì‘ë‹µí•˜ì„¸ìš”
- ì‘ì€ í¬ë§ì´ë¼ë„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”
- ì ìˆ  í•´ì„ì—ì„œ ê¸ì •ì  ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•˜ì„¸ìš”
- "í˜¼ìê°€ ì•„ë‹ˆì—ìš”"ë¼ëŠ” ë©”ì‹œì§€ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ì„¸ìš”
"""
            logger.info(f"[ASK-STREAM] Added crisis context for severity={severity}")

        # Build therapeutic context based on question type - ENHANCED with Jung psychology
        therapeutic_section = ""
        if HAS_COUNSELING and prompt:
            prompt_lower = prompt.lower()
            # Detect question themes and add therapeutic guidance
            if any(k in prompt_lower for k in ["í˜ë“¤", "ìš°ìš¸", "ì§€ì³", "í¬ê¸°", "ì˜ë¯¸ì—†", "í—ˆë¬´"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì˜ë¯¸/ì •ì„œ ì§€ì§€]
- ë¨¼ì € ê°ì •ì„ ì¶©ë¶„íˆ ì¸ì •: "ì •ë§ í˜ë“œì…¨ê² ì–´ìš”... ê·¸ ë¬´ê²Œë¥¼ í˜¼ì ì§€ê³  ê³„ì…¨êµ°ìš”"
- ìœµ ê´€ì : "ì˜í˜¼ì˜ ì–´ë‘ìš´ ë°¤(dark night of soul)"ì€ ë³€í™”ì˜ ì „ì¡°
- ì‚¬ì£¼/ì ì„±ì—ì„œ 'ì „í™˜ì 'ì´ë‚˜ 'ì„±ì¥ê¸°'ë¥¼ ì°¾ì•„ í¬ë§ ì—°ê²°
- ê·¸ë¦¼ì ì‘ì—…: "ì´ í˜ë“¦ì´ ë‹¹ì‹ ì—ê²Œ ê°€ë¥´ì¹˜ë ¤ëŠ” ê²Œ ìˆë‹¤ë©´?"
- ì‘ì€ ì•¡ì…˜ ì œì•ˆ: "ì˜¤ëŠ˜ í•˜ë‚˜ë§Œ ìì‹ ì„ ìœ„í•´ í•œë‹¤ë©´ ë­˜ í•˜ê³  ì‹¶ìœ¼ì„¸ìš”?"
"""
            elif any(k in prompt_lower for k in ["ì—°ì• ", "ì‚¬ë‘", "ê²°í˜¼", "ì´ë³„", "ì§ì‚¬ë‘", "ì¸"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ê´€ê³„/ì‚¬ë‘]
- ê°ì •ì˜ ê¹Šì´ë¥¼ ì¸ì •: "ë§ˆìŒì´ ë§ì´ ì“°ì´ì‹œë„¤ìš”"
- ìœµ ê´€ì  - ì•„ë‹ˆë§ˆ/ì•„ë‹ˆë¬´ìŠ¤ íˆ¬ì‚¬: "ëŒë¦¬ëŠ” ê·¸ íŠ¹ì„±ì´ í˜¹ì‹œ ë‚´ ì•ˆì—ë„ ìˆë‹¤ë©´?"
- ê·¸ë¦¼ì íˆ¬ì‚¬: "ì‹«ì€ ê·¸ ì ... ë‚´ ê·¸ë¦¼ìëŠ” ì•„ë‹ê¹Œìš”?"
- ì‚¬ì£¼ ê´€ì„±(å®˜æ˜Ÿ)/ì ì„± ê¸ˆì„±-7í•˜ìš°ìŠ¤ í•´ì„ì„ ì‹¬ë¦¬ì  íŒ¨í„´ê³¼ ì—°ê²°
- ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬: "ìƒëŒ€ì—ê²Œ ì§„ì§œ ì›í•˜ëŠ” ê±´ ë­˜ê¹Œìš”?" / "ì™„ë²½í•œ ê´€ê³„ë€ ì–´ë–¤ ëª¨ìŠµì´ì—ìš”?"
"""
            elif any(k in prompt_lower for k in ["ì·¨ì—…", "ì´ì§", "ì§„ë¡œ", "ì‚¬ì—…", "í‡´ì‚¬", "ì»¤ë¦¬ì–´"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì»¤ë¦¬ì–´/ì •ì²´ì„±]
- ë¶ˆì•ˆê° ì¸ì •: "ì¤‘ìš”í•œ ê²°ì • ì•ì—ì„œ ê³ ë¯¼ì´ ê¹Šìœ¼ì‹œë„¤ìš”"
- ìœµ ê´€ì  - ì†Œëª…(calling): "ëˆì„ ë– ë‚˜ì„œ, ì§„ì§œ í•˜ê³  ì‹¶ì€ ì¼ì€ ë­ì˜ˆìš”?"
- í˜ë¥´ì†Œë‚˜ vs ìê¸°(Self): "ì¼í•˜ëŠ” ë‚˜ vs ì§„ì§œ ë‚˜, ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€ìš”?"
- ì‚¬ì£¼ ì‹ìƒ/ì¬ì„±ê³¼ ì ì„± 10í•˜ìš°ìŠ¤/MC ì—°ê²°í•˜ì—¬ ì ì„± ë¶„ì„
- êµ¬ì²´ì  ì‹œê¸° ì œì‹œ: "2025ë…„ ìƒë°˜ê¸°ê°€ ì „í™˜ì " ì‹ìœ¼ë¡œ
- ì§ˆë¬¸: "ëˆ vs ë³´ëŒ, ì§€ê¸ˆ ë” ì¤‘ìš”í•œ ê±´?" / "5ë…„ ë’¤ ì–´ë–¤ ëª¨ìŠµì´ê³  ì‹¶ìœ¼ì„¸ìš”?"
"""
            elif any(k in prompt_lower for k in ["ë¶€ëª¨", "ì—„ë§ˆ", "ì•„ë¹ ", "ê°€ì¡±", "í˜•ì œ", "ìë§¤"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ê°€ì¡±/ì½¤í”Œë ‰ìŠ¤]
- ê°€ì¡± ê´€ê³„ì˜ ë³µì¡í•¨ ì¸ì •: "ê°€ì¡±ì´ë¼ ë” ì–´ë µì£ "
- ìœµ ê´€ì  - ë¶€ëª¨ ì½¤í”Œë ‰ìŠ¤: ì–´ë¨¸ë‹ˆ/ì•„ë²„ì§€ ì›í˜•ì´ í˜„ì¬ ê´€ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
- ë‚´ë©´ì•„ì´ ì‘ì—…: "ì–´ë¦° ì‹œì ˆì˜ ë‚˜ì—ê²Œ ë­ë¼ê³  ë§í•´ì£¼ê³  ì‹¶ìœ¼ì„¸ìš”?"
- ì‚¬ì£¼ ì¸ì„±(å°æ˜Ÿ)/ê´€ì„±(å®˜æ˜Ÿ)ê³¼ 4í•˜ìš°ìŠ¤/10í•˜ìš°ìŠ¤ ë¶„ì„
- ì§ˆë¬¸: "ë¶€ëª¨ë‹˜ê»˜ ì§„ì§œ í•˜ê³  ì‹¶ì€ ë§ì€?" / "ìš©ì„œê°€ í•„ìš”í•œ ê±´ ëˆ„êµ¬ì¸ê°€ìš”?"
"""
            elif any(k in prompt_lower for k in ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤", "ë¬´ì„œ"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ë¶ˆì•ˆ/ë‘ë ¤ì›€]
- ë¶ˆì•ˆ ì¸ì •: "ë¶ˆì•ˆí•œ ë§ˆìŒ, ì¶©ë¶„íˆ ì´í•´í•´ìš”"
- ìœµ ê´€ì : ë‘ë ¤ì›€ì€ ê·¸ë¦¼ìê°€ ë³´ë‚´ëŠ” ë©”ì‹œì§€ì¼ ìˆ˜ ìˆìŒ
- ê·¸ë¼ìš´ë”©: "ì§€ê¸ˆ ë°œì´ ë°”ë‹¥ì— ë‹¿ì•„ìˆëŠ” ê±¸ ëŠê»´ë³´ì„¸ìš”"
- ì§ˆë¬¸: "ê·¸ ë‘ë ¤ì›€ì´ ì‚¬ëŒì´ë¼ë©´, ë­ë¼ê³  ë§í•  ê²ƒ ê°™ì•„ìš”?"
- ì‚¬ì£¼/ì ì„±ì—ì„œ ì•ˆì •ê°ì„ ì¤„ ìˆ˜ ìˆëŠ” ì‹œê¸°ë‚˜ ìš”ì†Œ ì°¾ê¸°
"""
            elif any(k in prompt_lower for k in ["ì„±ê²©", "ë‚˜ëŠ”", "ì–´ë–¤ ì‚¬ëŒ", "ì¥ì ", "ë‹¨ì "]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ìê¸°íƒìƒ‰]
- í˜¸ê¸°ì‹¬ í‘œí˜„: "ìì‹ ì„ ì•Œê³  ì‹¶ì€ ë§ˆìŒì´ ë©‹ì§€ë„¤ìš”"
- ìœµ ê´€ì  - í˜ë¥´ì†Œë‚˜/ê·¸ë¦¼ì: ë³´ì—¬ì£¼ëŠ” ë‚˜ vs ìˆ¨ê¸°ëŠ” ë‚˜
- ì‚¬ì£¼ ì¼ê°„ íŠ¹ì„±ê³¼ ì ì„± íƒœì–‘/ìƒìŠ¹/ë‹¬ ì—°ê²°í•˜ì—¬ ë‹¤ì¸µì  ì„±ê²© ë¶„ì„
- ê·¸ë¦¼ì(ì•½ì )ë„ ì„±ì¥ ê°€ëŠ¥ì„±ìœ¼ë¡œ ì¬í•´ì„: "ê·¸ ì ì´ ê±´ê°•í•˜ê²Œ ë°œíœ˜ë˜ë©´?"
- ì§ˆë¬¸: "ê°€ì¥ 'ë‚˜ë‹µë‹¤'ê³  ëŠë‚„ ë•ŒëŠ”?" / "ë‚¨ë“¤ì€ ëª¨ë¥´ëŠ” ë‚˜ë§Œì˜ ëª¨ìŠµì´ ìˆë‹¤ë©´?"
"""
            elif any(k in prompt_lower for k in ["ê¿ˆ", "ì•…ëª½", "ê¿ˆì—ì„œ", "ê¿ˆì„ ê¿¨"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ê¿ˆ í•´ì„]
- í˜¸ê¸°ì‹¬ í‘œí˜„: "í¥ë¯¸ë¡œìš´ ê¿ˆì´ë„¤ìš”. ë¬´ì˜ì‹ì´ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ìˆì–´ìš”"
- ìœµ ê´€ì  - ê¿ˆì€ ë¬´ì˜ì‹ì˜ ì–¸ì–´: ìƒì§•ì  ì˜ë¯¸ íƒìƒ‰
- ê¿ˆì˜ ê°ì •ì— ì£¼ëª©: "ê·¸ ê¿ˆì—ì„œ ì–´ë–¤ ê°ì •ì´ ë“¤ì—ˆì–´ìš”?"
- í˜„ì¬ ìƒí™©ê³¼ ì—°ê²°: "ìš”ì¦˜ ì‚¶ì—ì„œ ë¹„ìŠ·í•œ ëŠë‚Œì´ ë“œëŠ” ê²Œ ìˆë‚˜ìš”?"
- ì ê·¹ì  ìƒìƒ ì œì•ˆ: "ê¿ˆ ì† ì¸ë¬¼ì—ê²Œ ë¬¼ì–´ë³¸ë‹¤ë©´, ë­˜ ë¬»ê³  ì‹¶ìœ¼ì„¸ìš”?"
"""
            elif any(k in prompt_lower for k in ["ì‹«ì–´", "ì§œì¦", "ë¯¸ì›Œ", "í˜ì˜¤"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ê·¸ë¦¼ì íˆ¬ì‚¬]
- ê°ì • ì¸ì •: "ì •ë§ ë¶ˆí¸í•˜ì…¨ê² ì–´ìš”"
- ìœµ ê´€ì  - ê·¸ë¦¼ì íˆ¬ì‚¬: ê°•í•˜ê²Œ ì‹«ì€ ê²ƒì€ ë‚´ ê·¸ë¦¼ìì¼ ìˆ˜ ìˆìŒ
- ì§ˆë¬¸: "ê·¸ ì‚¬ëŒì˜ ì–´ë–¤ ì ì´ ê°€ì¥ ì‹«ìœ¼ì„¸ìš”?"
- ë„ì „: "ê·¸ íŠ¹ì„±ì´ í˜¹ì‹œ ë‚˜í•œí…Œë„ ì¡°ê¸ˆ ìˆë‹¤ë©´?"
- í†µí•©: "ê·¸ ì—ë„ˆì§€ë¥¼ ê±´ê°•í•˜ê²Œ ì“´ë‹¤ë©´ ì–´ë–¤ ëª¨ìŠµì¼ê¹Œìš”?"
"""
            elif any(k in prompt_lower for k in ["ì–¸ì œ", "ì‹œê¸°", "íƒ€ì´ë°", "ëª‡ ì›”", "ì˜¬í•´", "ë‚´ë…„"]):
                therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì‹œê¸°/íƒ€ì´ë°]
- êµ¬ì²´ì  ì‹œê¸° ì œì‹œ í•„ìˆ˜: ì‚¬ì£¼ ëŒ€ìš´/ì„¸ìš´ + ì ì„± íŠ¸ëœì§“ ë¶„ì„
- ì›”/ë¶„ê¸° ë‹¨ìœ„ë¡œ ëª…í™•í•˜ê²Œ: "2025ë…„ 3-4ì›”ì´ ì¢‹ì•„ìš”"
- ì™œ ê·¸ ì‹œê¸°ì¸ì§€ ì„¤ëª…: "ëª©ì„±ì´ ~ì— ë“¤ì–´ì˜¤ë©´ì„œ..."
- ê·¸ ì‹œê¸°ì— í•  ì¼ ì œì•ˆ: "ì´ ì‹œê¸°ì— [êµ¬ì²´ì  í–‰ë™]ì„ ì‹œì‘í•˜ë©´ ì¢‹ê² ì–´ìš”"
- ì£¼ì˜í•  ì‹œê¸°ë„ í•¨ê»˜: "ë‹¤ë§Œ ~ì›”ì€ ì‹ ì¤‘í•˜ê²Œ"
"""

        rag_meta = {}
        if debug_rag or debug_log:
            rag_meta = {
                "enabled": True,
                "theme": theme,
                "question": current_user_question[:120],
                "graph_nodes": len(session_rag_data.get("graph_nodes", [])),
                "corpus_quotes": len(session_rag_data.get("corpus_quotes", [])) or len(instant_quotes),
                "persona_jung": len(persona_context.get("jung", [])),
                "persona_stoic": len(persona_context.get("stoic", [])),
                "cross_analysis": bool(cross_rules),
                "theme_fusion": bool(theme_fusion_section),
                "lifespan": bool(lifespan_section),
                "therapeutic": bool(therapeutic_section),
                "session_rag": bool(session_cache),
            }
            if debug_log:
                logger.info(
                    "[RAG-DEBUG] theme=%s q=%s graph=%s corpus=%s persona=%s cross=%s fusion=%s session=%s",
                    theme,
                    current_user_question[:80],
                    rag_meta["graph_nodes"],
                    rag_meta["corpus_quotes"],
                    rag_meta["persona_jung"] + rag_meta["persona_stoic"],
                    rag_meta["cross_analysis"],
                    rag_meta["theme_fusion"],
                    rag_meta["session_rag"],
                )
                if session_rag_data.get("graph_nodes"):
                    logger.debug("[RAG-DEBUG] graph_nodes_sample=%s", session_rag_data["graph_nodes"][:3])
                if session_rag_data.get("corpus_quotes"):
                    logger.debug("[RAG-DEBUG] corpus_quotes_sample=%s", [
                        q.get("text_ko") or q.get("text_en") for q in session_rag_data["corpus_quotes"][:2]
                    ])

        # ======================================================
        # FRONTEND STRUCTURED PROMPT - Use simplified backend system prompt
        # Frontend already sent complete prompt with all analysis data
        # Backend only adds RAG enrichment (Jung quotes, cross-analysis, etc.)
        # ======================================================
        if is_frontend_structured:
            # Build RAG-only enrichment section
            rag_enrichment_parts = []

            # 1. Cross-analysis rules (ì‚¬ì£¼+ì ì„± êµì°¨ í•´ì„)
            if cross_rules:
                rag_enrichment_parts.append(f"[ğŸ”— ì‚¬ì£¼+ì ì„± êµì°¨ í•´ì„ ê·œì¹™]\n{cross_rules[:1500]}")

            # 2. Jung/Stoic quotes from RAG
            if rag_context:
                rag_enrichment_parts.append(rag_context)

            # 3. Lifespan guidance
            if lifespan_section:
                rag_enrichment_parts.append(lifespan_section)

            # 4. Theme fusion rules
            if theme_fusion_section:
                rag_enrichment_parts.append(theme_fusion_section)

            # 5. Therapeutic guidance based on question type
            if therapeutic_section:
                rag_enrichment_parts.append(therapeutic_section)

            # 6. Crisis context if detected
            if crisis_context_section:
                rag_enrichment_parts.append(crisis_context_section)

            # 7. User context (returning users)
            if user_context_section:
                rag_enrichment_parts.append(user_context_section)

            # 8. CV section for career questions
            if cv_section:
                rag_enrichment_parts.append(cv_section)

            rag_enrichment = "\n\n".join(rag_enrichment_parts) if rag_enrichment_parts else ""

            # Simplified system prompt - frontend prompt is already comprehensive
            # Just add RAG enrichment and remind AI to use all provided data
            if locale == "en":
                system_prompt = f"""You are a Saju+Astrology integrated counselor. Speak naturally and weave the data into your sentences. Start the first sentence directly with an answer (e.g., "So," or "Right now,").

ABSOLUTELY AVOID:
- Formal greetings ("Hello", "Nice to meet you")
- Self-introductions
- Bullet lists or numbered lists
- Bold text

STYLE:
- Conversational and warm, but concise
- Use 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- End with exactly one follow-up question

EVIDENCE REQUIRED (inline, not as a list):
- At least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- At least one Astrology reference (Sun/Moon/ASC plus a planet+house if possible)
- Give 2-3 timing windows within 6 months using month+week phrasing, and include one caution point
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.

{timing_window_str}

Additional knowledge:
{rag_enrichment if rag_enrichment else "(none)"}

Response length: 400-600 words, {locale}, natural spoken tone."""
            else:
                system_prompt = f"""ì‚¬ì£¼+ì ì„± í†µí•© ìƒë‹´ì‚¬. ì¹œêµ¬ì—ê²Œ ë§í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ, ë°ì´í„°ë¥¼ ë…¹ì—¬ì„œ í•´ì„í•´. ì²« ë¬¸ì¥ì€ 'ì´ì•¼'ë¡œ ì‹œì‘í•´(ë§ì¤„ì„í‘œ ê°€ëŠ¥).

ğŸš« ì ˆëŒ€ ê¸ˆì§€:
- "ì¼ê°„ì´ Xì…ë‹ˆë‹¤" ë‚˜ì—´ì‹ ì„¤ëª… (ì‚¬ìš©ìëŠ” ì´ë¯¸ ìê¸° ì°¨íŠ¸ ì•Œê³  ìˆìŒ)
- "ì•ˆë…•í•˜ì„¸ìš”" ì¸ì‚¬
- "ì¡°ì‹¬í•˜ì„¸ìš”" "ì¢‹ì•„ì§ˆ ê±°ì˜ˆìš”" ëœ¬êµ¬ë¦„ ë§
- **ë³¼ë“œì²´**, ë²ˆí˜¸ ë§¤ê¸°ê¸°, ëª©ë¡ ë‚˜ì—´

âœ… ì˜¬ë°”ë¥¸ ìŠ¤íƒ€ì¼:
- ì¹´í˜ì—ì„œ ì¹œêµ¬í•œí…Œ ì–˜ê¸°í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ
- ë°ì´í„°ë¥¼ ë¬¸ì¥ ì†ì— ë…¹ì—¬ì„œ (ë‚˜ì—´ X)
- ì‹¤ìƒí™œê³¼ ì—°ê²°í•´ì„œ ì„¤ëª…
- í•´ìš”ì²´ë¡œ ì¹œê·¼í•˜ê²Œ (ë„ˆë¬´ ë”±ë”±í•œ ë¬¸ì–´ì²´ ê¸ˆì§€)
- ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  ë‹¤ì •í•˜ê²Œ, ë‹¨ì • ëŒ€ì‹  '~ê°™ì•„/ê°€ëŠ¥ì„±' í‘œí˜„ ì‚¬ìš©
- ë¬¸ë‹¨ 3ê°œ ë‚´ì™¸ (í•µì‹¬ ìš”ì•½ â†’ ê·¼ê±°/íŒ¨í„´ â†’ íƒ€ì´ë°/í–‰ë™ + ì§ˆë¬¸)

âœ… ê·¼ê±° í•„ìˆ˜:
- ì‚¬ì£¼ ê·¼ê±° 1ê°œ ì´ìƒ(ì¼ê°„/ëŒ€ìš´/ì„¸ìš´ ì¤‘ 1ê°œ) + ì˜¤í–‰/ì‹­ì„± ë°˜ë“œì‹œ ì–¸ê¸‰
- ì ì„± ê·¼ê±° 1ê°œ ì´ìƒ(íƒœì–‘/ë‹¬/ASC ì¤‘ 1ê°œ) + í–‰ì„±/í•˜ìš°ìŠ¤ ë°˜ë“œì‹œ ì–¸ê¸‰(ê°€ëŠ¥í•˜ë©´ ê° 1ê°œ)
- ê·¼ê±°ëŠ” ë¬¸ì¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ (ë‚˜ì—´ ê¸ˆì§€)
- 6ê°œì›” íƒ€ì´ë° 2~3ê°œë¥¼ ì›”+ì£¼ ë‹¨ìœ„ë¡œ ì œì‹œ(ì˜ˆ: 3ì›” 2~3ì£¼ì°¨)
- íƒ€ì´ë° ì¤‘ 1ê°œëŠ” ì£¼ì˜ì /í”¼í•´ì•¼ í•  í¬ì¸íŠ¸ í¬í•¨
- í…Œë§ˆ ê³ ì •: theme="{theme}"ë§Œ ë‹¤ë£¨ê³  ë‹¤ë¥¸ í…Œë§ˆë¡œ íë¥´ì§€ ë§ ê²ƒ.
- ë§ˆì§€ë§‰ì— í›„ì† ì§ˆë¬¸ 1ê°œ

ğŸ“… {timing_window_str}

ì˜ˆì‹œ) "ë‚˜ëŠ” ì–´ë–¤ ì‚¬ëŒì´ì•¼?" ì§ˆë¬¸:
âŒ ë‚˜ìœ ë‹µ:
"ë‹¹ì‹ ì˜ ì¼ê°„ì€ ì‹ ê¸ˆ(è¾›)ì…ë‹ˆë‹¤. íƒœì–‘ì€ ë¬¼ë³‘ìë¦¬ì…ë‹ˆë‹¤. íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. ë…ë¦½ì 
2. ë¶„ì„ì ..."

âœ… ì¢‹ì€ ë‹µ:
"ì´ ì°¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´, 'ë¨¸ë¦¬ëŠ” ì°¨ê°‘ê²Œ(ë¶„ì„/ì „ëµ), ëˆê³¼ ê¸°íšŒëŠ” ë¹ ë¥´ê²Œ(ì‚¬ì—…ê°ê°), ê´€ê³„ëŠ” ìì¡´ì‹¬ ë•Œë¬¸ì— í•œ ë²ˆì”© ëœ¨ê²ê²Œ' ê°€ëŠ” íƒ€ì…ì´ì—ìš”.

ë¬¼ë³‘ìë¦¬ ASC + íƒœì–‘ì´ë¼ëŠ” í–‰ì„±ì´ 1í•˜ìš°ìŠ¤ë¼ ë…ë¦½ì‹¬ ê°•í•˜ê³  'ë‚´ ë°©ì‹'ì´ í™•ì‹¤í•´ìš”. ìœ í–‰ì— íœ˜ë‘˜ë¦¬ê¸°ë³´ë‹¤ ìƒˆë¡œìš´ ê´€ì /íš¨ìœ¨ì„ ì¢‹ì•„í•˜ì£ . ë§ì´ ë¹ ë¥´ê³  ë…¼ë¦¬ì ì´ë¼ ì¿¨í•˜ê²Œ ë³´ì´ëŠ”ë°, ì‚¬ì‹¤ ì‚¬ëŒ ê´€ì°° ë§ì´ í•˜ëŠ” í¸.

ì‚¬ì£¼ë¡œ ë³´ë©´ ì¼ê°„ ì‹ ê¸ˆ(è¾›)ì´ê³  ì˜¤í–‰ì€ í™”ê°€ ì•½í•œ í¸, ì‹­ì„±ìœ¼ë¡œëŠ” í¸ì¬ê°€ ê°•í•´ì„œ ëˆì˜ íë¦„/ì‹œì¥ ê°ê°ì´ ìˆì–´ìš”. 'ê¸°íšŒ í¬ì°© â†’ êµ¬ì¡° ë§Œë“¤ê¸° â†’ êµ´ë¦¬ê¸°'ì— ì¬ëŠ¥. ë‹¤ë§Œ ì¶”ì§„ë ¥ì˜ ì—°ë£Œê°€ ë“¤ì­‰ë‚ ì­‰í•  ìˆ˜ ìˆì–´ìš”.

ê´€ê³„ì—ì„œëŠ” í™”ì„± ì‚¬ì 7í•˜ìš°ìŠ¤ ì—­í–‰ì´ë¼ ìì¡´ì‹¬Â·ì¸ì • ìš•êµ¬ê°€ ë²„íŠ¼. í‰ì†Œ ì°¸ë‹¤ê°€ ìŒ“ì´ë©´ í„°ì§€ëŠ” íŒ¨í„´ ì£¼ì˜. ì‘ì€ ë¶ˆë§Œì„ 'ì˜ˆì˜ ìˆê²Œ' ìì£¼ ë§í•˜ëŠ” ê²Œ ì˜¤íˆë ¤ ìœ ë¦¬í•´ìš”."

ğŸ“š ì¶”ê°€ ì§€ì‹:
{rag_enrichment if rag_enrichment else "(ì—†ìŒ)"}

ğŸ“Œ 500-800ì, {locale}, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´"""

            logger.info(f"[ASK-STREAM] Using SIMPLIFIED system prompt for frontend-structured request (RAG enrichment: {len(rag_enrichment)} chars)")

        else:
            # ======================================================
            # LEGACY PATH - Build full system prompt (for non-frontend requests)
            # ======================================================
            # Build system prompt - Enhanced counselor persona with Jung-inspired therapeutic approach
            counselor_persona = """ë‹¹ì‹ ì€ ì‚¬ì£¼+ì ì„±ìˆ  í†µí•© ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

âš ï¸ ì ˆëŒ€ ê·œì¹™:
1. ì¸ì‚¬ ê¸ˆì§€ - "ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°€ì›Œìš”" ë“± ì¸ì‚¬ ì ˆëŒ€ ê¸ˆì§€
2. ì‹ ìƒ ì†Œê°œ ê¸ˆì§€ - "ì¼ê°„ì´ Xì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ Y ì„±í–¥" ê°™ì€ ê¸°ë³¸ ì„¤ëª… ê¸ˆì§€. ì‚¬ìš©ìëŠ” ì´ë¯¸ ìê¸° ì‚¬ì£¼ë¥¼ ì•ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸ì— ë‹µí•´.
3. ì œê³µëœ ë°ì´í„°ë§Œ ì‚¬ìš© - ëŒ€ìš´/ì„¸ìš´ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ì•„ë˜ [ì‚¬ì£¼ ë¶„ì„]ì— ìˆëŠ” ê·¸ëŒ€ë¡œë§Œ ì¸ìš©
4. ì²« ë¬¸ì¥ë¶€í„° ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì‹œì‘

ğŸ’¬ ìƒë‹´ ìŠ¤íƒ€ì¼:
â€¢ ìƒì„¸í•˜ê³  ê¹Šì´ ìˆëŠ” ë¶„ì„ (400-600ë‹¨ì–´)
â€¢ ì‚¬ì£¼ì™€ ì ì„±ìˆ  ê· í˜•ìˆê²Œ í™œìš©í•˜ë˜ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´
â€¢ êµ¬ì²´ì  ë‚ ì§œ/ì‹œê¸° ì œì‹œ
â€¢ 'ì™œ ê·¸ëŸ°ì§€' ì´ìœ ë¥¼ ì¶©ë¶„íˆ ì„¤ëª…
â€¢ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©ì´ ìˆìœ¼ë©´ í•´ì„ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ê¹Šì´ ë”í•˜ê¸°"""

            if locale == "en":
                counselor_persona = """You are an integrated Saju + Astrology counselor.

ABSOLUTE RULES:
1. No greetings or self-introductions.
2. Answer the user's question from the first sentence.
3. Use only provided data; do not invent ìš´ or placements.

STYLE:
- 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- Provide concrete timing windows within 6 months, including one caution
- Keep the tone warm and practical
"""

            # Build advanced astrology section (only if data available)
            advanced_astro_section = ""
            if advanced_astro_detail:
                advanced_astro_section = f"""

[ğŸ”­ ì‹¬ì¸µ ì ì„± ë¶„ì„]
{advanced_astro_detail}
"""

        if not is_frontend_structured and rag_context:
            # RICH prompt with all RAG data
            if locale == "en":
                system_prompt = f"""{counselor_persona}

{timing_window_str}

[SAJU ANALYSIS]
{saju_detail}

[ASTROLOGY ANALYSIS]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[RESPONSE RULES]
- Include at least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- Include at least one Astrology reference (Sun/Moon/ASC + planet+house if possible)
- 2-3 timing windows within 6 months (month+week phrasing), include one caution
- End with exactly one follow-up question
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.
- Respond in English only
"""
            else:
                system_prompt = f"""{counselor_persona}

âš ï¸ {current_date_str} - ê³¼ê±° ë‚ ì§œë¥¼ ë¯¸ë˜ì²˜ëŸ¼ ë§í•˜ì§€ ë§ˆì„¸ìš”
âš ï¸ {timing_window_str} - ì´ ë²”ìœ„ ì•ˆì—ì„œ 2~3ê°œ ì‹œê¸°ë¥¼ ì œì‹œí•˜ì„¸ìš”

[ğŸ“Š ì‚¬ì£¼ ë¶„ì„]
{saju_detail}

[ğŸŒŸ ì ì„± ë¶„ì„]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[ğŸ¯ ì‘ë‹µ ìŠ¤íƒ€ì¼]
â€¢ ì²« ë¬¸ì¥ë¶€í„° ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€ - ì‹ ìƒ ì†Œê°œ NO
â€¢ ì‚¬ì£¼ì™€ ì ì„±ìˆ  í†µì°°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì„¤ëª…
â€¢ 'ì™œ ê·¸ëŸ°ì§€' ì´ìœ ë¥¼ ìƒì„¸íˆ í’€ì–´ì„œ ì„¤ëª…
â€¢ êµ¬ì²´ì ì¸ ë‚ ì§œ/ì‹œê¸° ë°˜ë“œì‹œ í¬í•¨
â€¢ ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì–¸ ì œê³µ
â€¢ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©ì´ ìˆìœ¼ë©´ 1-2ë¬¸ì¥ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš© (ë”±ë”±í•˜ê²Œ ì¸ìš© X)

âŒ ì ˆëŒ€ ê¸ˆì§€:
â€¢ ì¸ì‚¬/í™˜ì˜ ë©˜íŠ¸ ("ì•ˆë…•í•˜ì„¸ìš”", "ë‹¤ì‹œ ì°¾ì•„ì£¼ì…¨ë„¤ìš”")
â€¢ ì‹ ìƒ ì†Œê°œ ("ì¼ê°„ì´ Xì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ Y ì„±í–¥" ë“±)
â€¢ ëŒ€ìš´/ì„¸ìš´ ì§€ì–´ë‚´ê¸° (ìœ„ ë°ì´í„°ì— ì—†ëŠ” ê²ƒ ì–¸ê¸‰)
â€¢ ì¶”ìƒì  ë§ë§Œ ë‚˜ì—´ (êµ¬ì²´ì  ì‹œê¸° ì—†ì´)
â€¢ í”¼ìƒì ì´ê³  ì§§ì€ ë‹µë³€

ğŸ“Œ ì‘ë‹µ ê¸¸ì´: 400-600ë‹¨ì–´ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ({locale})"""
        elif not is_frontend_structured:
            # Standard prompt (no session data)
            if locale == "en":
                system_prompt = f"""{counselor_persona}

{timing_window_str}

[SAJU ANALYSIS]
{saju_detail}

[ASTROLOGY ANALYSIS]
{astro_detail}
{advanced_astro_section}{cross_section}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[RESPONSE RULES]
- Include at least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- Include at least one Astrology reference (Sun/Moon/ASC + planet+house if possible)
- 2-3 timing windows within 6 months (month+week phrasing), include one caution
- End with exactly one follow-up question
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.
- Respond in English only
"""
            else:
                system_prompt = f"""{counselor_persona}

âš ï¸ {current_date_str} - ê³¼ê±° ë‚ ì§œë¥¼ ë¯¸ë˜ì²˜ëŸ¼ ë§í•˜ì§€ ë§ˆì„¸ìš”
âš ï¸ {timing_window_str} - ì´ ë²”ìœ„ ì•ˆì—ì„œ 2~3ê°œ ì‹œê¸°ë¥¼ ì œì‹œí•˜ì„¸ìš”

[ğŸ“Š ì‚¬ì£¼ ë¶„ì„]
{saju_detail}

[ğŸŒŸ ì ì„± ë¶„ì„]
{astro_detail}
{advanced_astro_section}{cross_section}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[ğŸ¯ ì‘ë‹µ ìŠ¤íƒ€ì¼]
â€¢ ì²« ë¬¸ì¥ë¶€í„° ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€ - ì‹ ìƒ ì†Œê°œ NO
â€¢ ì‚¬ì£¼ì™€ ì ì„±ìˆ  í†µì°°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì„¤ëª…
â€¢ 'ì™œ ê·¸ëŸ°ì§€' ì´ìœ ë¥¼ ìƒì„¸íˆ í’€ì–´ì„œ ì„¤ëª…
â€¢ êµ¬ì²´ì ì¸ ë‚ ì§œ/ì‹œê¸° ë°˜ë“œì‹œ í¬í•¨
â€¢ ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì–¸ ì œê³µ
â€¢ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©ì´ ìˆìœ¼ë©´ 1-2ë¬¸ì¥ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš© (ë”±ë”±í•˜ê²Œ ì¸ìš© X)

âŒ ì ˆëŒ€ ê¸ˆì§€:
â€¢ ì¸ì‚¬/í™˜ì˜ ë©˜íŠ¸ ("ì•ˆë…•í•˜ì„¸ìš”", "ë‹¤ì‹œ ì°¾ì•„ì£¼ì…¨ë„¤ìš”")
â€¢ ì‹ ìƒ ì†Œê°œ ("ì¼ê°„ì´ Xì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ Y ì„±í–¥" ë“±)
â€¢ ëŒ€ìš´/ì„¸ìš´ ì§€ì–´ë‚´ê¸° (ìœ„ ë°ì´í„°ì— ì—†ëŠ” ê²ƒ ì–¸ê¸‰)
â€¢ ì¶”ìƒì  ë§ë§Œ ë‚˜ì—´ (êµ¬ì²´ì  ì‹œê¸° ì—†ì´)
â€¢ í”¼ìƒì ì´ê³  ì§§ì€ ë‹µë³€

ğŸ“Œ ì‘ë‹µ ê¸¸ì´: 400-600ë‹¨ì–´ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ({locale})"""
        # ======================================================
        # EMOTION TRACKING - Detect user's emotional state
        # ======================================================
        emotion_context = ""
        if prompt:
            prompt_lower = prompt.lower()
            # Detect emotional indicators
            emotions_detected = []
            if any(k in prompt_lower for k in ["í˜ë“¤", "ì§€ì³", "í”¼ê³¤", "ì§€ì¹¨"]):
                emotions_detected.append("exhausted")
            if any(k in prompt_lower for k in ["ìš°ìš¸", "ìŠ¬í¼", "ëˆˆë¬¼", "ìš¸ê³ "]):
                emotions_detected.append("sad")
            if any(k in prompt_lower for k in ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤", "ë¬´ì„œ"]):
                emotions_detected.append("anxious")
            if any(k in prompt_lower for k in ["í™”ë‚˜", "ì§œì¦", "ì–µìš¸", "ë¶„ë…¸"]):
                emotions_detected.append("angry")
            if any(k in prompt_lower for k in ["ì™¸ë¡œ", "í˜¼ì", "ê³ ë…"]):
                emotions_detected.append("lonely")
            if any(k in prompt_lower for k in ["ì„¤ë ˆ", "ê¸°ëŒ€", "í–‰ë³µ", "ì¢‹ì•„"]):
                emotions_detected.append("hopeful")
            if any(k in prompt_lower for k in ["í˜¼ë€", "ëª¨ë¥´ê² ", "ì–´ë–»ê²Œ", "ë­˜ í•´ì•¼"]):
                emotions_detected.append("confused")

            if emotions_detected:
                emotion_map = {
                    "exhausted": "ì§€ì¹¨/í”¼ë¡œ",
                    "sad": "ìŠ¬í””/ìš°ìš¸",
                    "anxious": "ë¶ˆì•ˆ/ê±±ì •",
                    "angry": "ë¶„ë…¸/ë‹µë‹µ",
                    "lonely": "ì™¸ë¡œì›€",
                    "hopeful": "í¬ë§/ì„¤ë ˜",
                    "confused": "í˜¼ë€/ë°©í–¥ìƒì‹¤"
                }
                detected_ko = [emotion_map.get(e, e) for e in emotions_detected]
                emotion_context = f"\n[ğŸ’­ ê°ì§€ëœ ê°ì • ìƒíƒœ: {', '.join(detected_ko)}]\nâ†’ ì´ ê°ì •ì„ ë¨¼ì € ì¸ì •í•˜ê³  ê³µê°í•˜ì„¸ìš”. ì„±ê¸‰íˆ í•´ê²°ì±…ìœ¼ë¡œ ë„˜ì–´ê°€ì§€ ë§ˆì„¸ìš”.\n"
                logger.info(f"[ASK-STREAM] Emotion detected: {emotions_detected}")

        # Add emotion context to system prompt if detected
        if emotion_context:
            system_prompt = system_prompt.replace("[ğŸ“ ì‘ë‹µ êµ¬ì¡°]", f"{emotion_context}\n[ğŸ“ ì‘ë‹µ êµ¬ì¡°]")

        def generate():
            """SSE generator for streaming response."""
            try:
                from openai import OpenAI
                import httpx
                client = OpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    timeout=httpx.Timeout(60.0, connect=10.0)
                )

                # Build messages with conversation history (EXPANDED: last 12 exchanges)
                messages = [{"role": "system", "content": system_prompt}]

                # Add conversation history - increased limit for better context
                history_limit = 12  # 6 user + 6 assistant messages (was 6)
                recent_history = conversation_history[-history_limit:] if conversation_history else []

                # Generate conversation summary for long sessions (>6 messages)
                conversation_summary = ""
                if len(conversation_history) > 6:
                    # Extract key topics from older messages
                    older_msgs = conversation_history[:-6]
                    topics = []
                    for m in older_msgs:
                        if m.get("role") == "user" and m.get("content"):
                            content = m["content"][:100]
                            if any(k in content for k in ["ì—°ì• ", "ì‚¬ë‘", "ê²°í˜¼"]):
                                topics.append("ì—°ì• /ê´€ê³„")
                            elif any(k in content for k in ["ì·¨ì—…", "ì´ì§", "ì»¤ë¦¬ì–´", "ì§„ë¡œ"]):
                                topics.append("ì»¤ë¦¬ì–´/ì§„ë¡œ")
                            elif any(k in content for k in ["í˜ë“¤", "ìš°ìš¸", "ì§€ì³"]):
                                topics.append("ê°ì •ì  ì–´ë ¤ì›€")
                            elif any(k in content for k in ["ë‚˜ëŠ”", "ì„±ê²©", "ì–´ë–¤ ì‚¬ëŒ"]):
                                topics.append("ìê¸°íƒìƒ‰")
                    if topics:
                        unique_topics = list(dict.fromkeys(topics))[:3]
                        conversation_summary = f"[ğŸ“‹ ì´ì „ ëŒ€í™” ìš”ì•½: {', '.join(unique_topics)} ì£¼ì œë¡œ ëŒ€í™”í•¨]\n"

                # Add summary if available
                if conversation_summary:
                    messages.append({
                        "role": "system",
                        "content": conversation_summary
                    })

                # Smart truncation: recent messages get more space
                for idx, msg in enumerate(recent_history):
                    if msg.get("role") in ("user", "assistant") and msg.get("content"):
                        # Older messages: shorter, Recent messages: longer
                        is_recent = idx >= len(recent_history) - 4
                        max_len = 800 if is_recent else 300
                        messages.append({
                            "role": msg["role"],
                            "content": msg["content"][:max_len]
                        })

                # Add current user message
                messages.append({"role": "user", "content": prompt})

                default_model = os.getenv("CHAT_MODEL") or os.getenv("FUSION_MODEL") or "gpt-4.1"
                default_temp = _clamp_temperature(_coerce_float(os.getenv("CHAT_TEMPERATURE")), 0.75)
                model_name, temperature, ab_variant = _select_model_and_temperature(
                    data,
                    default_model,
                    default_temp,
                    session_id,
                    g.request_id,
                )
                if debug_rag or debug_log:
                    rag_meta["model"] = model_name
                    rag_meta["temperature"] = temperature
                    rag_meta["ab_variant"] = ab_variant or ""
                if debug_log:
                    logger.info(
                        "[RAG-DEBUG] model=%s temp=%s ab=%s",
                        model_name,
                        temperature,
                        ab_variant or "default",
                    )
                max_tokens = _get_int_env("ASK_STREAM_MAX_TOKENS", 1600, min_value=400, max_value=4000)
                stream = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,  # Slightly more creative (was 0.7)
                    stream=True
                )

                full_text = ""

                for chunk in stream:
                    if not chunk.choices or not chunk.choices[0].delta.content:
                        continue
                    full_text += chunk.choices[0].delta.content

                full_text = _ensure_ko_prefix(full_text, locale)

                if full_text.strip().startswith("[ERROR]") or not full_text.strip():
                    yield "data: [DONE]\n\n"
                    return

                addendum = _build_missing_requirements_addendum(
                    full_text,
                    locale,
                    saju_data,
                    astro_data,
                    today_date,
                )
                if addendum:
                    full_text = _insert_addendum(full_text, addendum)

                debug_addendum = _build_rag_debug_addendum(rag_meta, locale) if debug_rag else ""
                if debug_addendum:
                    sep = "\n\n" if full_text else ""
                    full_text = f"{full_text}{sep}{debug_addendum}"

                full_text = _format_korean_spacing(full_text)
                if debug_rag and full_text:
                    full_text = full_text.rstrip() + "\n"

                if locale == "ko" and not full_text.rstrip().endswith("?"):
                    followup = "í˜¹ì‹œ ì§€ê¸ˆ ê°€ì¥ ê¶ê¸ˆí•œ í¬ì¸íŠ¸ê°€ ë­ì˜ˆìš”?"
                    separator = "" if (full_text.endswith((" ", "\n", "\t")) or not full_text) else " "
                    full_text += f"{separator}{followup}"

                chunk_size = _get_stream_chunk_size()
                for piece in _chunk_text(full_text, chunk_size):
                    yield _to_sse_event(piece)

                # Signal end of stream
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"[ASK-STREAM] Streaming error: {e}")
                yield f"data: [ERROR] {str(e)}\n\n"

        return Response(
            stream_with_context(generate()),
            mimetype="text/event-stream",
            headers={
                "Content-Type": "text/event-stream; charset=utf-8",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable nginx buffering
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] id={getattr(g, 'request_id', '')} /ask-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def counselor_init():
    """
    Initialize counselor session with pre-fetched RAG data.
    Call this ONCE when user enters counselor chat (before first message).

    This pre-computes all relevant RAG data (~10-20s) so subsequent
    chat messages are instant.

    Request body:
        {
            "saju": {...},      # User's saju data
            "astro": {...},     # User's astrology data
            "theme": "career",  # Optional theme (default: "chat")
        }

    Response:
        {
            "status": "success",
            "session_id": "abc123",
            "prefetch_time_ms": 15234,
            "data_summary": {
                "graph_nodes": 15,
                "corpus_quotes": 5,
                "persona_insights": 10
            }
        }
    """
    try:
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        saju_data = data.get("saju") or {}
        astro_data = data.get("astro") or {}
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "chat")
        locale = data.get("locale", "ko")

        # Normalize dayMaster structure (nested -> flat)
        saju_data = normalize_day_master(saju_data)

        has_saju_payload = _has_saju_payload(saju_data)
        has_astro_payload = _has_astro_payload(astro_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and (not has_saju_payload or not has_astro_payload):
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[COUNSELOR-INIT] Invalid birth format for missing payload")
                    return jsonify({"status": "error", "message": _build_birth_format_message(locale)}), 400
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=not has_saju_payload,
                missing_astro=not has_astro_payload,
            )
            logger.warning("[COUNSELOR-INIT] Missing computed payload(s)")
            return jsonify({"status": "error", "message": missing_message}), 400

        logger.info(f"[COUNSELOR-INIT] id={g.request_id} theme={theme}")
        logger.info(f"[COUNSELOR-INIT] saju dayMaster: {saju_data.get('dayMaster', {})}")
        logger.info(f"[COUNSELOR-INIT] astro_data keys: {list(astro_data.keys()) if astro_data else 'empty'}")

        allow_birth_compute = _bool_env("ALLOW_BIRTH_ONLY")
        if allow_birth_compute and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                saju_data = _calculate_simple_saju(
                    birth_data["date"],
                    birth_data["time"],
                )
                saju_data = normalize_day_master(saju_data)
                logger.info(f"[COUNSELOR-INIT] Computed simple saju from birth data: {saju_data.get('dayMaster', {})}")
            except Exception as e:
                logger.warning(f"[COUNSELOR-INIT] Failed to compute simple saju: {e}")

        # Generate session ID
        session_id = str(uuid4())[:12]

        # Pre-fetch ALL RAG data (this is slow but only happens once)
        rag_data = prefetch_all_rag_data(saju_data, astro_data, theme, locale)

        # Store in session cache
        set_session_rag_cache(session_id, {
            "rag_data": rag_data,
            "saju_data": saju_data,
            "astro_data": astro_data,
            "theme": theme,
        })

        return jsonify({
            "status": "success",
            "session_id": session_id,
            "prefetch_time_ms": rag_data.get("prefetch_time_ms", 0),
            "data_summary": {
                "graph_nodes": len(rag_data.get("graph_nodes", [])),
                "corpus_quotes": len(rag_data.get("corpus_quotes", [])),
                "persona_insights": len(rag_data.get("persona_context", {}).get("jung", [])) +
                                   len(rag_data.get("persona_context", {}).get("stoic", [])),
                "has_cross_analysis": bool(rag_data.get("cross_analysis")),
            }
        })

    except Exception as e:
        logger.exception(f"[ERROR] id={getattr(g, 'request_id', '')} /counselor/init failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Saju calc
def calc_saju():
    try:
        body = request.get_json(force=True)
        birth_date = body.get("birth_date")
        birth_time = body.get("birth_time")
        gender = body.get("gender", "male")

        result = calculate_saju_data(birth_date, birth_time, gender)
        return jsonify({"status": "success", "saju": result})
    except Exception as e:
        logger.exception(f"[ERROR] /calc_saju failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Astrology calc
def calc_astro():
    try:
        body = request.get_json(force=True)
        result = calculate_astrology_data(
            {
                "year": body.get("year"),
                "month": body.get("month"),
                "day": body.get("day"),
                "hour": body.get("hour"),
                "minute": body.get("minute"),
                "latitude": body.get("latitude"),
                "longitude": body.get("longitude"),
            }
        )
        return jsonify({"status": "success", "astro": result})
    except Exception as e:
        logger.exception(f"[ERROR] /calc_astro failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Dream interpretation endpoint
@app.route("/api/dream/interpret-stream", methods=["POST"])
def dream_interpret_stream():
    """
    Streaming dream interpretation - returns SSE for real-time display.
    Uses GPT-4o-mini for fast streaming.
    Streams: summary â†’ symbols â†’ recommendations â†’ done
    """
    try:
        data = request.get_json(force=True)
        logger.info(f"[DREAM_STREAM] id={g.request_id} Starting streaming interpretation")

        raw_dream_text = data.get("dream", "")
        symbols = data.get("symbols", [])
        emotions = data.get("emotions", [])
        themes = data.get("themes", [])
        context = data.get("context", [])
        locale = data.get("locale", "ko")

        # Input validation - sanitize dream text
        if is_suspicious_input(raw_dream_text):
            logger.warning(f"[DREAM_STREAM] Suspicious input detected")
        dream_text = sanitize_dream_text(raw_dream_text)

        # Cultural symbols
        cultural_parts = []
        if data.get("koreanTypes"):
            cultural_parts.append(f"Korean Types: {', '.join(data['koreanTypes'])}")
        if data.get("koreanLucky"):
            cultural_parts.append(f"Korean Lucky: {', '.join(data['koreanLucky'])}")
        if data.get("chinese"):
            cultural_parts.append(f"Chinese: {', '.join(data['chinese'])}")
        if data.get("islamicTypes"):
            cultural_parts.append(f"Islamic Types: {', '.join(data['islamicTypes'])}")
        if data.get("western"):
            cultural_parts.append(f"Western/Jungian: {', '.join(data['western'])}")
        if data.get("hindu"):
            cultural_parts.append(f"Hindu: {', '.join(data['hindu'])}")
        if data.get("japanese"):
            cultural_parts.append(f"Japanese: {', '.join(data['japanese'])}")

        cultural_context = '\n'.join(cultural_parts) if cultural_parts else 'None'

        is_korean = locale == "ko"
        lang_instruction = "Please respond entirely in Korean (í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”)." if is_korean else "Please respond in English."

        def generate_stream():
            """Generator for SSE streaming dream interpretation"""
            try:
                if not OPENAI_AVAILABLE or not openai_client:
                    yield f"data: {json.dumps({'error': 'OpenAI not available'})}\n\n"
                    return

                # === SECTION 1: Summary (streaming) ===
                yield f"data: {json.dumps({'section': 'summary', 'status': 'start'})}\n\n"

                summary_prompt = f"""ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ê¿ˆ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ë§ˆì¹˜ ì˜¤ëœ ì¹œêµ¬ì—ê²Œ ì´ì•¼ê¸°í•˜ë“¯ í¸ì•ˆí•˜ê²Œ ê¿ˆì˜ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•´ì£¼ì„¸ìš”.

{lang_instruction}

ê¿ˆ ë‚´ìš©:
{dream_text[:1500]}

ì‹¬ë³¼: {', '.join(symbols) if symbols else 'ì—†ìŒ'}
ê°ì •: {', '.join(emotions) if emotions else 'ì—†ìŒ'}
ìœ í˜•: {', '.join(themes) if themes else 'ì—†ìŒ'}
ìƒí™©: {', '.join(context) if context else 'ì—†ìŒ'}
ë¬¸í™”ì  ë§¥ë½: {cultural_context}

ìƒë‹´ ìŠ¤íƒ€ì¼:
- ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ë§íˆ¬ ("~í•˜ì…¨êµ°ìš”", "~ëŠë¼ì…¨ì„ ê±°ì˜ˆìš”")
- ê¿ˆì´ ì „í•˜ëŠ” ë©”ì‹œì§€ë¥¼ ë¶€ë“œëŸ½ê²Œ í•´ì„
- ë¶ˆì•ˆí•œ ê¿ˆì´ë¼ë„ ê¸ì •ì  ê´€ì ìœ¼ë¡œ ì¬í•´ì„
- 3-4ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½"""

                stream = openai_client.chat.completions.create(
                    model="gpt-4o",  # Upgraded for better dream interpretation quality
                    messages=[{"role": "user", "content": summary_prompt}],
                    temperature=0.7,
                    max_tokens=400,
                    stream=True
                )

                summary_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        summary_text += content
                        yield f"data: {json.dumps({'section': 'summary', 'content': content})}\n\n"

                yield f"data: {json.dumps({'section': 'summary', 'status': 'done', 'full_text': summary_text})}\n\n"

                # === SECTION 2: Symbol Analysis (streaming) ===
                yield f"data: {json.dumps({'section': 'symbols', 'status': 'start'})}\n\n"

                symbols_prompt = f"""ë‹¹ì‹ ì€ ë”°ëœ»í•œ ê¿ˆ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê¿ˆì— ë‚˜íƒ€ë‚œ ì‹¬ë³¼ë“¤ì˜ ì˜ë¯¸ë¥¼ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

{lang_instruction}

ê¿ˆ ë‚´ìš©: {dream_text[:1000]}
ì‹¬ë³¼: {', '.join(symbols) if symbols else 'ê¿ˆì—ì„œ ì¶”ì¶œ'}
ë¬¸í™”ì  ë§¥ë½: {cultural_context}

ìƒë‹´ ìŠ¤íƒ€ì¼:
- ê° ì‹¬ë³¼ì„ ê°œì¸ì˜ ìƒí™©ê³¼ ì—°ê²°í•˜ì—¬ í•´ì„
- ë¬¸í™”ì Â·ì‹¬ë¦¬í•™ì  ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
- ë¶€ì •ì  ì‹¬ë³¼ë„ ì„±ì¥ì˜ ë©”ì‹œì§€ë¡œ ì¬í•´ì„
- ë²ˆí˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ 2-3ê°œ ì‹¬ë³¼ ë¶„ì„"""

                symbol_stream = openai_client.chat.completions.create(
                    model="gpt-4o",  # Upgraded for better symbol interpretation
                    messages=[{"role": "user", "content": symbols_prompt}],
                    temperature=0.7,
                    max_tokens=500,
                    stream=True
                )

                symbols_text = ""
                for chunk in symbol_stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        symbols_text += content
                        yield f"data: {json.dumps({'section': 'symbols', 'content': content})}\n\n"

                yield f"data: {json.dumps({'section': 'symbols', 'status': 'done', 'full_text': symbols_text})}\n\n"

                # === SECTION 3: Recommendations (streaming) ===
                yield f"data: {json.dumps({'section': 'recommendations', 'status': 'start'})}\n\n"

                rec_prompt = f"""ë‹¹ì‹ ì€ ë”°ëœ»í•œ ê¿ˆ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê¿ˆì˜ ë©”ì‹œì§€ë¥¼ ì‹¤ìƒí™œì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.

{lang_instruction}

ê¿ˆ ìš”ì•½: {summary_text[:500]}
ê°ì •: {', '.join(emotions) if emotions else 'ì—†ìŒ'}

ìƒë‹´ ìŠ¤íƒ€ì¼:
- ì¹œêµ¬ì—ê²Œ ì¡°ì–¸í•˜ë“¯ í¸ì•ˆí•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ
- ì‘ì€ ì‹¤ì²œ ê°€ëŠ¥í•œ í–‰ë™ ì œì•ˆ (ì˜ˆ: "ì˜¤ëŠ˜ ì ê¹ ì‚°ì±…í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?")
- ê¿ˆì´ ì „í•˜ëŠ” ê¸ì •ì  ë©”ì‹œì§€ ê°•ì¡°
- 2-3ê°€ì§€ ë”°ëœ»í•œ ì¡°ì–¸"""

                rec_stream = openai_client.chat.completions.create(
                    model="gpt-4o",  # Upgraded for better recommendations
                    messages=[{"role": "user", "content": rec_prompt}],
                    temperature=0.7,
                    max_tokens=300,
                    stream=True
                )

                rec_text = ""
                for chunk in rec_stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        rec_text += content
                        yield f"data: {json.dumps({'section': 'recommendations', 'content': content})}\n\n"

                yield f"data: {json.dumps({'section': 'recommendations', 'status': 'done', 'full_text': rec_text})}\n\n"

                # === DONE ===
                yield f"data: {json.dumps({'done': True})}\n\n"

            except Exception as stream_error:
                logger.exception(f"[DREAM_STREAM] Error: {stream_error}")
                yield f"data: {json.dumps({'error': str(stream_error)})}\n\n"

        return Response(
            generate_stream(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] /api/dream/interpret-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/api/dream/chat-stream", methods=["POST"])
def dream_chat_stream():
    """
    Streaming dream follow-up chat - Enhanced with RAG + Saju + Celestial context.
    Returns Server-Sent Events (SSE) for real-time text streaming.

    Request body:
        {
            "messages": [{"role": "user"|"assistant", "content": "..."}],
            "dream_context": {
                "dream_text": "ì›ë˜ ê¿ˆ ë‚´ìš©",
                "summary": "í•´ì„ ìš”ì•½",
                "symbols": ["symbol1", "symbol2"],
                "emotions": ["emotion1"],
                "themes": ["theme1"],
                "recommendations": ["recommendation1"],
                "cultural_notes": {"korean": "...", "western": "..."},
                "celestial": {...},  # Optional: moon phase, retrogrades
                "saju": {...}  # Optional: birth data for saju context
            },
            "language": "ko"|"en"
        }
    """
    try:
        data = request.get_json(force=True)
        logger.info(f"[DREAM_CHAT_STREAM] id={g.request_id} Processing enhanced streaming chat with RAG")

        raw_messages = data.get("messages", [])
        dream_context = data.get("dream_context", {})
        language = data.get("language", "ko")
        session_id = data.get("session_id")  # Optional session ID for continuity

        # Sanitize all messages
        messages = sanitize_messages(raw_messages)

        if not messages:
            return jsonify({"status": "error", "message": "No messages provided"}), 400

        # Extract dream context
        dream_text = dream_context.get("dream_text", "")
        summary = dream_context.get("summary", "")
        symbols = dream_context.get("symbols", [])
        emotions = dream_context.get("emotions", [])
        themes = dream_context.get("themes", [])
        recommendations = dream_context.get("recommendations", [])
        cultural_notes = dream_context.get("cultural_notes", {})
        celestial = dream_context.get("celestial", {})
        saju_data = dream_context.get("saju", {})

        # Get last user message for RAG search
        last_user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")
                break

        # ============================================================
        # SESSION MANAGEMENT: Get or create counseling session
        # ============================================================
        counseling_engine = None
        counseling_session = None
        try:
            counseling_engine = get_counseling_engine()
            if counseling_engine and session_id:
                # Try to retrieve existing session
                counseling_session = counseling_engine.get_session(session_id)
                if counseling_session:
                    logger.info(f"[DREAM_CHAT_STREAM] Retrieved existing session: {session_id}, phase: {counseling_session.current_phase}")
                else:
                    # Create new session with provided ID
                    counseling_session = counseling_engine.create_session()
                    counseling_session.session_id = session_id
                    counseling_engine.sessions[session_id] = counseling_session
                    logger.info(f"[DREAM_CHAT_STREAM] Created new session: {session_id}")
            elif counseling_engine:
                # Create new session
                counseling_session = counseling_engine.create_session()
                logger.info(f"[DREAM_CHAT_STREAM] Created new session: {counseling_session.session_id}")
        except Exception as session_error:
            logger.warning(f"[DREAM_CHAT_STREAM] Session management failed: {session_error}")

        # ============================================================
        # CRISIS DETECTION: Use CounselingEngine's advanced crisis detection
        # ============================================================
        crisis_response = None
        try:
            # Use advanced CounselingEngine crisis detector (5-level severity)
            if not counseling_engine:
                counseling_engine = get_counseling_engine()
            if counseling_engine:
                crisis_detector = counseling_engine.crisis_detector
                crisis_check = crisis_detector.detect_crisis(last_user_message)

                if crisis_check["is_crisis"]:
                    # Get detailed crisis response
                    crisis_data = crisis_detector.get_crisis_response(
                        crisis_check["max_severity"],
                        locale=language
                    )
                    crisis_response = {
                        "type": "crisis",
                        "severity": crisis_check["max_severity"],
                        "response": crisis_data.get("immediate_message", ""),
                        "resources": crisis_data.get("resources", {}),
                        "requires_immediate_action": crisis_check["requires_immediate_action"]
                    }
                    logger.warning(f"[DREAM_CHAT_STREAM] Advanced crisis detected: severity={crisis_check['max_severity']}, immediate_action={crisis_check['requires_immediate_action']}")
            else:
                # Fallback to dream_embeddings CrisisDetector
                from backend_ai.app.dream_embeddings import CrisisDetector
                crisis_check = CrisisDetector.check_crisis(last_user_message)
                if crisis_check:
                    crisis_response = crisis_check
                    logger.warning(f"[DREAM_CHAT_STREAM] Fallback crisis detected: type={crisis_check['type']}")
        except Exception as crisis_error:
            logger.warning(f"[DREAM_CHAT_STREAM] Crisis detection failed: {crisis_error}")

        # ============================================================
        # RAG SEARCH: Find relevant dream interpretations for the question
        # ============================================================
        rag_context = ""
        therapeutic_context = ""
        counseling_context = ""

        try:
            from backend_ai.app.dream_logic import get_dream_embed_rag
            dream_rag = get_dream_embed_rag()

            # Search based on: original dream + user's current question
            search_query = f"{dream_text[:300]} {last_user_message}"
            rag_results = dream_rag.get_interpretation_context(search_query, top_k=6)

            if rag_results.get("texts"):
                rag_texts = rag_results.get("texts", [])[:5]
                korean_notes_rag = rag_results.get("korean_notes", [])[:3]
                specifics = rag_results.get("specifics", [])[:4]
                advice_rag = rag_results.get("advice", [])[:3]
                categories = rag_results.get("categories", [])

                rag_context = "\n\n[ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ - ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”]\n"

                if rag_texts:
                    rag_context += "\nê´€ë ¨ í•´ì„:\n" + "\n".join([f"â€¢ {t}" for t in rag_texts])

                if korean_notes_rag:
                    rag_context += "\n\ní•œêµ­ ì „í†µ í•´ëª½:\n" + "\n".join([f"â€¢ {n}" for n in korean_notes_rag])

                if specifics:
                    rag_context += "\n\nìƒì„¸ ìƒí™©ë³„ í•´ì„:\n" + "\n".join([f"â€¢ {s}" for s in specifics])

                if advice_rag:
                    rag_context += "\n\nì „í†µ ì¡°ì–¸:\n" + "\n".join([f"â€¢ {a}" for a in advice_rag])

                if categories:
                    rag_context += f"\n\nê¿ˆ ì¹´í…Œê³ ë¦¬: {', '.join(categories)}"

                logger.info(f"[DREAM_CHAT_STREAM] RAG found {len(rag_texts)} relevant texts, quality={rag_results.get('match_quality')}")

            # ============================================================
            # THERAPEUTIC QUESTIONS: Get Jung-based therapeutic questions
            # ============================================================
            therapeutic_data = dream_rag.get_therapeutic_questions(dream_text + " " + last_user_message)
            if therapeutic_data.get("therapeutic_questions"):
                therapeutic_context = "\n\n[ğŸ§  ìœµ ì‹¬ë¦¬í•™ ì¹˜ë£Œì  ì§ˆë¬¸ - ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”]\n"
                therapeutic_context += f"í†µì°°: {therapeutic_data.get('insight', '')}\n"
                therapeutic_context += "ì¹˜ë£Œì  ì§ˆë¬¸:\n" + "\n".join([f"â€¢ {q}" for q in therapeutic_data['therapeutic_questions'][:3]])

            # ============================================================
            # COUNSELING CONTEXT: Get scenario-based counseling insights
            # ============================================================
            counseling_data = dream_rag.get_counseling_context(last_user_message)
            if counseling_data.get("jungian_concept"):
                counseling_context = "\n\n[ğŸ’­ ìƒë‹´ ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸]\n"
                counseling_context += f"ìœµ ê°œë…: {counseling_data.get('jungian_concept', '')}\n"
                counseling_context += f"í•´ì„: {counseling_data.get('interpretation', '')}\n"
                if counseling_data.get("key_questions"):
                    counseling_context += "í•µì‹¬ ì§ˆë¬¸:\n" + "\n".join([f"â€¢ {q}" for q in counseling_data['key_questions'][:2]])
                if counseling_data.get("reframes"):
                    counseling_context += "\në¦¬í”„ë ˆì´ë°:\n" + "\n".join([f"â€¢ {r}" for r in counseling_data['reframes']])

        except Exception as rag_error:
            logger.warning(f"[DREAM_CHAT_STREAM] RAG search failed (continuing without): {rag_error}")

        # ============================================================
        # CELESTIAL CONTEXT: Moon phase and planetary influences
        # ============================================================
        celestial_context = ""
        if celestial:
            moon_phase = celestial.get("moon_phase", {})
            moon_sign = celestial.get("moon_sign", {})
            retrogrades = celestial.get("retrogrades", [])

            if moon_phase or moon_sign:
                celestial_context = "\n\n[ğŸŒ™ í˜„ì¬ ì²œì²´ ìƒí™©]\n"

                if moon_phase:
                    phase_name = moon_phase.get("korean", moon_phase.get("name", ""))
                    phase_emoji = moon_phase.get("emoji", "ğŸŒ™")
                    dream_meaning = moon_phase.get("dream_meaning", "")
                    celestial_context += f"ë‹¬ì˜ ìœ„ìƒ: {phase_emoji} {phase_name}\n"
                    if dream_meaning:
                        celestial_context += f"ê¿ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥: {dream_meaning}\n"

                if moon_sign:
                    sign_korean = moon_sign.get("korean", moon_sign.get("sign", ""))
                    dream_flavor = moon_sign.get("dream_flavor", "")
                    celestial_context += f"ë‹¬ ë³„ìë¦¬: {sign_korean}\n"
                    if dream_flavor:
                        celestial_context += f"ê¿ˆ ì„±ê²©: {dream_flavor}\n"

                if retrogrades:
                    retro_names = [r.get("korean", r.get("planet", "")) for r in retrogrades[:3]]
                    celestial_context += f"ì—­í–‰ ì¤‘ì¸ í–‰ì„±: {', '.join(retro_names)}\n"
        else:
            # Try to get current celestial data if not provided
            try:
                if HAS_REALTIME:
                    transits = get_current_transits()
                    if transits:
                        moon_phase = transits.get("moon_phase", {})
                        if moon_phase:
                            phase_name = moon_phase.get("korean", moon_phase.get("name", ""))
                            phase_emoji = moon_phase.get("emoji", "ğŸŒ™")
                            celestial_context = f"\n\n[ğŸŒ™ í˜„ì¬ ë‹¬ ìœ„ìƒ: {phase_emoji} {phase_name}]\n"
            except Exception:
                pass

        # ============================================================
        # SAJU CONTEXT: User's fortune influence on dreams
        # ============================================================
        saju_context = ""
        if saju_data and saju_data.get("birth_date"):
            try:
                birth_date = saju_data.get("birth_date", "")
                birth_time = saju_data.get("birth_time", "")

                # Calculate current saju if we have birth data
                saju_result = calculate_saju_data(
                    birth_date=birth_date,
                    birth_time=birth_time or "12:00",
                    birth_city=saju_data.get("birth_city", "Seoul"),
                    timezone=saju_data.get("timezone", "Asia/Seoul"),
                    language=language
                )

                if saju_result:
                    day_master = saju_result.get("dayMaster", {})
                    current_daeun = saju_result.get("currentDaeun", {})
                    today_iljin = saju_result.get("todayIljin", {})

                    saju_context = "\n\n[ğŸ”® ì‚¬ìš©ì ì‚¬ì£¼ ìš´ì„¸ ì»¨í…ìŠ¤íŠ¸]\n"

                    if day_master:
                        dm_stem = day_master.get("stem", "")
                        dm_element = day_master.get("element", "")
                        saju_context += f"ì¼ê°„(ë³¸ì§ˆ): {dm_stem} ({dm_element})\n"

                    if current_daeun:
                        daeun_info = f"{current_daeun.get('stem', '')} {current_daeun.get('branch', '')}"
                        saju_context += f"í˜„ì¬ ëŒ€ìš´(10ë…„): {daeun_info}\n"

                    if today_iljin:
                        iljin_info = f"{today_iljin.get('stem', '')} {today_iljin.get('branch', '')}"
                        saju_context += f"ì˜¤ëŠ˜ ì¼ì§„: {iljin_info}\n"

                    saju_context += "â†’ ì´ ìš´ì„¸ íë¦„ì´ ê¿ˆì˜ ë‚´ìš©ê³¼ ì‹œì ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n"

                    logger.info(f"[DREAM_CHAT_STREAM] Added saju context for user")
            except Exception as saju_error:
                logger.warning(f"[DREAM_CHAT_STREAM] Saju calculation failed: {saju_error}")

        # Format basic context
        symbols_str = ", ".join(symbols) if symbols else "ì—†ìŒ"
        emotions_str = ", ".join(emotions) if emotions else "ì—†ìŒ"
        themes_str = ", ".join(themes) if themes else "ì—†ìŒ"
        recommendations_str = " / ".join(recommendations) if recommendations else "ì—†ìŒ"

        # ============================================================
        # PREVIOUS CONSULTATIONS CONTEXT (Memory/Continuity)
        # ============================================================
        previous_context = ""
        previous_consultations = dream_context.get("previous_consultations", [])
        if previous_consultations:
            previous_context = "\n\n[ğŸ”„ ì´ì „ ìƒë‹´ ê¸°ë¡ - ì‚¬ìš©ìì™€ì˜ ì—°ì†ì„± ìœ ì§€]\n"
            for i, prev in enumerate(previous_consultations[:3], 1):
                prev_summary = prev.get("summary", "")[:150]
                prev_dream = prev.get("dreamText", "")[:100]
                prev_date = prev.get("date", "")[:10]
                if prev_summary:
                    previous_context += f"{i}. ({prev_date}) {prev_summary}\n"
                    if prev_dream:
                        previous_context += f"   ì´ì „ ê¿ˆ: {prev_dream}...\n"
            previous_context += "â†’ ì´ì „ ìƒë‹´ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"

        # ============================================================
        # PERSONA MEMORY (Personalization)
        # ============================================================
        persona_context = ""
        persona_memory = dream_context.get("persona_memory", {})
        if persona_memory:
            session_count = persona_memory.get("sessionCount", 0)
            key_insights = persona_memory.get("keyInsights", [])
            emotional_tone = persona_memory.get("emotionalTone", "")

            if session_count > 1 or key_insights or emotional_tone:
                persona_context = "\n\n[ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ (ê°œì¸í™”)]\n"
                if session_count > 1:
                    persona_context += f"ìƒë‹´ íšŸìˆ˜: {session_count}íšŒ (ë‹¨ê³¨ ì‚¬ìš©ì)\n"
                if emotional_tone:
                    persona_context += f"ì „ë°˜ì  ê°ì • í†¤: {emotional_tone}\n"
                if key_insights:
                    persona_context += f"í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {', '.join(key_insights[:3])}\n"
                persona_context += "â†’ ì´ì „ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"

        # ============================================================
        # JUNGIAN ENHANCED CONTEXT (from CounselingEngine)
        # ============================================================
        jung_context_str = ""
        if counseling_engine:
            try:
                # Get enhanced Jung context from counseling engine
                jung_context = counseling_engine.get_enhanced_context(
                    user_message=last_user_message,
                    saju_data=saju_data if saju_data else None
                )

                if jung_context:
                    jung_context_str = "\n\n[ğŸ§  ìœµ ì‹¬ë¦¬í•™ ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ - CounselingEngine]\n"

                    # Psychological Type (from Saju mapping)
                    if jung_context.get("psychological_type"):
                        ptype = jung_context["psychological_type"]
                        jung_context_str += f"ì‹¬ë¦¬ ìœ í˜•: {ptype.get('name_ko', ptype.get('name', ''))}\n"
                        jung_context_str += f"  íŠ¹ì§•: {ptype.get('description', '')[:100]}\n"

                    # Alchemical Stage (Nigredoâ†’Albedoâ†’Rubedo)
                    if jung_context.get("alchemy_stage"):
                        stage = jung_context["alchemy_stage"]
                        jung_context_str += f"ì—°ê¸ˆìˆ  ë‹¨ê³„: {stage.get('name_ko', stage.get('name', ''))}\n"
                        jung_context_str += f"  ì´ˆì : {stage.get('therapeutic_focus', '')[:100]}\n"

                    # Scenario Guidance
                    if jung_context.get("scenario_guidance"):
                        scenario = jung_context["scenario_guidance"]
                        jung_context_str += f"ìƒë‹´ ì ‘ê·¼: {scenario.get('approach', '')[:100]}\n"

                    # RAG-based recommended questions
                    if jung_context.get("rag_questions"):
                        jung_context_str += "ì¶”ì²œ ì¹˜ë£Œì  ì§ˆë¬¸:\n"
                        for q in jung_context["rag_questions"][:2]:
                            jung_context_str += f"  â€¢ {q}\n"

                    # RAG insights
                    if jung_context.get("rag_insights"):
                        jung_context_str += "ê´€ë ¨ í†µì°°:\n"
                        for insight in jung_context["rag_insights"][:2]:
                            jung_context_str += f"  â€¢ {insight[:80]}...\n"

                    jung_context_str += "â†’ ì´ ìœµ ì‹¬ë¦¬í•™ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¿ˆ í•´ì„ì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì„¸ìš”.\n"

                    logger.info(f"[DREAM_CHAT_STREAM] Added Jung enhanced context from CounselingEngine")
            except Exception as jung_error:
                logger.warning(f"[DREAM_CHAT_STREAM] Jung context generation failed: {jung_error}")

        # ============================================================
        # SESSION PHASE TRACKING
        # ============================================================
        session_phase_context = ""
        if counseling_session:
            try:
                # Add user message to session
                counseling_session.add_message("user", last_user_message)

                # Get current phase info
                phase_info = counseling_session.get_phase_info()
                session_phase_context = f"\n\n[ğŸ“ ìƒë‹´ ì§„í–‰ ë‹¨ê³„: {phase_info.get('name', '')}]\n"
                session_phase_context += f"ëª©í‘œ: {', '.join(phase_info.get('goals', []))}\n"
                session_phase_context += f"â†’ í˜„ì¬ ë‹¨ê³„ì˜ ëª©í‘œì— ë§ì¶° ë‹µë³€í•˜ì„¸ìš”.\n"

                logger.info(f"[DREAM_CHAT_STREAM] Session phase: {counseling_session.current_phase}")
            except Exception as phase_error:
                logger.warning(f"[DREAM_CHAT_STREAM] Session phase tracking failed: {phase_error}")

        # Build conversation history
        conversation_history = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "system":
                continue
            conversation_history.append(f"{'ì‚¬ìš©ì' if role == 'user' else 'AI'}: {content}")

        is_korean = language == "ko"

        # ============================================================
        # BUILD ENHANCED SYSTEM PROMPT (Jung + Stoic + Korean Haemong)
        # ============================================================
        if is_korean:
            system_prompt = """ì „ë¬¸ ê¿ˆ í•´ì„ ìƒë‹´ì‚¬. ìœµ ì‹¬ë¦¬í•™ + ìŠ¤í† ì•„ ì² í•™ + í•œêµ­ í•´ëª½ ìœµí•©.

ğŸš« ì ˆëŒ€ ê¸ˆì§€:
- "ì¢‹ì€ ê¿ˆì´ì—ìš”" "ì¡°ì‹¬í•˜ì„¸ìš”" ê°™ì€ ëœ¬êµ¬ë¦„ ë§
- ëª¨ë“  ê¿ˆì— ì ìš©ë˜ëŠ” ì¼ë°˜ë¡ 
- ë°ì´í„° ì—†ì´ ì¶”ì¸¡

âœ… ì˜¬ë°”ë¥¸ ë‹µë³€:
- ì•„ë˜ ì»¨í…ìŠ¤íŠ¸(ì‚¬ì£¼, ì²œì²´, ë¬¸í™”ë³„ í•´ì„)ë¥¼ ë°˜ë“œì‹œ ì¸ìš©
- "ì™œ ì§€ê¸ˆ ì´ ê¿ˆì„ ê¾¸ì—ˆëŠ”ì§€" í˜„ì¬ ìš´ì„¸/ì²œì²´ë¡œ ì„¤ëª…
- êµ¬ì²´ì  ì‹œê¸°/í–‰ë™ ì œì‹œ (ì˜ˆ: "ì´ë²ˆ ë‹¬ì€ ë¬¼ ê·¼ì²˜ í”¼í•˜ì„¸ìš”")

ì˜ˆì‹œ:
âŒ ë‚˜ìœ ë‹µ: "ë±€ì€ ë³€í™”ë¥¼ ì˜ë¯¸í•´ìš”."
âœ… ì¢‹ì€ ë‹µ: "í˜„ì¬ ë³‘ì(ä¸™å­) ëŒ€ìš´ì—ì„œ ìˆ˜(æ°´)ê¸°ìš´ì´ ê°•í•œë°, ë±€ì€ ìˆ˜ ì—ë„ˆì§€ì˜ ìƒì§•ì´ì—ìš”. ë‹¬ì´ ì „ê°ˆìë¦¬ì— ìˆì–´ ê¹Šì€ ë³€í™˜ ìš•êµ¬ê°€ ê¿ˆì— ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ìœµ ì‹¬ë¦¬í•™ì—ì„œ ë±€ì€ ë¬´ì˜ì‹ì˜ ì§€í˜œë¥¼ ìƒì§•í•˜ëŠ”ë°, ì§€ê¸ˆ ë‹¹ì‹ ì—ê²Œ ì–´ë–¤ ë³€í™”ê°€ í•„ìš”í•œì§€ ìŠ¤ìŠ¤ë¡œ ë¬¼ì–´ë³´ì„¸ìš”."

í•µì‹¬ í•´ì„ í‹€:
- í•œêµ­ í•´ëª½: ê¸¸ëª½/í‰ëª½, íƒœëª½, ì¬ë¬¼ëª½
- ìœµ ì‹¬ë¦¬í•™: ê·¸ë¦¼ì, ì•„ë‹ˆë§ˆ/ì•„ë‹ˆë¬´ìŠ¤ (ì¹˜ë£Œì  ì§ˆë¬¸ í™œìš©)
- ìŠ¤í† ì•„: ì‹¤ìš©ì  í–‰ë™ ì¡°ì–¸"""

            # Build enhanced chat prompt with all context
            chat_prompt = f"""[ê¿ˆ í•´ì„ ì»¨í…ìŠ¤íŠ¸]
ì›ë˜ ê¿ˆ: {dream_text[:600] if dream_text else "(ì—†ìŒ)"}
í•´ì„ ìš”ì•½: {summary[:400] if summary else "(ì—†ìŒ)"}
ì£¼ìš” ì‹¬ë³¼: {symbols_str}
ê°ì •: {emotions_str}
í…Œë§ˆ: {themes_str}
ê¸°ì¡´ ì¡°ì–¸: {recommendations_str}"""

            # Add cultural notes if available
            if cultural_notes:
                if cultural_notes.get("korean"):
                    chat_prompt += f"\ní•œêµ­ í•´ëª½ í•´ì„: {cultural_notes['korean'][:200]}"
                if cultural_notes.get("western"):
                    chat_prompt += f"\nì„œì–‘ ì‹¬ë¦¬í•™ í•´ì„: {cultural_notes['western'][:200]}"

            # Add RAG context
            chat_prompt += rag_context

            # Add therapeutic context (Jung-based questions from DreamRAG)
            chat_prompt += therapeutic_context

            # Add counseling context (scenario-based from DreamRAG)
            chat_prompt += counseling_context

            # Add Jung enhanced context (from CounselingEngine) â­ NEW
            chat_prompt += jung_context_str

            # Add session phase tracking â­ NEW
            chat_prompt += session_phase_context

            # Add celestial context
            chat_prompt += celestial_context

            # Add saju context
            chat_prompt += saju_context

            # Add previous consultations
            chat_prompt += previous_context

            # Add persona memory
            chat_prompt += persona_context

            # Add crisis context if detected
            crisis_instruction = ""
            if crisis_response:
                crisis_instruction = f"""

[âš ï¸ ìœ„ê¸° ìƒí™© ê°ì§€ - ìš°ì„  ëŒ€ì‘ í•„ìš”]
ê°ì§€ ìœ í˜•: {crisis_response['type']}
ì‹¬ê°ë„: {crisis_response['severity']}
ê¶Œì¥ ëŒ€ì‘: {crisis_response['response']}
ì „ë¬¸ ê¸°ê´€: {', '.join([f"{k}: {v}" for k, v in crisis_response['resources'].items()])}

ì¤‘ìš”: ë¨¼ì € ê³µê°ê³¼ ì§€ì§€ë¥¼ í‘œí˜„í•˜ê³ , ì „ë¬¸ ìƒë‹´ ê¸°ê´€ ì—°ë½ì²˜ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”."""

            chat_prompt += f"""

[ëŒ€í™” ê¸°ë¡]
{chr(10).join(conversation_history[-6:])}

[ì‚¬ìš©ì ì§ˆë¬¸]
{last_user_message}
{crisis_instruction}

ìœ„ì˜ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸(ì§€ì‹ë² ì´ìŠ¤, ì²œì²´, ì‚¬ì£¼, ì´ì „ ìƒë‹´, ì¹˜ë£Œì  ì§ˆë¬¸)ë¥¼ í™œìš©í•˜ì—¬:
1. í•œêµ­ í•´ëª½ ê´€ì ì˜ êµ¬ì²´ì  í•´ì„
2. ìœµ ì‹¬ë¦¬í•™ì  í†µì°° (í•„ìš”ì‹œ ì›í˜• ì–¸ê¸‰, ì¹˜ë£Œì  ì§ˆë¬¸ í™œìš©)
3. ìŠ¤í† ì•„ ì² í•™ì˜ ì‹¤ìš©ì  ì¡°ì–¸
ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœµí•©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""

        else:
            system_prompt = """Expert dream counselor. Jung psychology + Stoic philosophy + Korean Haemong.

ğŸš« FORBIDDEN:
- "Good dream" "Be careful" vague statements
- Generic interpretations applicable to any dream
- Speculation without data

âœ… CORRECT ANSWERS:
- MUST cite context below (saju fortune, celestial, cultural interpretations)
- Explain "why this dream NOW" using current fortune/celestial data
- Specific timing/actions (e.g., "avoid water activities this month")

Example:
âŒ Bad: "Snake represents transformation."
âœ… Good: "In your current Byeongja (ä¸™å­) major fortune, Water energy is strong - snake symbolizes this Water energy. Moon in Scorpio amplifies transformation urges in your dream. In Jungian terms, snake represents unconscious wisdom. Ask yourself: what change do you need right now?"

Core frameworks:
- Korean Haemong: auspicious/inauspicious, conception, wealth dreams
- Jungian: Shadow, Anima/Animus (use therapeutic questions)
- Stoic: practical action advice"""

            chat_prompt = f"""[Dream Interpretation Context]
Original Dream: {dream_text[:600] if dream_text else "(none)"}
Summary: {summary[:400] if summary else "(none)"}
Key Symbols: {symbols_str}
Emotions: {emotions_str}
Themes: {themes_str}
Previous Recommendations: {recommendations_str}"""

            if cultural_notes:
                if cultural_notes.get("korean"):
                    chat_prompt += f"\nKorean Traditional: {cultural_notes['korean'][:200]}"
                if cultural_notes.get("western"):
                    chat_prompt += f"\nWestern Psychology: {cultural_notes['western'][:200]}"

            chat_prompt += rag_context
            chat_prompt += therapeutic_context
            chat_prompt += counseling_context
            chat_prompt += celestial_context
            chat_prompt += saju_context
            chat_prompt += previous_context
            chat_prompt += persona_context

            # Add crisis context if detected (English)
            crisis_instruction_en = ""
            if crisis_response:
                crisis_instruction_en = f"""

[âš ï¸ CRISIS DETECTED - PRIORITY RESPONSE NEEDED]
Type: {crisis_response['type']}
Severity: {crisis_response['severity']}
Recommended Response: First express empathy and support, then provide professional helpline information.
Korean Crisis Lines: Suicide Prevention 1393, Mental Health Crisis 1577-0199

Important: Prioritize emotional support and professional referral."""

            chat_prompt += f"""

[Conversation History]
{chr(10).join(conversation_history[-6:])}

[User Question]
{last_user_message}
{crisis_instruction_en}

Using all context (knowledge base, celestial, saju, previous consultations, therapeutic questions), provide a response that naturally blends:
1. Korean traditional dream interpretation
2. Jungian psychological insight (use therapeutic questions when appropriate)
3. Stoic practical wisdom"""

        def generate_stream():
            """Generator for SSE streaming"""
            try:
                if not OPENAI_AVAILABLE or not openai_client:
                    yield f"data: {json.dumps({'error': 'OpenAI not available'})}\n\n"
                    return

                stream = openai_client.chat.completions.create(
                    model="gpt-4o",  # Upgraded from gpt-4o-mini for better Jung psychology + Korean haemong fusion
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": chat_prompt}
                    ],
                    temperature=0.75,
                    max_tokens=2000,  # Increased for comprehensive dream interpretation responses
                    stream=True
                )

                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        yield f"data: {json.dumps({'content': content})}\n\n"

                yield f"data: {json.dumps({'done': True})}\n\n"

            except Exception as stream_error:
                logger.exception(f"[DREAM_CHAT_STREAM] Streaming error: {stream_error}")
                yield f"data: {json.dumps({'error': str(stream_error)})}\n\n"

        return Response(
            generate_stream(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] /api/dream/chat-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/dream", methods=["POST"])
@app.route("/api/dream", methods=["POST"])
def dream_interpret():
    """
    Dream interpretation endpoint.
    Accepts dream text, symbols, emotions, themes, and cultural context.
    """
    try:
        data = request.get_json(force=True)
        logger.info(f"[DREAM] id={g.request_id} Processing dream interpretation")

        # Extract dream data
        birth_data = _normalize_birth_payload(data)
        locale = data.get("locale", "en")
        facts = {
            "dream": data.get("dream", ""),
            "symbols": data.get("symbols", []),
            "emotions": data.get("emotions", []),
            "themes": data.get("themes", []),
            "context": data.get("context", []),
            "locale": locale,
            # Cultural symbols
            "koreanTypes": data.get("koreanTypes", []),
            "koreanLucky": data.get("koreanLucky", []),
            "chinese": data.get("chinese", []),
            "islamicTypes": data.get("islamicTypes", []),
            "islamicBlessed": data.get("islamicBlessed", []),
            "western": data.get("western", []),
            "hindu": data.get("hindu", []),
            "nativeAmerican": data.get("nativeAmerican", []),
            "japanese": data.get("japanese", []),
            # Optional birth data
            "birth": birth_data,
        }

        start_time = time.time()
        result = interpret_dream(facts)
        duration_ms = int((time.time() - start_time) * 1000)

        logger.info(f"[DREAM] id={g.request_id} completed in {duration_ms}ms")

        if isinstance(result, dict):
            result["performance"] = {"duration_ms": duration_ms}

        # ğŸ’¾ Save to user memory (MOAT)
        if HAS_USER_MEMORY and birth_data:
            try:
                user_id = generate_user_id(birth_data)
                memory = get_user_memory(user_id)
                interpretation = result.get("interpretation", "") if isinstance(result, dict) else str(result)
                record_id = memory.save_consultation(
                    theme="dream",
                    locale=locale,
                    birth_data=birth_data,
                    fusion_result=interpretation,
                    service_type="dream",
                )
                result["user_id"] = user_id
                result["record_id"] = record_id
                logger.info(f"[DREAM] Saved to memory: {record_id}")
            except Exception as mem_e:
                logger.warning(f"[DREAM] Memory save failed: {mem_e}")

        return jsonify({"status": "success", "data": result})

    except Exception as e:
        logger.exception(f"[ERROR] id={getattr(g, 'request_id', '')} /dream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Cache stats and management
def cache_stats():
    """Get cache statistics."""
    try:
        cache = get_cache()
        stats = cache.stats()
        return jsonify({"status": "success", "cache": stats})
    except Exception as e:
        logger.exception(f"[ERROR] /cache/stats failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def cache_clear():
    """Clear cache (admin only)."""
    try:
        cache = get_cache()
        pattern = request.json.get("pattern", "fusion:*") if request.json else "fusion:*"
        cleared = cache.clear(pattern)
        return jsonify({"status": "success", "cleared": cleared})
    except Exception as e:
        logger.exception(f"[ERROR] /cache/clear failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Performance monitoring endpoints
def performance_stats():
    """Get performance statistics with optimization suggestions."""
    try:
        stats = get_performance_stats()
        suggestions = suggest_optimizations(stats)
        cache_health = get_cache_health()

        return jsonify({
            "status": "success",
            "performance": stats,
            "cache_health": cache_health,
            "suggestions": suggestions
        })
    except Exception as e:
        logger.exception(f"[ERROR] /performance/stats failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def health_check():
    """Simple health check for Railway/load balancer."""
    return jsonify({
        "status": "ok",
        "timestamp": time.time(),
        "version": "1.0.0"
    })


def readiness_check():
    """Readiness check - indicates app is ready to receive traffic."""
    try:
        # Check if essential services are available
        checks = {
            "app": True,
            "openai_key": bool(os.getenv("OPENAI_API_KEY")),
        }

        # Check Redis if available
        try:
            cache = get_cache()
            if cache:
                cache.ping()
                checks["redis"] = True
            else:
                checks["redis"] = False
        except Exception:
            checks["redis"] = False

        all_ready = all(checks.values())

        return jsonify({
            "ready": all_ready,
            "checks": checks,
            "timestamp": time.time()
        }), 200 if all_ready else 503
    except Exception as e:
        return jsonify({
            "ready": False,
            "error": str(e),
            "timestamp": time.time()
        }), 503


def prometheus_metrics():
    """Prometheus-compatible metrics endpoint."""
    try:
        perf_stats = get_performance_stats()
        cache_health = get_cache_health()

        # Format as Prometheus metrics
        metrics = []

        # Request metrics
        metrics.append(f'# HELP ai_backend_requests_total Total number of requests')
        metrics.append(f'# TYPE ai_backend_requests_total counter')
        metrics.append(f'ai_backend_requests_total {perf_stats.get("total_requests", 0)}')

        # Cache metrics
        metrics.append(f'# HELP ai_backend_cache_hit_rate Cache hit rate percentage')
        metrics.append(f'# TYPE ai_backend_cache_hit_rate gauge')
        metrics.append(f'ai_backend_cache_hit_rate {perf_stats.get("cache_hit_rate", 0)}')

        # Response time
        metrics.append(f'# HELP ai_backend_response_time_ms Average response time in milliseconds')
        metrics.append(f'# TYPE ai_backend_response_time_ms gauge')
        metrics.append(f'ai_backend_response_time_ms {perf_stats.get("avg_response_time_ms", 0)}')

        # Memory (if available)
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            metrics.append(f'# HELP ai_backend_memory_mb Memory usage in MB')
            metrics.append(f'# TYPE ai_backend_memory_mb gauge')
            metrics.append(f'ai_backend_memory_mb {memory_mb:.2f}')
        except ImportError:
            pass

        return Response('\n'.join(metrics), mimetype='text/plain')
    except Exception as e:
        return Response(f'# Error: {str(e)}', mimetype='text/plain'), 500


def full_health_check():
    """Comprehensive health check including performance metrics."""
    try:
        perf_stats = get_performance_stats()
        cache_health = get_cache_health()

        # Calculate overall health score
        health_score = 100
        issues = []

        # Penalize for low cache hit rate
        if perf_stats["cache_hit_rate"] < 30:
            health_score -= 20
            issues.append("Low cache hit rate")

        # Penalize for slow responses
        if perf_stats["avg_response_time_ms"] > 2000:
            health_score -= 15
            issues.append("Slow response times")

        # Penalize for cache issues
        if cache_health["health_score"] < 80:
            health_score -= 15
            issues.append("Cache degradation")

        # Check memory (if available)
        try:
            import psutil
            memory = psutil.Process().memory_info()
            memory_mb = memory.rss / 1024 / 1024
            if memory_mb > 450:  # Railway 512MB limit
                health_score -= 20
                issues.append(f"High memory usage: {memory_mb:.0f}MB")
        except ImportError:
            memory_mb = None

        # Check rate limit state size
        rate_state_size = len(_rate_state)
        if rate_state_size > 1000:
            issues.append(f"Large rate state: {rate_state_size} clients")

        status_text = "excellent" if health_score >= 90 else "good" if health_score >= 70 else "degraded"

        return jsonify({
            "status": "success",
            "health_score": max(0, health_score),
            "status_text": status_text,
            "issues": issues,
            "performance": perf_stats,
            "cache": cache_health,
            "memory_mb": memory_mb,
            "rate_state_clients": rate_state_size,
            "timestamp": time.time(),
            "version": "1.0.0"
        })
    except Exception as e:
        logger.exception(f"[ERROR] /health/full failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# ===============================================================
# GEMINI-LEVEL ENDPOINTS
# ===============================================================

# Real-time transit data
def get_transits():
    """Get current planetary transits (real-time)."""
    if not HAS_REALTIME:
        return jsonify({"status": "error", "message": "Realtime astro not available"}), 501

    try:
        locale = request.args.get("locale", "en")
        transits = get_current_transits()
        interpretation = get_transit_interpretation(transits, locale)

        return jsonify({
            "status": "success",
            "transits": transits,
            "interpretation": interpretation,
        })
    except Exception as e:
        logger.exception(f"[ERROR] /transits failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# Chart generation
def generate_saju_chart():
    """Generate Saju Paljja table SVG."""
    if not HAS_CHARTS:
        return jsonify({"status": "error", "message": "Chart generator not available"}), 501

    try:
        data = request.get_json(force=True)
        pillars = data.get("pillars", {})
        day_master = data.get("dayMaster", {})
        five_elements = data.get("fiveElements", {})

        svg = generate_saju_table_svg(pillars, day_master, five_elements)

        return jsonify({
            "status": "success",
            "svg": svg,
            "base64": svg_to_base64(svg),
        })
    except Exception as e:
        logger.exception(f"[ERROR] /charts/saju failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def generate_natal_chart():
    """Generate natal chart wheel SVG."""
    if not HAS_CHARTS:
        return jsonify({"status": "error", "message": "Chart generator not available"}), 501

    try:
        data = request.get_json(force=True)
        planets = data.get("planets", [])
        ascendant = data.get("ascendant", 0)
        size = data.get("size", 400)

        svg = generate_natal_chart_svg(planets, ascendant=ascendant, size=size)

        return jsonify({
            "status": "success",
            "svg": svg,
            "base64": svg_to_base64(svg),
        })
    except Exception as e:
        logger.exception(f"[ERROR] /charts/natal failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def generate_full_charts():
    """Generate complete HTML with all charts."""
    if not HAS_CHARTS:
        return jsonify({"status": "error", "message": "Chart generator not available"}), 501

    try:
        data = request.get_json(force=True)
        saju_data = data.get("saju", {})
        astro_data = data.get("astro", {})
        locale = data.get("locale", "en")

        html = generate_full_chart_html(saju_data, astro_data, locale)

        return jsonify({
            "status": "success",
            "html": html,
        })
    except Exception as e:
        logger.exception(f"[ERROR] /charts/full failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# User memory endpoints
# ===============================================================
# I CHING (PREMIUM) ENDPOINTS
# ===============================================================

# ===============================================================
# TAROT (PREMIUM) ENDPOINTS
# ===============================================================

# Theme mapping: Frontend IDs â†’ Backend theme names
TAROT_THEME_MAPPING = {
    # Direct matches
    "love": "love",
    "career": "career",
    "health": "health",
    "spiritual": "spiritual",
    "daily": "daily",
    "monthly": "monthly",
    "life_path": "life_path",
    "family": "family",

    # Frontend uses hyphens, backend uses underscores/different names
    "love-relationships": "love",
    "career-work": "career",
    "money-finance": "wealth",  # Key mapping: frontend uses money-finance, backend uses wealth
    "well-being-health": "health",
    "spiritual-growth": "spiritual",
    "daily-reading": "daily",
    "general-insight": "life_path",  # General maps to life_path
    "decisions-crossroads": "life_path",  # Maps to life_path (contains crossroads sub_topic)
    "self-discovery": "life_path",  # Maps to life_path (contains true_self sub_topic)
}

# Sub-topic mapping for themes that use different sub_topic names
TAROT_SUBTOPIC_MAPPING = {
    # decisions-crossroads spreads â†’ life_path sub_topics
    ("decisions-crossroads", "simple-choice"): ("life_path", "crossroads"),
    ("decisions-crossroads", "decision-cross"): ("life_path", "major_decision"),
    ("decisions-crossroads", "path-ahead"): ("life_path", "life_direction"),

    # self-discovery spreads â†’ life_path sub_topics
    ("self-discovery", "inner-self"): ("life_path", "true_self"),
    ("self-discovery", "personal-growth"): ("life_path", "life_lessons"),

    # general-insight spreads â†’ various themes
    ("general-insight", "quick-reading"): ("daily", "one_card"),
    ("general-insight", "past-present-future"): ("daily", "three_card"),
    ("general-insight", "celtic-cross"): ("life_path", "life_direction"),
}


def _map_tarot_theme(category: str, spread_id: str, user_question: str = "") -> tuple:
    """Map frontend theme/spread to backend theme/sub_topic, considering user's question"""
    # Check specific mapping first
    key = (category, spread_id)
    if key in TAROT_SUBTOPIC_MAPPING:
        return TAROT_SUBTOPIC_MAPPING[key]

    # Fall back to theme-only mapping
    mapped_theme = TAROT_THEME_MAPPING.get(category, category)

    # Dynamic sub_topic selection based on user question keywords
    if user_question and mapped_theme == "career":
        q = user_question.lower()
        if any(kw in q for kw in ["ì‚¬ì—…", "ì°½ì—…", "ìì˜ì—…", "business", "startup", "entrepreneur"]):
            return (mapped_theme, "entrepreneurship")
        elif any(kw in q for kw in ["ì·¨ì—…", "ì·¨ì§", "ì…ì‚¬", "job", "employment", "hire"]):
            return (mapped_theme, "job_search")
        elif any(kw in q for kw in ["ì´ì§", "í‡´ì‚¬", "ì „ì§", "resign", "quit", "change job"]):
            return (mapped_theme, "career_change")
        elif any(kw in q for kw in ["ìŠ¹ì§„", "promotion", "raise"]):
            return (mapped_theme, "promotion")
        elif any(kw in q for kw in ["ì§ì¥", "íšŒì‚¬", "ìƒì‚¬", "ë™ë£Œ", "workplace", "boss", "colleague"]):
            return (mapped_theme, "workplace")

    elif user_question and mapped_theme == "love":
        q = user_question.lower()
        if any(kw in q for kw in ["ì§ì‚¬ë‘", "ê³ ë°±", "crush", "confess"]):
            return (mapped_theme, "crush")
        elif any(kw in q for kw in ["í—¤ì–´", "ì´ë³„", "breakup", "separate"]):
            return (mapped_theme, "breakup")
        elif any(kw in q for kw in ["ê²°í˜¼", "ì•½í˜¼", "marriage", "wedding"]):
            return (mapped_theme, "marriage")
        elif any(kw in q for kw in ["ì¬íšŒ", "ë‹¤ì‹œ", "reconcile", "ex"]):
            return (mapped_theme, "reconciliation")
        elif any(kw in q for kw in ["ë§Œë‚¨", "ì†Œê°œíŒ…", "dating", "meet"]):
            return (mapped_theme, "new_love")

    elif user_question and mapped_theme == "wealth":
        q = user_question.lower()
        if any(kw in q for kw in ["íˆ¬ì", "ì£¼ì‹", "ì½”ì¸", "invest", "stock", "crypto"]):
            return (mapped_theme, "investment")
        elif any(kw in q for kw in ["ë¹š", "ëŒ€ì¶œ", "ë¶€ì±„", "debt", "loan"]):
            return (mapped_theme, "debt")
        elif any(kw in q for kw in ["ì €ì¶•", "ì ˆì•½", "save", "saving"]):
            return (mapped_theme, "saving")

    return (mapped_theme, spread_id)


# ===============================================================
# ğŸ´ AI íŠ¹ìœ  í‘œí˜„ í›„ì²˜ë¦¬ í•„í„°
# ===============================================================
def _clean_ai_phrases(text: str) -> str:
    """
    Remove AI-sounding phrases from tarot interpretations.
    Makes output more natural and less robotic.
    """
    import re

    # AI íŠ¹ìœ ì˜ í•œêµ­ì–´ í‘œí˜„ íŒ¨í„´
    ai_patterns_ko = [
        (r'~í•˜ì‹œëŠ”êµ°ìš”\.?', ''),
        (r'~ëŠë¼ì‹¤ ìˆ˜ ìˆì–´ìš”\.?', ''),
        (r'~í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤\.?', ''),
        (r'~í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”\?', ''),
        (r'ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ëŠê»´ì§€ë„¤ìš”\.?', ''),
        (r'ì¢‹ì€ ê²°ê³¼ê°€ ìˆì„ ê±°ì˜ˆìš”\.?', ''),
        (r'ì˜ ë  ê±°ì˜ˆìš”\.?', ''),
        (r'ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”\.?', ''),
        (r'ìì‹ ê°ì„ ê°€ì§€ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤\.?', ''),
        (r'~ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤\.', 'ë‹¤.'),
        (r'~ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤\.', 'ë‹¤.'),
        (r'~ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\.', 'ë‹¤.'),
        (r'í¬ë§ì ì¸ ë©”ì‹œì§€ë¥¼ ì „í•˜ê³  ìˆë„¤ìš”\.?', ''),
        (r'ì‘ì›í•©ë‹ˆë‹¤\.?', ''),
        (r'íŒŒì´íŒ…ì´ì—ìš”\.?', ''),
        (r'í™”ì´íŒ…!?', ''),
    ]

    # AI íŠ¹ìœ ì˜ ì˜ì–´ í‘œí˜„ íŒ¨í„´
    ai_patterns_en = [
        (r'I hope this helps\.?', ''),
        (r'Feel free to ask.*', ''),
        (r'I\'m here to help\.?', ''),
        (r'This suggests that you should\.?', 'This suggests'),
        (r'It\'s important to remember that\.?', ''),
        (r'positive energy', 'energy'),
    ]

    result = text
    for pattern, replacement in ai_patterns_ko + ai_patterns_en:
        result = re.sub(pattern, replacement, result)

    # ì—°ì†ëœ ê³µë°±/ë§ˆì¹¨í‘œ ì •ë¦¬
    result = re.sub(r'\s+', ' ', result)
    result = re.sub(r'\.+', '.', result)
    result = result.strip()

    return result


# ===============================================================
# ğŸ´ DYNAMIC FOLLOW-UP QUESTIONS GENERATOR
# ===============================================================
def generate_dynamic_followup_questions(
    interpretation: str,
    cards: list,
    category: str,
    user_question: str = "",
    language: str = "ko",
    static_questions: list = None
) -> list:
    """
    Generate dynamic, contextual follow-up questions based on the interpretation.
    Uses GPT to create specific, engaging questions that change with each reading.

    Args:
        interpretation: The full interpretation text
        cards: List of card dicts with 'name' and 'isReversed'
        category: Theme category (love, career, etc.)
        user_question: Original user question if any
        language: 'ko' or 'en'
        static_questions: Fallback static questions

    Returns:
        List of 5 dynamic follow-up questions
    """
    try:
        # Extract key elements from interpretation for context
        interpretation_preview = interpretation[:800] if len(interpretation) > 800 else interpretation
        card_names = [f"{c.get('name', '')}{'(ì—­ë°©í–¥)' if c.get('isReversed') else ''}" for c in cards]
        cards_str = ", ".join(card_names)

        # Detect reading tone from interpretation
        positive_keywords = ["ê¸°íšŒ", "ì„±ê³µ", "í–‰ìš´", "ê¸ì •", "ë°œì „", "í¬ë§", "ì‚¬ë‘", "ì¶•ë³µ", "ì„±ì·¨", "ê¸°ì¨",
                           "opportunity", "success", "luck", "positive", "growth", "hope", "love", "blessing", "joy"]
        challenging_keywords = ["ì£¼ì˜", "ê²½ê³ ", "ìœ„í—˜", "ë„ì „", "ê°ˆë“±", "ì–´ë ¤ì›€", "ì¥ì• ", "ì‹œë ¨", "ì¡°ì‹¬",
                               "caution", "warning", "danger", "challenge", "conflict", "difficulty", "obstacle"]

        tone = "neutral"
        positive_count = sum(1 for k in positive_keywords if k in interpretation.lower())
        challenging_count = sum(1 for k in challenging_keywords if k in interpretation.lower())

        if positive_count > challenging_count + 2:
            tone = "positive"
        elif challenging_count > positive_count + 2:
            tone = "challenging"

        # Build GPT prompt for generating dynamic questions
        is_korean = language == "ko"

        if is_korean:
            prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ íƒ€ë¡œ ë¦¬ë”ì…ë‹ˆë‹¤. ë°©ê¸ˆ ì œê³µëœ íƒ€ë¡œ í•´ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë” ê¹Šì´ íƒêµ¬í•˜ê³  ì‹¶ì–´í•  ë§Œí•œ í›„ì† ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

## í•´ì„ ìš”ì•½
ì¹´ë“œ: {cards_str}
ì¹´í…Œê³ ë¦¬: {category}
ë¦¬ë”© í†¤: {tone}
{'ì›ë˜ ì§ˆë¬¸: ' + user_question if user_question else ''}

## í•´ì„ ë‚´ìš©
{interpretation_preview}

## ì§ˆë¬¸ ìƒì„± ì§€ì¹¨
1. í•´ì„ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë‚´ìš©/ìƒì§•/ì¡°ì–¸ì— ê¸°ë°˜í•œ ì§ˆë¬¸
2. ì‚¬ìš©ìê°€ "ì™€, ì´ê±¸ ë” ì•Œê³  ì‹¶ë‹¤!" ë¼ê³  ëŠë‚„ ë§Œí¼ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸
3. ë‹¨ìˆœ ì˜ˆ/ì•„ë‹ˆì˜¤ê°€ ì•„ë‹Œ, ê¹Šì´ ìˆëŠ” ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸
4. ì¹´ë“œ ì´ë¦„ì´ë‚˜ ìƒì§•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰
5. ê° ì§ˆë¬¸ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì  ì œì‹œ (ì‹œê¸°, ì¡°ì–¸, ìˆ¨ê²¨ì§„ ì˜ë¯¸, ê´€ê³„, í–‰ë™)

## ì‘ë‹µ í˜•ì‹
ì§ˆë¬¸ 5ê°œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•˜ì„¸ìš”. ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ ì—†ì´ ì§ˆë¬¸ë§Œ ì‘ì„±.

ì˜ˆì‹œ:
{card_names[0] if card_names else 'ê´‘ëŒ€'} ì¹´ë“œê°€ ì•”ì‹œí•˜ëŠ” ìƒˆë¡œìš´ ì‹œì‘ì˜ êµ¬ì²´ì ì¸ íƒ€ì´ë°ì€?
ì´ ë¦¬ë”©ì—ì„œ ê²½ê³ í•˜ëŠ” ìˆ¨ê²¨ì§„ ì¥ì• ë¬¼ì„ ê·¹ë³µí•˜ëŠ” ë°©ë²•ì€?"""
        else:
            prompt = f"""You are an expert tarot reader. Based on the tarot interpretation just provided, generate 5 follow-up questions the user would want to explore deeper.

## Reading Summary
Cards: {cards_str}
Category: {category}
Reading Tone: {tone}
{'Original Question: ' + user_question if user_question else ''}

## Interpretation
{interpretation_preview}

## Question Guidelines
1. Based on specific content/symbols/advice mentioned in the interpretation
2. Intriguing enough that user thinks "I want to know more about this!"
3. Open-ended questions that lead to deeper conversation
4. Specifically mention card names or symbols
5. Each question offers a different perspective (timing, advice, hidden meaning, relationships, actions)

## Response Format
Write 5 questions separated by newlines. No numbers or bullets, just questions.

Example:
What specific timing does {card_names[0] if card_names else 'The Fool'} suggest for this new beginning?
How can I overcome the hidden obstacles this reading warns about?"""

        # Generate with GPT-4o-mini for speed
        response = _generate_with_gpt4(prompt, max_tokens=500, temperature=0.8, use_mini=True)

        # Parse response into list
        questions = [q.strip() for q in response.strip().split('\n') if q.strip() and len(q.strip()) > 10]

        # Ensure we have exactly 5 questions
        if len(questions) >= 5:
            return questions[:5]
        elif len(questions) > 0:
            # Pad with static questions if needed
            if static_questions:
                remaining = 5 - len(questions)
                questions.extend(static_questions[:remaining])
            return questions[:5]
        else:
            # Fallback to static
            return static_questions[:5] if static_questions else []

    except Exception as e:
        logger.warning(f"[TAROT] Dynamic question generation failed: {e}")
        return static_questions[:5] if static_questions else []


# ===============================================================
# TAROT TOPIC DETECTION (ì±„íŒ… ê¸°ë°˜ íƒ€ë¡œ ì£¼ì œ ìë™ ê°ì§€)
# ===============================================================

# Sub-topic keyword mappings for each theme
_TAROT_TOPIC_KEYWORDS = {
    "career": {
        "job_search": {
            "keywords": ["ì·¨ì—…", "êµ¬ì§", "ì¼ìë¦¬", "ì§ì¥ êµ¬í•˜", "ì·¨ì§", "ì…ì‚¬", "ì‹ ì…", "ì²« ì§ì¥", "job", "employment"],
            "korean": "ì·¨ì—…ì€ ì–¸ì œ",
            "priority": 10
        },
        "interview": {
            "keywords": ["ë©´ì ‘", "ì¸í„°ë·°", "í•©ê²©", "ë¶ˆí•©ê²©", "ì„œë¥˜", "ì±„ìš©", "interview"],
            "korean": "ë©´ì ‘ ê²°ê³¼",
            "priority": 9
        },
        "job_change": {
            "keywords": ["ì´ì§", "í‡´ì‚¬", "ì§ì¥ ì˜®ê¸°", "íšŒì‚¬ ë°”ê¾¸", "ì „ì§", "ìƒˆ ì§ì¥", "career change"],
            "korean": "ì´ì§í•´ì•¼ í• ê¹Œ",
            "priority": 10
        },
        "promotion": {
            "keywords": ["ìŠ¹ì§„", "ì§„ê¸‰", "ìŠ¹ê¸‰", "ì„ì›", "íŒ€ì¥", "ê³¼ì¥", "ë¶€ì¥", "promotion"],
            "korean": "ìŠ¹ì§„ ê°€ëŠ¥ì„±",
            "priority": 8
        },
        "business": {
            "keywords": ["ì‚¬ì—…", "ì°½ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "ìì˜ì—…", "ê°œì—…", "ì‚¬ì¥", "CEO", "business", "startup"],
            "korean": "ì‚¬ì—… ì‹œì‘/í™•ì¥",
            "priority": 9
        },
        "side_hustle": {
            "keywords": ["ë¶€ì—…", "íˆ¬ì¡", "ì•Œë°”", "ì•„ë¥´ë°”ì´íŠ¸", "ë¶€ìˆ˜ì…", "side job"],
            "korean": "ë¶€ì—…/íˆ¬ì¡",
            "priority": 7
        },
        "career_path": {
            "keywords": ["ì§„ë¡œ", "ì ì„±", "ì–´ë–¤ ì§ì—…", "ë¬´ìŠ¨ ì¼", "ì í•©í•œ ì§ì—…", "ë§ëŠ” ì§ì—…", "career path", "aptitude"],
            "korean": "ë‚˜ì—ê²Œ ë§ëŠ” ì§ì—…",
            "priority": 8
        },
        "workplace": {
            "keywords": ["ì§ì¥ ìƒí™œ", "íšŒì‚¬ ìƒí™œ", "ë™ë£Œ", "ìƒì‚¬", "ì§ì¥ ë‚´", "ì‚¬ë‚´", "workplace"],
            "korean": "ì§ì¥ ë‚´ ê´€ê³„/ìƒí™©",
            "priority": 6
        },
        "salary": {
            "keywords": ["ì—°ë´‰", "ê¸‰ì—¬", "ì›”ê¸‰", "ì„ê¸ˆ", "ëˆ", "ì¸ìƒ", "í˜‘ìƒ", "salary"],
            "korean": "ì—°ë´‰ í˜‘ìƒ/ì¸ìƒ",
            "priority": 7
        },
        "project": {
            "keywords": ["í”„ë¡œì íŠ¸", "ì—…ë¬´", "ê³¼ì œ", "ì¼ ì˜", "ì„±ê³¼", "project"],
            "korean": "í”„ë¡œì íŠ¸ ì„±ê³µ",
            "priority": 6
        }
    },
    "love": {
        "secret_admirer": {
            "keywords": ["ë‚˜ë¥¼ ì¢‹ì•„í•˜ëŠ”", "ë‚  ì¢‹ì•„í•˜ëŠ”", "ê´€ì‹¬ ìˆëŠ” ì‚¬ëŒ", "ëˆ„ê°€ ì¢‹ì•„", "secret admirer"],
            "korean": "ë‚˜ë¥¼ ì¢‹ì•„í•˜ëŠ” ì¸ì—°",
            "priority": 8
        },
        "current_partner": {
            "keywords": ["ì—°ì¸", "ë‚¨ì¹œ", "ì—¬ì¹œ", "ë‚¨ìì¹œêµ¬", "ì—¬ìì¹œêµ¬", "ì• ì¸", "partner"],
            "korean": "ì§€ê¸ˆ ì—°ì¸ì˜ ì†ë§ˆìŒ",
            "priority": 9
        },
        "crush": {
            "keywords": ["ì§ì‚¬ë‘", "ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒ", "ë§ˆìŒì— ë“œëŠ”", "ê³ ë°±", "crush"],
            "korean": "ì§ì‚¬ë‘ ìƒëŒ€ì˜ ë§ˆìŒ",
            "priority": 8
        },
        "reconciliation": {
            "keywords": ["ì¬íšŒ", "ë‹¤ì‹œ ë§Œë‚˜", "í—¤ì–´ì§„", "ì „ ë‚¨ì¹œ", "ì „ ì—¬ì¹œ", "ëŒì•„ì˜¬", "reconciliation", "ex"],
            "korean": "í—¤ì–´ì§„ ì—°ì¸ê³¼ì˜ ì¬íšŒ",
            "priority": 9
        },
        "situationship": {
            "keywords": ["ì¸", "ì¸íƒ€ëŠ”", "ë°€ë‹¹", "ê´€ê³„ ì§„ì „", "situationship"],
            "korean": "ì¸íƒ€ëŠ” ìƒëŒ€",
            "priority": 8
        },
        "marriage": {
            "keywords": ["ê²°í˜¼", "ê²°í˜¼ìš´", "ë°°ìš°ì", "ì‹ ë‘", "ì‹ ë¶€", "í˜¼ì¸", "ì›¨ë”©", "marriage", "wedding"],
            "korean": "ê²°í˜¼ìš´",
            "priority": 10
        },
        "breakup": {
            "keywords": ["ì´ë³„", "í—¤ì–´ì§ˆ", "í—¤ì–´ì ¸ì•¼", "ëë‚´ì•¼", "ê·¸ë§Œ ë§Œë‚˜", "breakup"],
            "korean": "ì´ë³„í•´ì•¼ í• ê¹Œ",
            "priority": 9
        },
        "new_love": {
            "keywords": ["ìƒˆë¡œìš´ ì¸ì—°", "ìƒˆ ì‚¬ë‘", "ì–¸ì œ ì—°ì• ", "ì¸ì—°ì´ ì–¸ì œ", "new love"],
            "korean": "ìƒˆë¡œìš´ ì‚¬ë‘ì€ ì–¸ì œ",
            "priority": 8
        },
        "cheating": {
            "keywords": ["ë°”ëŒ", "ì™¸ë„", "ë¶ˆë¥œ", "ì–‘ë‹¤ë¦¬", "cheating", "affair", "ë°”ëŒí”¼"],
            "korean": "ìƒëŒ€ê°€ ë°”ëŒí”¼ìš°ëŠ”ì§€",
            "priority": 11
        },
        "soulmate": {
            "keywords": ["ì†Œìš¸ë©”ì´íŠ¸", "ìš´ëª…", "ì§„ì •í•œ ì‚¬ë‘", "soulmate", "destiny"],
            "korean": "ì†Œìš¸ë©”ì´íŠ¸ ë¦¬ë”©",
            "priority": 7
        }
    },
    "wealth": {
        "money_luck": {
            "keywords": ["ì¬ë¬¼ìš´", "ê¸ˆì „ìš´", "ëˆ ìš´", "ë¶€ì", "wealth", "money luck"],
            "korean": "ì¬ë¬¼ìš´",
            "priority": 9
        },
        "investment": {
            "keywords": ["íˆ¬ì", "ì£¼ì‹", "ì½”ì¸", "ë¶€ë™ì‚°", "í€ë“œ", "investment", "stock"],
            "korean": "íˆ¬ì ê²°ì •",
            "priority": 9
        },
        "debt": {
            "keywords": ["ë¹š", "ëŒ€ì¶œ", "ë¶€ì±„", "ê°š", "loan", "debt"],
            "korean": "ë¹š/ëŒ€ì¶œ",
            "priority": 8
        },
        "windfall": {
            "keywords": ["ë³µê¶Œ", "ë¡œë˜", "íš¡ì¬", "lottery", "windfall"],
            "korean": "íš¡ì¬ìš´",
            "priority": 7
        }
    },
    "health": {
        "general_health": {
            "keywords": ["ê±´ê°•", "ê±´ê°•ìš´", "ëª¸", "ì•„í”„", "ë³‘", "health"],
            "korean": "ê±´ê°•ìš´",
            "priority": 9
        },
        "mental_health": {
            "keywords": ["ì •ì‹  ê±´ê°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ìš°ìš¸", "ë¶ˆì•ˆ", "mental health"],
            "korean": "ì •ì‹  ê±´ê°•",
            "priority": 8
        },
        "recovery": {
            "keywords": ["íšŒë³µ", "ì¹˜ë£Œ", "ì™„ì¹˜", "recovery"],
            "korean": "íšŒë³µ",
            "priority": 8
        }
    },
    "family": {
        "parent": {
            "keywords": ["ë¶€ëª¨", "ì—„ë§ˆ", "ì•„ë¹ ", "ì–´ë¨¸ë‹ˆ", "ì•„ë²„ì§€", "parent"],
            "korean": "ë¶€ëª¨ë‹˜ê³¼ì˜ ê´€ê³„",
            "priority": 8
        },
        "children": {
            "keywords": ["ìë…€", "ì•„ì´", "ì•„ë“¤", "ë”¸", "ì„ì‹ ", "children", "pregnancy"],
            "korean": "ìë…€ìš´",
            "priority": 9
        },
        "sibling": {
            "keywords": ["í˜•ì œ", "ìë§¤", "ì˜¤ë¹ ", "ì–¸ë‹ˆ", "ë™ìƒ", "sibling"],
            "korean": "í˜•ì œ/ìë§¤ ê´€ê³„",
            "priority": 7
        }
    },
    "spiritual": {
        "life_purpose": {
            "keywords": ["ì‚¶ì˜ ëª©ì ", "ì¸ìƒì˜ ì˜ë¯¸", "ì™œ ì‚¬ëŠ”", "purpose"],
            "korean": "ì‚¶ì˜ ëª©ì ",
            "priority": 8
        },
        "karma": {
            "keywords": ["ì „ìƒ", "ì¹´ë¥´ë§ˆ", "ì—…", "karma", "past life"],
            "korean": "ì „ìƒ/ì¹´ë¥´ë§ˆ",
            "priority": 7
        },
        "spiritual_growth": {
            "keywords": ["ì˜ì  ì„±ì¥", "ê¹¨ë‹¬ìŒ", "ëª…ìƒ", "spiritual"],
            "korean": "ì˜ì  ì„±ì¥",
            "priority": 7
        }
    },
    "life_path": {
        "general": {
            "keywords": ["ì¸ìƒ", "ì•ìœ¼ë¡œ", "ë¯¸ë˜", "ìš´ì„¸", "ì „ë°˜ì ", "life", "future"],
            "korean": "ì¸ìƒ ì „ë°˜",
            "priority": 5
        },
        "decision": {
            "keywords": ["ê²°ì •", "ì„ íƒ", "ì–´ë–»ê²Œ í•´ì•¼", "ë­˜ í•´ì•¼", "decision"],
            "korean": "ê²°ì •/ì„ íƒ",
            "priority": 6
        }
    }
}

# Cache for spread configurations (loaded once)
_SPREAD_CONFIG_CACHE = {}

def _load_spread_config(theme: str) -> dict:
    """Load and cache spread configuration for a theme."""
    if theme in _SPREAD_CONFIG_CACHE:
        return _SPREAD_CONFIG_CACHE[theme]

    spread_file = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        "data", "graph", "rules", "tarot", "spreads",
        f"{theme}_spreads.json"
    )

    try:
        if os.path.exists(spread_file):
            with open(spread_file, "r", encoding="utf-8") as f:
                _SPREAD_CONFIG_CACHE[theme] = json.load(f)
                return _SPREAD_CONFIG_CACHE[theme]
    except Exception as e:
        logger.warning(f"Could not load spread file {spread_file}: {e}")

    return {}


def detect_tarot_topic(text: str) -> dict:
    """
    Analyze chat text and detect the most relevant tarot theme and sub-topic.

    Args:
        text: Chat message or conversation text to analyze

    Returns:
        {
            "theme": "career",
            "sub_topic": "job_search",
            "korean": "ì·¨ì—…ì€ ì–¸ì œ",
            "confidence": 0.85,
            "card_count": 10,
            "matched_keywords": ["ì·¨ì—…", "ì§ì¥"]
        }
    """
    text_lower = text.lower()

    # Collect all matches with scores
    all_matches = []

    # Score each theme and sub-topic
    for theme, sub_topics in _TAROT_TOPIC_KEYWORDS.items():
        for sub_topic_id, sub_topic_data in sub_topics.items():
            matched = []
            for keyword in sub_topic_data["keywords"]:
                if keyword.lower() in text_lower or keyword in text:
                    matched.append(keyword)

            if matched:
                # Calculate raw score (not capped) for comparison
                # - Base priority score (0.1 per priority point)
                # - Keyword matches (0.2 per match)
                # - Specificity bonus: longer keywords are more specific
                priority_score = sub_topic_data["priority"] * 0.1
                match_score = len(matched) * 0.2
                avg_keyword_len = sum(len(k) for k in matched) / len(matched)
                specificity_bonus = min(avg_keyword_len * 0.02, 0.2)

                raw_score = priority_score + match_score + specificity_bonus

                all_matches.append({
                    "theme": theme,
                    "sub_topic": sub_topic_id,
                    "korean": sub_topic_data["korean"],
                    "confidence": round(min(raw_score, 1.0), 2),
                    "_raw_score": raw_score,  # Internal, removed before return
                    "_priority": sub_topic_data["priority"],  # Internal
                    "matched_keywords": matched,
                })

    # Sort by raw_score (desc), then by priority (desc) for tie-breaking
    all_matches.sort(key=lambda x: (x["_raw_score"], x["_priority"]), reverse=True)

    if all_matches:
        best_match = all_matches[0]
        # Remove internal fields
        del best_match["_raw_score"]
        del best_match["_priority"]
    else:
        best_match = {
            "theme": "life_path",
            "sub_topic": "general",
            "korean": "ì¸ìƒ ì „ë°˜",
            "confidence": 0.0,
            "matched_keywords": []
        }

    # Load spread configuration to get card count (cached)
    spread_data = _load_spread_config(best_match["theme"])
    sub_topic_config = spread_data.get("sub_topics", {}).get(best_match["sub_topic"], {})

    best_match["card_count"] = sub_topic_config.get("card_count", 3)
    best_match["spread_name"] = sub_topic_config.get("spread_name", "")
    best_match["positions"] = sub_topic_config.get("positions", [])

    return best_match


# ===============================================================
# JUNGIAN COUNSELING ENDPOINTS (ì‹¬ë¦¬ìƒë‹´)
# ===============================================================

@app.route("/api/counseling/chat", methods=["POST"])
def counseling_chat():
    """
    ìœµ ì‹¬ë¦¬í•™ ê¸°ë°˜ ìƒë‹´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    - ìœ„ê¸° ê°ì§€ ìë™í™”
    - RAG + RuleEngine ê¸°ë°˜ ì¹˜ë£Œì  ê°œì…
    - ì‚¬ì£¼/ì ì„±/íƒ€ë¡œ ì»¨í…ìŠ¤íŠ¸ í†µí•©
    """
    if not HAS_COUNSELING:
        return jsonify({"status": "error", "message": "Counseling engine not available"}), 501

    try:
        data = request.get_json(force=True)
        user_message = data.get("message", "")
        session_id = data.get("session_id")

        # ì‚¬ì£¼/ì ì„±/íƒ€ë¡œ ì»¨í…ìŠ¤íŠ¸
        saju_data = data.get("saju")
        astro_data = data.get("astro")
        tarot_data = data.get("tarot")

        if not user_message.strip():
            return jsonify({"status": "error", "message": "Message is required"}), 400

        engine = get_counseling_engine()
        if not engine:
            return jsonify({"status": "error", "message": "Counseling engine initialization failed"}), 500

        # ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        session = None
        if session_id:
            session = engine.get_session(session_id)

        # ìœµ ì‹¬ë¦¬í•™ ì»¨í…ìŠ¤íŠ¸ í†µí•© ì²˜ë¦¬
        result = engine.process_with_jung_context(
            user_message=user_message,
            session=session,
            saju_data=saju_data,
            astro_data=astro_data,
            tarot_data=tarot_data
        )

        return jsonify({
            "status": "success",
            "response": result["response"],
            "session_id": result["session_id"],
            "phase": result.get("phase"),
            "crisis_detected": result.get("crisis_detected", False),
            "severity": result.get("severity"),
            "should_continue": result.get("should_continue", True),
            "jung_context": result.get("jung_context", {})
        })

    except Exception as e:
        logger.exception(f"[ERROR] /api/counseling/chat failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/api/counseling/therapeutic-questions", methods=["POST"])
def therapeutic_questions():
    """
    ìœµ ì‹¬ë¦¬í•™ ê¸°ë°˜ ì¹˜ë£Œì  ì§ˆë¬¸ ìƒì„±
    - í…Œë§ˆë³„ ë§ì¶¤ ì§ˆë¬¸
    - ì›í˜•(archetype)ë³„ ì§ˆë¬¸
    - ì‹œë§¨í‹± ê²€ìƒ‰ ê¸°ë°˜ ì§ˆë¬¸ ì¶”ì²œ
    """
    if not HAS_COUNSELING:
        return jsonify({"status": "error", "message": "Counseling engine not available"}), 501

    try:
        data = request.get_json(force=True)
        theme = data.get("theme")
        user_message = data.get("user_message", "")
        archetype = data.get("archetype")
        question_type = data.get("question_type", "deepening")

        engine = get_counseling_engine()
        if not engine:
            return jsonify({"status": "error", "message": "Counseling engine initialization failed"}), 500

        # ê¸°ë³¸ ì¹˜ë£Œì  ì§ˆë¬¸
        question = engine.get_therapeutic_question(
            theme=theme,
            archetype=archetype,
            question_type=question_type
        )

        # RAG ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸ (ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš°)
        rag_questions = []
        if user_message and engine.jungian_rag:
            intervention = engine.jungian_rag.get_therapeutic_intervention(
                user_message,
                context={"theme": theme}
            )
            rag_questions = intervention.get("recommended_questions", [])

        return jsonify({
            "status": "success",
            "question": question,
            "rag_questions": rag_questions[:3],
            "theme": theme,
            "archetype": archetype
        })

    except Exception as e:
        logger.exception(f"[ERROR] /api/counseling/therapeutic-questions failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/api/counseling/health", methods=["GET"])
def counseling_health():
    """ìƒë‹´ ì—”ì§„ ìƒíƒœ í™•ì¸"""
    if not HAS_COUNSELING:
        return jsonify({
            "status": "unavailable",
            "message": "Counseling engine not loaded"
        }), 501

    try:
        engine = get_counseling_engine()
        if not engine:
            return jsonify({
                "status": "error",
                "message": "Counseling engine initialization failed"
            }), 500

        is_healthy, status_message = engine.health_check()

        return jsonify({
            "status": "healthy" if is_healthy else "degraded",
            "message": status_message,
            "has_openai": engine.client is not None,
            "model": engine.model_name,
            "has_rag": engine.jungian_rag is not None
        })

    except Exception as e:
        logger.exception(f"[ERROR] /api/counseling/health failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# ===============================================================
# RLHF FEEDBACK LEARNING ENDPOINTS
# ===============================================================

# ===============================================================
# BADGE SYSTEM ENDPOINTS
# ===============================================================

# ===============================================================
# AGENTIC RAG ENDPOINTS (Next Level Features)
# ===============================================================

# ===============================================================
# PREDICTION ENGINE ENDPOINTS (v5.0)
# ëŒ€ìš´/ì„¸ìš´ + íŠ¸ëœì§“ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
# ===============================================================

# ===============================================================
# THEME ENDPOINTS - Moved to routers/theme_routes.py
# ===============================================================


# =============================================================================
# FORTUNE SCORE API (v1.0) - Real-time Saju+Astrology Unified Score
# =============================================================================

# =========================================================
# ê°„ì´ ë§Œì„¸ë ¥ ê³„ì‚° (Daily Fortuneìš©)
# =========================================================
def _calculate_simple_saju(birth_date: str, birth_time: str = "12:00") -> dict:
    """
    ìƒë…„ì›”ì¼ì‹œë¡œ ê¸°ë³¸ ì‚¬ì£¼ ë°ì´í„° ê³„ì‚° (ë§Œì„¸ë ¥ ê°„ì´ ë²„ì „)
    """
    from datetime import datetime as dt_module

    # ì²œê°„/ì§€ì§€ ë°ì´í„°
    STEMS = ["ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"]
    BRANCHES = ["å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥"]
    STEM_ELEMENTS = {"ç”²": "æœ¨", "ä¹™": "æœ¨", "ä¸™": "ç«", "ä¸": "ç«", "æˆŠ": "åœŸ",
                     "å·±": "åœŸ", "åºš": "é‡‘", "è¾›": "é‡‘", "å£¬": "æ°´", "ç™¸": "æ°´"}
    BRANCH_ELEMENTS = {"å­": "æ°´", "ä¸‘": "åœŸ", "å¯…": "æœ¨", "å¯": "æœ¨", "è¾°": "åœŸ", "å·³": "ç«",
                       "åˆ": "ç«", "æœª": "åœŸ", "ç”³": "é‡‘", "é…‰": "é‡‘", "æˆŒ": "åœŸ", "äº¥": "æ°´"}

    # ì‹­ì‹  ê³„ì‚° í—¬í¼
    def get_sibsin(day_stem: str, target_stem: str) -> str:
        dm_idx = STEMS.index(day_stem)
        t_idx = STEMS.index(target_stem)
        diff = (t_idx - dm_idx) % 10
        sibsin_map = {0: "ë¹„ê²¬", 1: "ê²ì¬", 2: "ì‹ì‹ ", 3: "ìƒê´€", 4: "í¸ì¬",
                      5: "ì •ì¬", 6: "í¸ê´€", 7: "ì •ê´€", 8: "í¸ì¸", 9: "ì •ì¸"}
        return sibsin_map.get(diff, "ë¹„ê²¬")

    try:
        # Parse birth date
        bd = dt_module.strptime(birth_date, "%Y-%m-%d")
        year, month, day = bd.year, bd.month, bd.day

        # Parse birth time
        hour = 12
        if birth_time:
            try:
                hour = int(birth_time.split(":")[0])
            except (ValueError, IndexError, AttributeError):
                hour = 12

        # ë…„ì£¼ ê³„ì‚° (1984=ç”²å­ ê¸°ì¤€)
        year_offset = (year - 1984) % 60
        year_stem = STEMS[year_offset % 10]
        year_branch = BRANCHES[year_offset % 12]

        # ì›”ì£¼ ê³„ì‚° (ê°„ëµí™” - ì‹¤ì œë¡œëŠ” ì ˆê¸° ê³ ë ¤ í•„ìš”)
        month_branch_idx = (month + 1) % 12  # å¯…ì›”(1ì›”)ë¶€í„° ì‹œì‘
        month_branch = BRANCHES[month_branch_idx]
        # ì›”ê°„ ê³„ì‚° (ë…„ê°„ ê¸°ì¤€)
        year_stem_idx = STEMS.index(year_stem)
        month_stem_idx = (year_stem_idx * 2 + month) % 10
        month_stem = STEMS[month_stem_idx]

        # ì¼ì£¼ ê³„ì‚° (JDN ê¸°ë°˜)
        a = (14 - month) // 12
        y = year + 4800 - a
        m = month + 12 * a - 3
        jdn = day + (153 * m + 2) // 5 + 365 * y + y // 4 - y // 100 + y // 400 - 32045
        day_offset = (jdn - 11) % 60  # ç”²å­ì¼ ë³´ì •
        day_stem = STEMS[day_offset % 10]
        day_branch = BRANCHES[day_offset % 12]

        # ì‹œì£¼ ê³„ì‚°
        hour_branch_idx = ((hour + 1) // 2) % 12
        hour_branch = BRANCHES[hour_branch_idx]
        day_stem_idx = STEMS.index(day_stem)
        hour_stem_idx = (day_stem_idx * 2 + hour_branch_idx) % 10
        hour_stem = STEMS[hour_stem_idx]

        # ì¼ê°„ (day master)
        dm_element = STEM_ELEMENTS[day_stem]

        # ì‹­ì‹  ë¶„í¬ ê³„ì‚°
        sibsin_dist = {}
        for stem in [year_stem, month_stem, hour_stem]:
            s = get_sibsin(day_stem, stem)
            sibsin_dist[s] = sibsin_dist.get(s, 0) + 1

        # ì˜¤ëŠ˜ ì¼ì§„ ê³„ì‚°
        today = dt_module.now()
        today_jdn = today.day + (153 * ((today.month + 12 * ((14 - today.month) // 12) - 3)) + 2) // 5 + \
                    365 * (today.year + 4800 - ((14 - today.month) // 12)) + \
                    (today.year + 4800 - ((14 - today.month) // 12)) // 4 - \
                    (today.year + 4800 - ((14 - today.month) // 12)) // 100 + \
                    (today.year + 4800 - ((14 - today.month) // 12)) // 400 - 32045
        today_offset = (today_jdn - 11) % 60
        today_stem = STEMS[today_offset % 10]
        today_branch = BRANCHES[today_offset % 12]
        today_element = STEM_ELEMENTS[today_stem]

        # í˜•ì¶©íšŒí•© ê°„ì´ ê³„ì‚°
        CHONG_PAIRS = [("å­", "åˆ"), ("ä¸‘", "æœª"), ("å¯…", "ç”³"), ("å¯", "é…‰"), ("è¾°", "æˆŒ"), ("å·³", "äº¥")]
        HAP_PAIRS = [("å­", "ä¸‘"), ("å¯…", "äº¥"), ("å¯", "æˆŒ"), ("è¾°", "é…‰"), ("å·³", "ç”³"), ("åˆ", "æœª")]

        natal_branches = [year_branch, month_branch, day_branch, hour_branch]
        chung_list = []
        hap_list = []
        for b in natal_branches:
            if (b, today_branch) in CHONG_PAIRS or (today_branch, b) in CHONG_PAIRS:
                chung_list.append(f"{b}-{today_branch}")
            if (b, today_branch) in HAP_PAIRS or (today_branch, b) in HAP_PAIRS:
                hap_list.append(f"{b}-{today_branch}")

        return {
            "dayMaster": {"name": day_stem, "element": dm_element},
            "pillars": {
                "year": year_stem + year_branch,
                "month": month_stem + month_branch,
                "day": day_stem + day_branch,
                "time": hour_stem + hour_branch,
            },
            "unse": {
                "iljin": [{"gan": today_stem, "ji": today_branch, "element": today_element}],
                "monthly": [{"element": STEM_ELEMENTS.get(month_stem, "åœŸ")}],
                "annual": [{"element": STEM_ELEMENTS.get(year_stem, "åœŸ")}],
            },
            "advancedAnalysis": {
                "sibsin": {"distribution": sibsin_dist},
                "hyeongchung": {"chung": chung_list, "hap": hap_list},
                "yongsin": {"primary": {"element": dm_element}},  # ê°„ì´ ìš©ì‹ 
                "geokguk": {"grade": "ì¤‘"},
            },
        }
    except Exception as e:
        logger.warning(f"[SimpleSaju] Calculation error: {e}")
        # Fallback minimal data
        return {
            "dayMaster": {"name": "ç”²", "element": "æœ¨"},
            "pillars": {"year": "ç”²å­", "month": "ç”²å¯…", "day": "ç”²åˆ", "time": "ç”²å­"},
            "unse": {"iljin": [{"element": "æœ¨"}], "monthly": [{"element": "æœ¨"}], "annual": [{"element": "æœ¨"}]},
            "advancedAnalysis": {
                "sibsin": {"distribution": {}},
                "hyeongchung": {"chung": [], "hap": []},
            },
        }


def domain_rag_search():
    """
    Lightweight domain search over precomputed embeddings.
    body: { "domain": "destiny_map|tarot|dream|iching", "query": "...", "top_k": 5 }
    """
    if not HAS_DOMAIN_RAG:
        return jsonify({"status": "error", "message": "DomainRAG not available"}), 501

    def _expand_tarot_query(query: str) -> str:
        """Add lightweight Korean hints when English tarot queries return empty."""
        lower = query.lower()
        extras = []
        if any(k in lower for k in ["business", "startup", "entrepreneur", "start a business", "company"]):
            extras.append("ì‚¬ì—… ì°½ì—…")
        if any(k in lower for k in ["career", "job", "work", "promotion", "interview", "resume"]):
            extras.append("ì§ì¥ ì»¤ë¦¬ì–´ ì´ì§")
        if any(k in lower for k in ["love", "relationship", "dating", "partner", "marriage", "breakup", "ex"]):
            extras.append("ì—°ì•  ê´€ê³„ ê²°í˜¼")
        if any(k in lower for k in ["travel", "trip", "journey", "move", "relocation", "relocate"]):
            extras.append("ì—¬í–‰ ì´ë™ ì´ì‚¬")
        if any(k in lower for k in ["blocking", "blockage", "stuck", "progress", "obstacle", "challenge"]):
            extras.append("ì¥ì• ë¬¼ ì •ì²´ ì„±ì¥")
        if any(k in lower for k in ["strength", "strengths", "talent", "ability"]):
            extras.append("ê°•ì  ì¬ëŠ¥")
        if any(k in lower for k in ["money", "finance", "financial", "invest", "investment", "stock", "stocks", "crypto", "bitcoin"]):
            extras.append("ì¬ë¬¼ ëˆ íˆ¬ì")
        if any(k in lower for k in ["health", "ill", "sick", "anxiety", "stress", "depression", "mental"]):
            extras.append("ê±´ê°• ë§ˆìŒ ë¶ˆì•ˆ")
        if any(k in lower for k in ["decision", "choice", "choose", "should i", "which", "either", "vs"]):
            extras.append("ì„ íƒ ê²°ì •")
        if any(k in lower for k in ["timing", "when", "soon", "next", "this year", "next year"]):
            extras.append("íƒ€ì´ë° ì‹œê¸°")
        if any(k in lower for k in ["family", "parents", "child", "children"]):
            extras.append("ê°€ì¡± ê´€ê³„")
        if any(k in lower for k in ["study", "school", "exam", "test"]):
            extras.append("ì‹œí—˜ ê³µë¶€")

        # Korean keywords â†’ English hints (help when corpus is English-heavy)
        if any(k in query for k in ["ì‚¬ì—…", "ì°½ì—…", "ìì˜ì—…", "ìŠ¤íƒ€íŠ¸ì—…"]):
            extras.append("business startup")
        if any(k in query for k in ["ì§ì¥", "ì»¤ë¦¬ì–´", "ì´ì§", "ì·¨ì—…", "ì§ë¬´", "ë©´ì ‘", "ìŠ¹ì§„", "ì—°ë´‰", "ì—…ë¬´"]):
            extras.append("career job work")
        if any(k in query for k in ["ì—°ì• ", "ì‚¬ë‘", "ê´€ê³„", "ê²°í˜¼", "ì´ë³„", "ì¬íšŒ", "ê¶í•©", "ì¸", "ì§ì‚¬ë‘", "ì „ë‚¨ì¹œ", "ì „ì—¬ì¹œ", "ê·¸ ì‚¬ëŒ", "ìƒëŒ€", "ìƒëŒ€ë°©", "ë§ˆìŒ", "í˜¸ê°"]):
            extras.append("love relationship")
        if any(k in query for k in ["ëˆ", "ì¬ë¬¼", "ê¸ˆì „", "ì¬ì •", "íˆ¬ì", "ì£¼ì‹", "ì½”ì¸", "ë¶€ë™ì‚°", "ëŒ€ì¶œ", "ë¹š", "ì €ì¶•", "ìˆ˜ì…", "ì›”ê¸‰", "ìˆ˜ìµ"]):
            extras.append("money finance investment")
        if any(k in query for k in ["ê±´ê°•", "ëª¸", "ìš°ìš¸", "ë¶ˆì•ˆ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë³‘", "ì¹˜ë£Œ", "íšŒë³µ", "ë©˜íƒˆ"]):
            extras.append("health stress")
        if any(k in query for k in ["ê²°ì •", "ì„ íƒ", "ê°ˆë¦¼ê¸¸", "í• ê¹Œ", "ë ê¹Œ", "íƒ€ì´ë°", "ì‹œê¸°", "ì–¸ì œ"]):
            extras.append("decision timing")
        if any(k in query for k in ["ì—¬í–‰", "ì´ì‚¬", "ì´ë™", "ì¶œì¥"]):
            extras.append("travel move")
        if any(k in query for k in ["ê°•ì ", "ì¥ì ", "ì¬ëŠ¥", "ëŠ¥ë ¥"]):
            extras.append("strength identity")
        if any(k in query for k in ["ë§‰í˜", "ì¥ì• ë¬¼", "ì •ì²´", "ì§„ì „", "ë°©í•´"]):
            extras.append("obstacle growth")
        if any(k in query for k in ["ê°€ì¡±", "ë¶€ëª¨", "ìë…€", "ì•„ì´"]):
            extras.append("family")
        if any(k in query for k in ["ê³µë¶€", "ì‹œí—˜", "í•©ê²©", "ìˆ˜ëŠ¥", "ìê²©ì¦", "ìœ í•™", "í•™ì—…"]):
            extras.append("study exam")
        if not extras:
            return query
        return f"{query} | {' '.join(extras)}"

    def _fallback_tarot_queries(query: str) -> list:
        """Provide compact fallback queries when expanded search still returns empty."""
        lower = query.lower()
        fallbacks = []
        if any(k in lower for k in ["business", "startup", "entrepreneur", "start a business", "company"]):
            fallbacks.extend(["business", "career"])
        if any(k in lower for k in ["career", "job", "work", "promotion", "interview", "resume"]):
            fallbacks.extend(["career", "job"])
        if any(k in lower for k in ["love", "relationship", "dating", "partner", "marriage", "breakup", "ex"]):
            fallbacks.extend(["love", "relationship"])
        if any(k in lower for k in ["money", "finance", "financial", "invest", "investment", "stock", "stocks", "crypto", "bitcoin"]):
            fallbacks.extend(["money", "finance"])
        if any(k in lower for k in ["health", "ill", "sick", "anxiety", "stress", "depression", "mental"]):
            fallbacks.extend(["health", "stress"])
        if any(k in lower for k in ["decision", "choice", "choose", "should i", "which", "either", "vs"]):
            fallbacks.extend(["decision", "timing"])
        if any(k in lower for k in ["travel", "trip", "journey", "move", "relocation", "relocate"]):
            fallbacks.extend(["travel", "journey"])
        if any(k in lower for k in ["blocking", "blockage", "stuck", "progress", "obstacle", "challenge"]):
            fallbacks.extend(["obstacle", "challenge"])
        if any(k in lower for k in ["strength", "strengths", "talent", "ability"]):
            fallbacks.extend(["strength", "identity"])
        if any(k in lower for k in ["timing", "when", "soon", "next", "this year", "next year"]):
            fallbacks.extend(["timing", "when"])
        if any(k in lower for k in ["family", "parents", "child", "children"]):
            fallbacks.extend(["family"])
        if any(k in lower for k in ["study", "school", "exam", "test"]):
            fallbacks.extend(["study", "exam"])
        if any(k in query for k in ["ì‚¬ì—…", "ì°½ì—…", "ìì˜ì—…", "ìŠ¤íƒ€íŠ¸ì—…"]):
            fallbacks.extend(["business", "career"])
        if any(k in query for k in ["ì§ì¥", "ì»¤ë¦¬ì–´", "ì´ì§", "ì·¨ì—…", "ì§ë¬´", "ë©´ì ‘", "ìŠ¹ì§„", "ì—°ë´‰", "ì—…ë¬´"]):
            fallbacks.extend(["career", "job"])
        if any(k in query for k in ["ì—°ì• ", "ì‚¬ë‘", "ê´€ê³„", "ê²°í˜¼", "ì´ë³„", "ì¬íšŒ", "ê¶í•©", "ì¸", "ì§ì‚¬ë‘", "ì „ë‚¨ì¹œ", "ì „ì—¬ì¹œ", "ê·¸ ì‚¬ëŒ", "ìƒëŒ€", "ìƒëŒ€ë°©", "ë§ˆìŒ", "í˜¸ê°"]):
            fallbacks.extend(["love", "relationship"])
        if any(k in query for k in ["ëˆ", "ì¬ë¬¼", "ê¸ˆì „", "ì¬ì •", "íˆ¬ì", "ì£¼ì‹", "ì½”ì¸", "ë¶€ë™ì‚°", "ëŒ€ì¶œ", "ë¹š", "ì €ì¶•", "ìˆ˜ì…", "ì›”ê¸‰", "ìˆ˜ìµ"]):
            fallbacks.extend(["money", "finance"])
        if any(k in query for k in ["ê±´ê°•", "ëª¸", "ìš°ìš¸", "ë¶ˆì•ˆ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë³‘", "ì¹˜ë£Œ", "íšŒë³µ", "ë©˜íƒˆ"]):
            fallbacks.extend(["health", "stress"])
        if any(k in query for k in ["ê²°ì •", "ì„ íƒ", "ê°ˆë¦¼ê¸¸", "í• ê¹Œ", "ë ê¹Œ", "íƒ€ì´ë°", "ì‹œê¸°", "ì–¸ì œ"]):
            fallbacks.extend(["decision", "timing"])
        if any(k in query for k in ["ì—¬í–‰", "ì´ì‚¬", "ì´ë™", "ì¶œì¥"]):
            fallbacks.extend(["travel", "journey"])
        if any(k in query for k in ["ê°•ì ", "ì¥ì ", "ì¬ëŠ¥", "ëŠ¥ë ¥"]):
            fallbacks.extend(["strength", "identity"])
        if any(k in query for k in ["ë§‰í˜", "ì¥ì• ë¬¼", "ì •ì²´", "ì§„ì „", "ë°©í•´"]):
            fallbacks.extend(["obstacle", "challenge"])
        if any(k in query for k in ["ê°€ì¡±", "ë¶€ëª¨", "ìë…€", "ì•„ì´"]):
            fallbacks.extend(["family"])
        if any(k in query for k in ["ê³µë¶€", "ì‹œí—˜", "í•©ê²©", "ìˆ˜ëŠ¥", "ìê²©ì¦", "ìœ í•™", "í•™ì—…"]):
            fallbacks.extend(["study", "exam"])
        # De-dup while preserving order
        seen = set()
        deduped = []
        for item in fallbacks:
            if item in seen:
                continue
            seen.add(item)
            deduped.append(item)
        return deduped

    try:
        data = request.get_json(force=True)
        domain = (data.get("domain") or "").strip()
        query = (data.get("query") or "").strip()
        top_k = int(data.get("top_k", 5))
        top_k = max(1, min(top_k, 20))

        if not query:
            return jsonify({"status": "error", "message": "query is required"}), 400

        rag = get_domain_rag()
        if not rag:
            return jsonify({"status": "error", "message": "DomainRAG not available"}), 501

        if not domain or domain not in DOMAIN_RAG_DOMAINS:
            return jsonify({
                "status": "error",
                "message": f"domain must be one of {DOMAIN_RAG_DOMAINS}",
            }), 400

        rag.load_domain(domain)

        results = rag.search(domain, query, top_k=top_k)
        context = rag.get_context(domain, query, top_k=min(top_k, 3), max_chars=1500)
        expanded_query = ""

        fallback_query = ""
        if domain == "tarot" and not results:
            expanded_query = _expand_tarot_query(query)
            if expanded_query != query:
                results = rag.search(domain, expanded_query, top_k=top_k)
                context = rag.get_context(domain, expanded_query, top_k=min(top_k, 3), max_chars=1500)
        if domain == "tarot" and not results:
            for candidate in _fallback_tarot_queries(query):
                results = rag.search(domain, candidate, top_k=top_k)
                context = rag.get_context(domain, candidate, top_k=min(top_k, 3), max_chars=1500)
                if results:
                    fallback_query = candidate
                    break

        return jsonify({
            "status": "success",
            "domain": domain,
            "query": query,
            "expanded_query": expanded_query or None,
            "fallback_query": fallback_query or None,
            "results": results,
            "context": context,
        })

    except Exception as e:
        logger.exception(f"[ERROR] /api/search/domain failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def hybrid_rag_search():
    """
    Hybrid search (vector + BM25 + graph, optional rerank).
    body: { "query": "...", "top_k": 8, "rerank": true, "graph_root": "<optional>" }
    """
    if not HAS_HYBRID_RAG:
        return jsonify({"status": "error", "message": "Hybrid RAG not available"}), 501

    try:
        data = request.get_json(force=True)
        query = (data.get("query") or "").strip()
        top_k = int(data.get("top_k", 8))
        top_k = max(1, min(top_k, 30))
        rerank = bool(data.get("rerank", True))
        graph_root = data.get("graph_root")

        if not query:
            return jsonify({"status": "error", "message": "query is required"}), 400

        results = hybrid_search(
            query=query,
            top_k=top_k,
            use_reranking=rerank,
            graph_root=graph_root,
        )
        context = build_rag_context(query, top_k=min(12, max(top_k, 6)))

        return jsonify({
            "status": "success",
            "query": query,
            "top_k": top_k,
            "rerank": rerank,
            "results": results,
            "context": context,
        })

    except Exception as e:
        logger.exception(f"[ERROR] /api/search/hybrid failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# ===============================================================
# COMPATIBILITY ENDPOINTS - Moved to routers/compatibility_routes.py
# ===============================================================


# System capabilities
def get_capabilities():
    """Get system capabilities (what's enabled)."""
    return jsonify({
        "status": "success",
        "capabilities": {
            "realtime_transits": HAS_REALTIME,
            "chart_generation": HAS_CHARTS,
            "user_memory": HAS_USER_MEMORY,
            "iching_premium": HAS_ICHING,
            "persona_embeddings": HAS_PERSONA_EMBED,
            "tarot_premium": HAS_TAROT,
            "rlhf_learning": HAS_RLHF,
            "badge_system": HAS_BADGES,
            "agentic_rag": HAS_AGENTIC,
            "jungian_counseling": HAS_COUNSELING,
            "prediction_engine": HAS_PREDICTION,
            "theme_cross_filter": HAS_THEME_FILTER,
            "fortune_score": HAS_FORTUNE_SCORE,
            "hybrid_rag": HAS_HYBRID_RAG,
            "domain_rag": HAS_DOMAIN_RAG,
            "compatibility": HAS_COMPATIBILITY,
            "numerology": HAS_NUMEROLOGY,
            "icp": HAS_ICP,
        },
        "version": "5.3.0-numerology-icp",
    })


# ===============================================================
# NUMEROLOGY ENDPOINTS - Moved to routers/numerology_routes.py
# ===============================================================

# ===============================================================
# ICP ENDPOINTS - Moved to routers/icp_routes.py
# ===============================================================

# ===============================================================
# SESSION SUMMARY API - Auto-generate counseling session summaries
# ===============================================================

# ============================================================
# SAJU-ONLY COUNSELOR ENDPOINTS
# ============================================================

def saju_counselor_init():
    """
    Initialize saju-only counselor session with pre-fetched RAG data.
    Similar to /counselor/init but focuses only on saju knowledge.
    """
    try:
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        saju_data = data.get("saju") or {}
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "life")
        locale = data.get("locale", "ko")

        # Normalize dayMaster structure
        saju_data = normalize_day_master(saju_data)

        has_saju_payload = _has_saju_payload(saju_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and not has_saju_payload:
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[SAJU-COUNSELOR-INIT] Invalid birth format for missing payload")
                    return jsonify({"status": "error", "message": _build_birth_format_message(locale)}), 400
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=True,
                missing_astro=False,
            )
            logger.warning("[SAJU-COUNSELOR-INIT] Missing computed saju payload")
            return jsonify({"status": "error", "message": missing_message}), 400

        logger.info(f"[SAJU-COUNSELOR-INIT] id={g.request_id} theme={theme}")

        # Compute saju if not provided but birth info is available
        if _bool_env("ALLOW_BIRTH_ONLY") and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                saju_data = _calculate_simple_saju(
                    birth_data["date"],
                    birth_data["time"],
                )
                saju_data = normalize_day_master(saju_data)
                logger.info(f"[SAJU-COUNSELOR-INIT] Computed simple saju from birth: {saju_data.get('dayMaster', {})}")
            except Exception as e:
                logger.warning(f"[SAJU-COUNSELOR-INIT] Failed to compute simple saju: {e}")

        # Generate session ID
        session_id = str(uuid4())[:12]

        start_time = time.time()

        # Pre-fetch saju-specific RAG data only (no astrology)
        rag_data = {
            "graph_nodes": [],
            "corpus_quotes": [],
            "persona_context": {},
        }

        # Load saju-specific graph rules
        if HAS_GRAPH_RAG:
            try:
                from backend_ai.app.saju_astro_rag import search_graphs
                # Query saju-specific rules
                day_master = saju_data.get("dayMaster", {}).get("heavenlyStem", "")
                queries = [
                    f"ì‚¬ì£¼ ì¼ê°„ {day_master} íŠ¹ì„±",
                    f"ì˜¤í–‰ ê· í˜• ë¶„ì„",
                    f"ëŒ€ìš´ ì„¸ìš´ í•´ì„",
                    f"ì‚¬ì£¼ {theme} ìš´ì„¸",
                ]
                for q in queries:
                    nodes = search_graphs(q, top_k=3)
                    for node in nodes:
                        text = node.get("description") or node.get("label") or ""
                        if text:
                            rag_data["graph_nodes"].append(text)
            except Exception as e:
                logger.warning(f"[SAJU-COUNSELOR-INIT] Graph RAG failed: {e}")

        prefetch_time_ms = int((time.time() - start_time) * 1000)
        rag_data["prefetch_time_ms"] = prefetch_time_ms

        # Store in session cache
        set_session_rag_cache(session_id, {
            "rag_data": rag_data,
            "saju_data": saju_data,
            "astro_data": {},  # No astrology data
            "theme": theme,
            "counselor_type": "saju",
        })

        return jsonify({
            "status": "success",
            "session_id": session_id,
            "prefetch_time_ms": prefetch_time_ms,
            "data_summary": {
                "graph_nodes": len(rag_data.get("graph_nodes", [])),
            }
        })

    except Exception as e:
        logger.exception(f"[ERROR] /saju/counselor/init failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def saju_ask_stream():
    """
    Streaming chat for saju-only counselor.
    Uses Server-Sent Events (SSE) for real-time responses.
    Focuses exclusively on saju interpretation without astrology.
    """
    try:
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        saju_data = data.get("saju") or {}
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "life")
        locale = data.get("locale", "ko")
        prompt = (data.get("prompt") or "")[:1500]
        session_id = data.get("session_id")
        conversation_history = data.get("history") or []
        user_context = data.get("user_context") or {}

        # Normalize dayMaster structure
        saju_data = normalize_day_master(saju_data)

        logger.info(f"[SAJU-ASK-STREAM] id={g.request_id} theme={theme} locale={locale}")

        # Check for pre-fetched RAG data from session
        session_cache = None
        rag_context = ""
        if session_id:
            session_cache = get_session_rag_cache(session_id)
            if session_cache:
                if not saju_data:
                    saju_data = session_cache.get("saju_data", {})

                rag_data = session_cache.get("rag_data", {})
                if rag_data.get("graph_nodes"):
                    rag_context += "\n[ì‚¬ì£¼ ê´€ë ¨ ì§€ì‹]\n"
                    rag_context += "\n".join(rag_data["graph_nodes"][:8])

        # Compute saju if not provided (optional fallback)
        if _bool_env("ALLOW_BIRTH_ONLY") and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                saju_data = _calculate_simple_saju(
                    birth_data["date"],
                    birth_data["time"],
                )
                saju_data = normalize_day_master(saju_data)
            except Exception as e:
                logger.warning(f"[SAJU-ASK-STREAM] Failed to compute simple saju: {e}")

        has_saju_payload = _has_saju_payload(saju_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and not has_saju_payload:
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[SAJU-ASK-STREAM] Invalid birth format for missing payload")
                    return _sse_error_response(_build_birth_format_message(locale))
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=True,
                missing_astro=False,
            )
            logger.warning("[SAJU-ASK-STREAM] Missing computed saju payload")
            return _sse_error_response(missing_message)

        # Build detailed saju context (NO astrology)
        saju_detail = _build_detailed_saju(saju_data)

        # Current date
        from datetime import datetime
        now = datetime.now()
        weekdays_ko = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        current_date_str = f"ì˜¤ëŠ˜: {now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekdays_ko[now.weekday()]})"

        # Build user context section
        user_context_section = ""
        if user_context:
            persona = user_context.get("persona", {})
            if persona.get("sessionCount", 0) > 0:
                user_context_section = f"\n[ì´ì „ ìƒë‹´]\nâ€¢ {persona.get('sessionCount', 0)}íšŒ ë°©ë¬¸ ê³ ê°\n"

        # Build saju-focused system prompt
        if locale == "ko":
            system_prompt = f"""ë„ˆëŠ” ì‚¬ì£¼(å››æŸ±) ì „ë¬¸ ìƒë‹´ì‚¬ë‹¤. ë™ì–‘ ëª…ë¦¬í•™ ì „ë¬¸ê°€ë¡œì„œ ìƒë‹´í•´.

ì ˆëŒ€ ê·œì¹™:
1. ì¸ì‚¬ ê¸ˆì§€ - ë°”ë¡œ ë¶„ì„ ì‹œì‘
2. ì‚¬ì£¼ ë¶„ì„ì—ë§Œ ì§‘ì¤‘ - ì„œì–‘ ì ì„±ìˆ  ì–¸ê¸‰ ê¸ˆì§€
3. ì œê³µëœ ëŒ€ìš´/ì„¸ìš´ ë°ì´í„°ë§Œ ì‚¬ìš©
4. í•œêµ­ ì‚¬ì£¼ ìš©ì–´ ì‚¬ìš© (ì¼ê°„, ìš©ì‹ , ëŒ€ìš´, ì„¸ìš´, ì˜¤í–‰ ë“±)

{current_date_str}

[ì‚¬ì£¼ ëª…ì‹]
{saju_detail}

{rag_context}
{user_context_section}

ì‘ë‹µ í˜•ì‹:
ã€ì¼ê°„ã€‘ ì¼ê°„ì˜ íŠ¹ì„±ê³¼ í˜„ì¬ ìƒíƒœ
ã€ëŒ€ìš´ã€‘ í˜„ì¬ ëŒ€ìš´ ë¶„ì„
ã€ì„¸ìš´ã€‘ ì˜¬í•´ ì„¸ìš´ ë¶„ì„
ã€ì˜¤í–‰ã€‘ ì˜¤í–‰ ê· í˜•ê³¼ ë³´ì™„ ë°©ë²•
ã€ì¡°ì–¸ã€‘ 2-3ê°œ ì‹¤ì²œ ì¡°ì–¸

200-300ë‹¨ì–´ë¡œ ë‹µë³€."""
        else:
            system_prompt = f"""You are a Saju (Four Pillars of Destiny) counselor specializing in Eastern fortune-telling.

RULES:
1. NO GREETING - Start directly with analysis
2. Focus ONLY on Saju - NO Western astrology
3. Use only provided daeun/seun data
4. Use proper Saju terminology

{current_date_str}

[Saju Chart]
{saju_detail}

{rag_context}
{user_context_section}

Response format:
ã€Day Masterã€‘ Characteristics and current state
ã€Major Luckã€‘ Current major luck cycle
ã€Annual Luckã€‘ This year's luck
ã€Five Elementsã€‘ Balance and recommendations
ã€Adviceã€‘ 2-3 practical actions

200-300 words."""

        # Full prompt
        full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"

        default_model = os.getenv("CHAT_MODEL") or os.getenv("FUSION_MODEL") or "gpt-4.1"
        default_temp = _clamp_temperature(_coerce_float(os.getenv("CHAT_TEMPERATURE")), 0.75)
        model_name, temperature, _ab_variant = _select_model_and_temperature(
            data,
            default_model,
            default_temp,
            session_id,
            g.request_id,
        )

        # Streaming response
        def generate():
            try:
                max_tokens = _get_int_env("SAJU_ASK_MAX_TOKENS", 700, min_value=300, max_value=2000)
                response = openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                full_text = ""
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        full_text += chunk.choices[0].delta.content

                if not full_text.strip():
                    yield "data: [DONE]\n\n"
                    return

                addendum = _build_missing_requirements_addendum(
                    full_text,
                    locale,
                    saju_data,
                    {},
                    now.date(),
                    require_saju=True,
                    require_astro=False,
                    require_timing=True,
                    require_caution=True,
                )
                if addendum:
                    full_text = _insert_addendum(full_text, addendum)

                full_text = _format_korean_spacing(full_text)
                if locale == "ko" and not full_text.rstrip().endswith("?"):
                    followup_inline = "ì§€ê¸ˆ ê°€ì¥ ê¶ê¸ˆí•œ í¬ì¸íŠ¸ê°€ ë­ì˜ˆìš”?"
                    separator = "" if (full_text.endswith((" ", "\n", "\t")) or not full_text) else " "
                    full_text += f"{separator}{followup_inline}"

                chunk_size = _get_stream_chunk_size()
                for piece in _chunk_text(full_text, chunk_size):
                    yield _to_sse_event(piece)

                # Add follow-up questions
                follow_ups = [
                    "ì˜¬í•´ ì„¸ìš´ì´ ì œ ìš´ì„¸ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ë‚˜ìš”?",
                    "ì œ ìš©ì‹ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ì˜¤í–‰ ê· í˜•ì„ ì–´ë–»ê²Œ ë§ì¶œ ìˆ˜ ìˆë‚˜ìš”?",
                ] if locale == "ko" else [
                    "How does this year's luck affect me?",
                    "What is my favorable element?",
                    "How can I balance my five elements?",
                ]
                yield f"data: ||FOLLOWUP||{json.dumps(follow_ups, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"[SAJU-ASK-STREAM] Streaming error: {e}")
                yield f"data: ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n"
                yield "data: [DONE]\n\n"

        return Response(
            stream_with_context(generate()),
            mimetype="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] /saju/ask-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# ============================================================
# ASTROLOGY-ONLY COUNSELOR ENDPOINTS
# ============================================================

def astrology_counselor_init():
    """
    Initialize astrology-only counselor session with pre-fetched RAG data.
    Similar to /counselor/init but focuses only on Western astrology.
    """
    try:
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        astro_data = data.get("astro") or {}
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "life")
        locale = data.get("locale", "ko")

        has_astro_payload = _has_astro_payload(astro_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and not has_astro_payload:
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[ASTROLOGY-COUNSELOR-INIT] Invalid birth format for missing payload")
                    return jsonify({"status": "error", "message": _build_birth_format_message(locale)}), 400
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=False,
                missing_astro=True,
            )
            logger.warning("[ASTROLOGY-COUNSELOR-INIT] Missing computed astro payload")
            return jsonify({"status": "error", "message": missing_message}), 400

        logger.info(f"[ASTROLOGY-COUNSELOR-INIT] id={g.request_id} theme={theme}")

        # Generate session ID
        session_id = str(uuid4())[:12]

        start_time = time.time()

        # Compute astrology if not provided but birth info is available (optional fallback)
        if _bool_env("ALLOW_BIRTH_ONLY") and (not _has_astro_payload(astro_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                lat = birth_data.get("lat") or birth_data.get("latitude") or 37.5665
                lon = birth_data.get("lon") or birth_data.get("longitude") or 126.9780
                date_parts = birth_data["date"].split("-")
                time_parts = birth_data["time"].split(":")
                astro_data = calculate_astrology_data({
                    "year": int(date_parts[0]),
                    "month": int(date_parts[1]),
                    "day": int(date_parts[2]),
                    "hour": int(time_parts[0]),
                    "minute": int(time_parts[1]) if len(time_parts) > 1 else 0,
                    "latitude": lat,
                    "longitude": lon,
                })
            except Exception as e:
                logger.warning(f"[ASTROLOGY-COUNSELOR-INIT] Failed to compute astro: {e}")

        # Pre-fetch astrology-specific RAG data only (no saju)
        rag_data = {
            "graph_nodes": [],
            "corpus_quotes": [],
        }

        # Load astrology-specific graph rules
        try:
            from backend_ai.app.graph_rag import get_graph_rag
            graph_rag = get_graph_rag()
            if graph_rag:
                sun_sign = astro_data.get("sun", {}).get("sign", "")
                moon_sign = astro_data.get("moon", {}).get("sign", "")
                queries = [
                    f"íƒœì–‘ {sun_sign} íŠ¹ì„±",
                    f"ë‹¬ {moon_sign} ê°ì •",
                    f"í–‰ì„± íŠ¸ëœì§“ ì˜í–¥",
                    f"ì ì„±ìˆ  {theme} í•´ì„",
                ]
                for q in queries:
                    nodes = graph_rag.search(q, top_k=3)
                    rag_data["graph_nodes"].extend([n.get("text", "") for n in nodes if n.get("text")])
        except Exception as e:
            logger.warning(f"[ASTROLOGY-COUNSELOR-INIT] Graph RAG failed: {e}")

        prefetch_time_ms = int((time.time() - start_time) * 1000)
        rag_data["prefetch_time_ms"] = prefetch_time_ms

        # Store in session cache
        set_session_rag_cache(session_id, {
            "rag_data": rag_data,
            "saju_data": {},  # No saju data
            "astro_data": astro_data,
            "theme": theme,
            "counselor_type": "astrology",
        })

        return jsonify({
            "status": "success",
            "session_id": session_id,
            "astro": astro_data,  # Return computed astro data
            "prefetch_time_ms": prefetch_time_ms,
            "data_summary": {
                "graph_nodes": len(rag_data.get("graph_nodes", [])),
            }
        })

    except Exception as e:
        logger.exception(f"[ERROR] /astrology/counselor/init failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


def astrology_ask_stream():
    """
    Streaming chat for astrology-only counselor.
    Uses Server-Sent Events (SSE) for real-time responses.
    Focuses exclusively on Western astrology without saju.
    """
    try:
        import json as json_mod
        raw_data = request.get_data(as_text=False)
        data = json_mod.loads(raw_data.decode('utf-8'))

        astro_data = data.get("astro") or {}
        birth_data = _normalize_birth_payload(data)
        theme = data.get("theme", "life")
        locale = data.get("locale", "ko")
        prompt = (data.get("prompt") or "")[:1500]
        session_id = data.get("session_id")
        conversation_history = data.get("history") or []
        user_context = data.get("user_context") or {}

        logger.info(f"[ASTROLOGY-ASK-STREAM] id={g.request_id} theme={theme} locale={locale}")

        # Check for pre-fetched RAG data from session
        session_cache = None
        rag_context = ""
        if session_id:
            session_cache = get_session_rag_cache(session_id)
            if session_cache:
                if not astro_data:
                    astro_data = session_cache.get("astro_data", {})

                rag_data = session_cache.get("rag_data", {})
                if rag_data.get("graph_nodes"):
                    rag_context += "\n[ì ì„±ìˆ  ê´€ë ¨ ì§€ì‹]\n"
                    rag_context += "\n".join(rag_data["graph_nodes"][:8])

        # Compute astrology if not provided (optional fallback)
        if _bool_env("ALLOW_BIRTH_ONLY") and (not _has_astro_payload(astro_data)) and birth_data.get("date") and birth_data.get("time"):
            try:
                lat = birth_data.get("lat") or birth_data.get("latitude") or 37.5665
                lon = birth_data.get("lon") or birth_data.get("longitude") or 126.9780
                date_parts = birth_data["date"].split("-")
                time_parts = birth_data["time"].split(":")
                astro_data = calculate_astrology_data({
                    "year": int(date_parts[0]),
                    "month": int(date_parts[1]),
                    "day": int(date_parts[2]),
                    "hour": int(time_parts[0]),
                    "minute": int(time_parts[1]) if len(time_parts) > 1 else 0,
                    "latitude": lat,
                    "longitude": lon,
                })
            except Exception as e:
                logger.warning(f"[ASTROLOGY-ASK-STREAM] Failed to compute astro: {e}")

        has_astro_payload = _has_astro_payload(astro_data)
        require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))
        if require_computed_payload and not has_astro_payload:
            if birth_data.get("date") or birth_data.get("time"):
                valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                if not valid_birth:
                    logger.warning("[ASTROLOGY-ASK-STREAM] Invalid birth format for missing payload")
                    return _sse_error_response(_build_birth_format_message(locale))
            missing_message = _build_missing_payload_message(
                locale,
                missing_saju=False,
                missing_astro=True,
            )
            logger.warning("[ASTROLOGY-ASK-STREAM] Missing computed astro payload")
            return _sse_error_response(missing_message)

        # Build detailed astrology context (NO saju)
        astro_detail = _build_detailed_astro(astro_data)

        # Current date
        from datetime import datetime
        now = datetime.now()
        weekdays_ko = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        current_date_str = f"ì˜¤ëŠ˜: {now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekdays_ko[now.weekday()]})"

        # Build user context section
        user_context_section = ""
        if user_context:
            persona = user_context.get("persona", {})
            if persona.get("sessionCount", 0) > 0:
                user_context_section = f"\n[ì´ì „ ìƒë‹´]\nâ€¢ {persona.get('sessionCount', 0)}íšŒ ë°©ë¬¸ ê³ ê°\n"

        # Build astrology-focused system prompt
        if locale == "ko":
            system_prompt = f"""ë„ˆëŠ” ì„œì–‘ ì ì„±ìˆ  ì „ë¬¸ ìƒë‹´ì‚¬ë‹¤. ì¶œìƒ ì°¨íŠ¸ ë¶„ì„ê³¼ í–‰ì„± íŠ¸ëœì§“ ì „ë¬¸ê°€ì•¼.

ì ˆëŒ€ ê·œì¹™:
1. ì¸ì‚¬ ê¸ˆì§€ - ë°”ë¡œ ë¶„ì„ ì‹œì‘
2. ì„œì–‘ ì ì„±ìˆ ì—ë§Œ ì§‘ì¤‘ - ì‚¬ì£¼/ë™ì–‘ ì—­ìˆ  ì–¸ê¸‰ ê¸ˆì§€
3. ì ì„±ìˆ  ìš©ì–´ ì‚¬ìš© (ë³„ìë¦¬, í•˜ìš°ìŠ¤, ì• ìŠ¤í™íŠ¸, íŠ¸ëœì§“ ë“±)
4. êµ¬ì²´ì ì¸ í–‰ì„± ìœ„ì¹˜ì™€ ê°ë„ ì–¸ê¸‰

{current_date_str}

[ì¶œìƒ ì°¨íŠ¸]
{astro_detail}

{rag_context}
{user_context_section}

ì‘ë‹µ í˜•ì‹:
ã€íƒœì–‘/ë‹¬ã€‘ íƒœì–‘ê³¼ ë‹¬ ë³„ìë¦¬ì˜ í•µì‹¬ ì„±ê²©
ã€ìƒìŠ¹ê¶ã€‘ ì–´ì„¼ë˜íŠ¸ê°€ ì™¸ì  í˜ë¥´ì†Œë‚˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
ã€íŠ¸ëœì§“ã€‘ í˜„ì¬ í–‰ì„± íŠ¸ëœì§“ê³¼ ê·¸ ì˜í–¥
ã€í•˜ìš°ìŠ¤ã€‘ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•˜ìš°ìŠ¤ ë°°ì¹˜
ã€ì¡°ì–¸ã€‘ 2-3ê°œ ì‹¤ì²œ ì¡°ì–¸

200-300ë‹¨ì–´ë¡œ ë‹µë³€."""
        else:
            system_prompt = f"""You are a Western Astrology counselor specializing in birth chart analysis.

RULES:
1. NO GREETING - Start directly with analysis
2. Focus ONLY on Western Astrology - NO Eastern fortune-telling
3. Use proper astrological terminology (signs, houses, aspects, transits)
4. Include specific planetary positions

{current_date_str}

[Birth Chart]
{astro_detail}

{rag_context}
{user_context_section}

Response format:
ã€Sun/Moonã€‘ Core personality from Sun and Moon signs
ã€Risingã€‘ Ascendant influence
ã€Transitsã€‘ Current planetary transits
ã€Housesã€‘ Relevant house placements
ã€Guidanceã€‘ 2-3 practical actions

200-300 words."""

        # Full prompt
        full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"

        default_model = os.getenv("CHAT_MODEL") or os.getenv("FUSION_MODEL") or "gpt-4.1"
        default_temp = _clamp_temperature(_coerce_float(os.getenv("CHAT_TEMPERATURE")), 0.75)
        model_name, temperature, _ab_variant = _select_model_and_temperature(
            data,
            default_model,
            default_temp,
            session_id,
            g.request_id,
        )

        # Streaming response
        def generate():
            try:
                max_tokens = _get_int_env("ASTRO_ASK_MAX_TOKENS", 700, min_value=300, max_value=2000)
                response = openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                full_text = ""
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        full_text += chunk.choices[0].delta.content

                if not full_text.strip():
                    yield "data: [DONE]\n\n"
                    return

                addendum = _build_missing_requirements_addendum(
                    full_text,
                    locale,
                    {},
                    astro_data,
                    now.date(),
                    require_saju=False,
                    require_astro=True,
                    require_timing=True,
                    require_caution=True,
                )
                if addendum:
                    full_text = _insert_addendum(full_text, addendum)

                full_text = _format_korean_spacing(full_text)
                if locale == "ko" and not full_text.rstrip().endswith("?"):
                    followup_inline = "ì§€ê¸ˆ ê°€ì¥ ê¶ê¸ˆí•œ í¬ì¸íŠ¸ê°€ ë­ì˜ˆìš”?"
                    separator = "" if (full_text.endswith((" ", "\n", "\t")) or not full_text) else " "
                    full_text += f"{separator}{followup_inline}"

                chunk_size = _get_stream_chunk_size()
                for piece in _chunk_text(full_text, chunk_size):
                    yield _to_sse_event(piece)

                # Add follow-up questions
                follow_ups = [
                    "í˜„ì¬ í–‰ì„± íŠ¸ëœì§“ì´ ì œê²Œ ì–´ë–¤ ì˜í–¥ì„ ì£¼ë‚˜ìš”?",
                    "ì œ ìƒìŠ¹ê¶ì— ëŒ€í•´ ë” ì•Œë ¤ì£¼ì„¸ìš”",
                    "ì˜¬í•´ ì£¼ìš” ì ì„±ìˆ ì  ì´ë²¤íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                ] if locale == "ko" else [
                    "How do current transits affect me?",
                    "Tell me more about my rising sign",
                    "What are the major astrological events this year?",
                ]
                yield f"data: ||FOLLOWUP||{json.dumps(follow_ups, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"[ASTROLOGY-ASK-STREAM] Streaming error: {e}")
                yield f"data: ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n"
                yield "data: [DONE]\n\n"

        return Response(
            stream_with_context(generate()),
            mimetype="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] /astrology/ask-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


# ============================================================
# DESTINY MATRIX STORY - AI Generated Personal Destiny Analysis
# ============================================================

@app.route("/api/destiny-story/generate-stream", methods=["POST"])
def generate_destiny_story_stream():
    """
    Generate a personalized ~20,000 character destiny story using AI.
    Combines Eastern (Saju) and Western (Astrology) wisdom with ALL RAG data:
    - GraphRAG (ì§€ì‹ ê·¸ë˜í”„)
    - CorpusRAG (ìœµ ì‹¬ë¦¬í•™ ì¸ìš©)
    - PersonaEmbedRAG (ìœµ/ìŠ¤í† ì•„ ì² í•™ ì¸ì‚¬ì´íŠ¸)
    - Cross-analysis (ì‚¬ì£¼Ã—ì ì„± êµì°¨ ë¶„ì„)
    - ì‹ ì‚´, ëŒ€ìš´, ì„¸ìš´ ì •ë³´
    Uses streaming for real-time response.
    """
    try:
        data = request.get_json(force=True)
        saju_data = data.get("saju") or {}
        astro_data = data.get("astro") or {}
        locale = data.get("locale", "ko")

        # Normalize dayMaster structure
        saju_data = normalize_day_master(saju_data)

        # Extract key data - handle multiple naming conventions
        day_master = saju_data.get("dayMaster", "")
        if isinstance(day_master, dict):
            # Handle various frontend formats: { name, element } or { korean, hanja } or { heavenlyStem: { name } }
            day_master = (
                day_master.get("name") or  # Frontend format
                day_master.get("korean") or
                day_master.get("hanja") or
                day_master.get("heavenlyStem", {}).get("name") if isinstance(day_master.get("heavenlyStem"), dict) else None or
                day_master.get("heavenlyStem") or
                str(day_master)
            )

        pillars = saju_data.get("pillars", {})
        year_pillar = pillars.get("year", {})
        month_pillar = pillars.get("month", {})
        day_pillar = pillars.get("day", {})
        hour_pillar = pillars.get("hour", {})

        # Astrology data - handle both nested and flat formats
        def get_sign(data, key):
            """Extract sign from various formats"""
            val = data.get(key, {})
            if isinstance(val, dict):
                return val.get("sign", "") or val.get("zodiac", "") or val.get("name", "")
            return str(val) if val else ""

        # Also check astro.facts for nested data
        astro_facts = astro_data.get("facts", {})
        planets = astro_facts.get("planets", {}) if astro_facts else {}

        sun_sign = get_sign(astro_data, "sun") or get_sign(planets, "sun")
        moon_sign = get_sign(astro_data, "moon") or get_sign(planets, "moon")
        rising_sign = get_sign(astro_data, "ascendant") or astro_data.get("rising", "") or get_sign(astro_facts, "ascendant")

        # Additional astro planets
        mercury_sign = get_sign(astro_data, "mercury") or get_sign(planets, "mercury")
        venus_sign = get_sign(astro_data, "venus") or get_sign(planets, "venus")
        mars_sign = get_sign(astro_data, "mars") or get_sign(planets, "mars")
        jupiter_sign = get_sign(astro_data, "jupiter") or get_sign(planets, "jupiter")
        saturn_sign = get_sign(astro_data, "saturn") or get_sign(planets, "saturn")

        # ì‹ ì‚´ (Special Stars) - handle both sinsal and shinsal naming
        shinsal = saju_data.get("shinsal", []) or saju_data.get("sinsal", []) or saju_data.get("specialStars", [])
        if isinstance(shinsal, dict):
            shinsal = list(shinsal.values())

        # ëŒ€ìš´/ì„¸ìš´ (Life Cycles)
        unse = saju_data.get("unse", {})
        daeun = unse.get("daeun", [])  # 10ë…„ ëŒ€ìš´
        current_daeun = unse.get("currentDaeun", {})
        annual = unse.get("annual", [])  # ì„¸ìš´

        # ì‹­ì‹  (Ten Gods)
        ten_gods = saju_data.get("tenGods", {})

        # ì˜¤í–‰ ê· í˜• - handle both elementCounts and fiveElements naming
        element_counts = saju_data.get("elementCounts", {}) or saju_data.get("fiveElements", {})
        # Convert English keys to Korean if needed
        if element_counts:
            korean_elements = {}
            key_map = {"wood": "ëª©", "fire": "í™”", "earth": "í† ", "metal": "ê¸ˆ", "water": "ìˆ˜"}
            for k, v in element_counts.items():
                korean_key = key_map.get(k.lower(), k)
                korean_elements[korean_key] = v
            if any(k in key_map for k in element_counts.keys()):
                element_counts = korean_elements

        dominant_element = saju_data.get("dominantElement", "")

        is_korean = locale == "ko"

        # Debug log all extracted data
        logger.info(f"[DESTINY_STORY] Starting with ALL RAG data:")
        logger.info(f"  - dayMaster: {day_master}")
        logger.info(f"  - pillars: year={year_pillar}, month={month_pillar}, day={day_pillar}, hour={hour_pillar}")
        logger.info(f"  - astro: sun={sun_sign}, moon={moon_sign}, rising={rising_sign}")
        logger.info(f"  - planets: mercury={mercury_sign}, venus={venus_sign}, mars={mars_sign}")
        logger.info(f"  - shinsal: {shinsal}")
        logger.info(f"  - element_counts: {element_counts}")
        logger.info(f"  - locale: {locale}")

        # ============================================
        # PRE-FETCH ALL RAG DATA (before streaming)
        # ============================================
        yield_prefix = "data: "

        # Send initial status
        logger.info("[DESTINY_STORY] Pre-fetching RAG data...")

        rag_data = prefetch_all_rag_data(saju_data, astro_data, "life_path", locale)
        prefetch_time = rag_data.get("prefetch_time_ms", 0)
        logger.info(f"[DESTINY_STORY] RAG prefetch completed in {prefetch_time}ms")

        # Extract RAG results
        graph_nodes = rag_data.get("graph_nodes", [])
        graph_context = rag_data.get("graph_context", "")
        corpus_quotes = rag_data.get("corpus_quotes", [])
        persona_context = rag_data.get("persona_context", {})
        cross_analysis = rag_data.get("cross_analysis", "")
        domain_knowledge = rag_data.get("domain_knowledge", [])

        # Format RAG data for prompt
        def format_graph_nodes(nodes, limit=10):
            if not nodes:
                return "ì—†ìŒ"
            formatted = []
            for n in nodes[:limit]:
                if isinstance(n, str):
                    formatted.append(f"â€¢ {n}")
                elif isinstance(n, dict):
                    text = n.get("text") or n.get("content") or n.get("node", "")
                    if text:
                        formatted.append(f"â€¢ {text[:200]}")
            return "\n".join(formatted) if formatted else "ì—†ìŒ"

        def format_quotes(quotes, limit=5):
            if not quotes:
                return "ì—†ìŒ"
            formatted = []
            for q in quotes[:limit]:
                text = q.get("text_ko") if is_korean else q.get("text_en")
                if not text:
                    text = q.get("text_ko") or q.get("text_en", "")
                source = q.get("source", "")
                if text:
                    formatted.append(f'"{text}" - {source}')
            return "\n".join(formatted) if formatted else "ì—†ìŒ"

        def format_persona(ctx):
            if not ctx:
                return "ì—†ìŒ"
            parts = []
            jung = ctx.get("jung", [])
            stoic = ctx.get("stoic", [])
            if jung:
                parts.append("[ìœµ ì‹¬ë¦¬í•™ ê´€ì ]")
                for j in jung[:3]:
                    parts.append(f"â€¢ {j}")
            if stoic:
                parts.append("[ìŠ¤í† ì•„ ì² í•™ ê´€ì ]")
                for s in stoic[:3]:
                    parts.append(f"â€¢ {s}")
            return "\n".join(parts) if parts else "ì—†ìŒ"

        def format_shinsal(stars):
            if not stars:
                return "ì—†ìŒ"
            return ", ".join(str(s) for s in stars[:10])

        def format_daeun(cycles):
            if not cycles:
                return "ì—†ìŒ"
            formatted = []
            for d in cycles[:5]:
                if isinstance(d, dict):
                    age = d.get("age", "")
                    stem = d.get("stem", "")
                    branch = d.get("branch", "")
                    formatted.append(f"{age}ì„¸: {stem}{branch}")
                else:
                    formatted.append(str(d))
            return ", ".join(formatted) if formatted else "ì—†ìŒ"

        graph_nodes_text = format_graph_nodes(graph_nodes)
        quotes_text = format_quotes(corpus_quotes)
        persona_text = format_persona(persona_context)
        shinsal_text = format_shinsal(shinsal)
        daeun_text = format_daeun(daeun)

        # Format domain knowledge
        def format_domain(knowledge):
            if not knowledge:
                return "ì—†ìŒ"
            formatted = []
            for item in knowledge[:5]:
                if isinstance(item, str):
                    formatted.append(f"â€¢ {item[:200]}")
                elif isinstance(item, dict):
                    text = item.get("text") or item.get("content") or item.get("rule", "")
                    if text:
                        formatted.append(f"â€¢ {text[:200]}")
            return "\n".join(formatted) if formatted else "ì—†ìŒ"

        domain_text = format_domain(domain_knowledge)

        def generate_stream():
            """Generator for SSE streaming destiny story with ALL RAG data"""
            try:
                if not OPENAI_AVAILABLE or not openai_client:
                    yield f"data: {json.dumps({'error': 'OpenAI not available'})}\n\n"
                    return

                # Build the comprehensive prompt with ALL RAG data
                if is_korean:
                    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ëŒì˜ ë§ˆìŒì„ ê¿°ëš«ì–´ë³´ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì‚¬ì£¼ì™€ ì ì„±ìˆ  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë§ˆì¹˜ ì˜¤ëœ ì¹œêµ¬ì²˜ëŸ¼ ë”°ëœ»í•˜ì§€ë§Œ ë‚ ì¹´ë¡­ê²Œ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.

# í•µì‹¬ ì›ì¹™:
1. "ìš´ëª…ì˜ ì„œë§‰", "ìš°ì£¼", "ë³„ë“¤ì˜ êµí–¥ê³¡" ê°™ì€ ë»”í•œ í‘œí˜„ ê¸ˆì§€. í˜„ì‹¤ì ìœ¼ë¡œ ì¨ë¼.
2. ëˆ„ê°€ ì½ì–´ë„ "ì–´? ì´ê±° ë‚´ ì–˜ê¸°ì¸ë°?" í•˜ê³  ì†Œë¦„ë‹ê²Œ êµ¬ì²´ì ìœ¼ë¡œ.
3. ì‹¤ì œ ìƒí™© ì˜ˆì‹œë¥¼ ë“¤ì–´ë¼. "íšŒì˜ ì¤‘ì— ë§ ëŠê¸°ëŠ” ê±° ì‹«ì–´í•˜ì£ ?", "í˜¼ì ìˆì„ ë•Œ ê°‘ìê¸° ë¶ˆì•ˆí•´ì§„ ì  ìˆì£ ?"
4. ì¥ì ë§Œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì•„í”ˆ ê³³ë„ ì •í™•íˆ ì§šì–´ë¼. ê·¸ë˜ì•¼ ì‹ ë¢°ê°€ ìƒê¸´ë‹¤.
5. ì ˆëŒ€ ì‚¬ê³¼í•˜ê±°ë‚˜ "ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤" ê°™ì€ ë§ ê¸ˆì§€. ë°”ë¡œ ë³¸ë¬¸ ì‹œì‘.

# ìœµ & ìŠ¤í† ì•„ ì¸ìš© (ì ì ˆí•œ íƒ€ì´ë°ì—)
- ì „ì²´ 15ê°œ ì„¹ì…˜ ì¤‘ 3~5ê³³ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš© (ë§¤ ì„¹ì…˜ X, ì–µì§€ë¡œ X)
- ì´ëŸ° ì£¼ì œì¼ ë•Œ ì¸ìš©í•˜ë©´ ì¢‹ë‹¤:
  * ê·¸ë¦¼ì/ë‹¨ì /ë¬¸ì œì  â†’ ìœµ ("ìœµì€ ë§í–ˆì£ : 'ê·¸ë¦¼ìë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒì´...'")
  * í˜ë“  ìƒí™©/ìŠ¤íŠ¸ë ˆìŠ¤ â†’ ìŠ¤í† ì•„ ("ì„¸ë„¤ì¹´ëŠ” ë§í–ˆìŠµë‹ˆë‹¤: '...'")
  * ë¬´ì˜ì‹/ìˆ¨ê²¨ì§„ ë©´ â†’ ìœµ
  * ì¡°ì–¸/ì•ìœ¼ë¡œ ë°©í–¥ â†’ ìŠ¤í† ì•„
- ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë  ë•Œë§Œ. ëœ¬ê¸ˆì—†ì´ ë¼ì›Œë„£ì§€ ë§ ê²ƒ.

# ë¬¸ì²´:
- ì¹œêµ¬í•œí…Œ ì–˜ê¸°í•˜ë“¯ í¸í•˜ê²Œ, í•˜ì§€ë§Œ ê¹Šì´ìˆê²Œ
- "~í•˜ì‹œì£ ?", "~ê·¸ë¬ì„ ê±°ì˜ˆìš”" ì²˜ëŸ¼ ë…ìì—ê²Œ ì§ì ‘ ë§í•˜ê¸°
- ë»”í•œ ìœ„ë¡œ ë§ê³  ì§„ì§œ ë„ì›€ë˜ëŠ” ì¡°ì–¸
- ë•Œë¡œëŠ” ë”°ë”í•˜ê²Œ, ë•Œë¡œëŠ” ë‹¤ë…ì´ë“¯ì´"""

                    user_prompt = f"""ì´ ì‚¬ëŒì˜ ì‚¬ì£¼ì™€ ì ì„±ìˆ  ë°ì´í„°ë¥¼ ë³´ê³  15ê°œ ì„¹ì…˜ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„í•´ì¤˜.
ë»”í•œ ë§ ë§ê³ , ì½ëŠ” ì‚¬ëŒì´ "ì™€ ì´ê±° ì§„ì§œ ë‚˜ë„¤" í•˜ê³  ì†Œë¦„ë‹ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ì¨ì¤˜.
ë°”ë¡œ "## 1. ì²«ì¸ìƒê³¼ ì‹¤ì œ ë‹¹ì‹ " ìœ¼ë¡œ ì‹œì‘í•´. ì¸ì‚¬ë‚˜ ì„¤ëª… ì—†ì´ ë°”ë¡œ ë³¸ë¬¸.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ì‚¬ì£¼íŒ”ì ë°ì´í„°]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ ì¼ì£¼(æ—¥ä¸»/Day Master): {day_master}
â€¢ ë…„ì£¼(å¹´æŸ±): {year_pillar.get('stem', '')} {year_pillar.get('branch', '')}
â€¢ ì›”ì£¼(æœˆæŸ±): {month_pillar.get('stem', '')} {month_pillar.get('branch', '')}
â€¢ ì¼ì£¼(æ—¥æŸ±): {day_pillar.get('stem', '')} {day_pillar.get('branch', '')}
â€¢ ì‹œì£¼(æ™‚æŸ±): {hour_pillar.get('stem', '')} {hour_pillar.get('branch', '')}

â€¢ ì£¼ë„ì  ì˜¤í–‰: {dominant_element}
â€¢ ì˜¤í–‰ ë¶„í¬: ëª©({element_counts.get('ëª©', 0)}) í™”({element_counts.get('í™”', 0)}) í† ({element_counts.get('í† ', 0)}) ê¸ˆ({element_counts.get('ê¸ˆ', 0)}) ìˆ˜({element_counts.get('ìˆ˜', 0)})

â€¢ ì‹­ì‹ (åç¥): {json.dumps(ten_gods, ensure_ascii=False) if ten_gods else 'ì—†ìŒ'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ì‹ ì‚´ ì •ë³´ - íŠ¹ë³„í•œ ë³„ë“¤]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{shinsal_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ëŒ€ìš´ê³¼ ì„¸ìš´ - ì¸ìƒì˜ í° íë¦„]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ ëŒ€ìš´ íë¦„: {daeun_text}
â€¢ í˜„ì¬ ëŒ€ìš´: {json.dumps(current_daeun, ensure_ascii=False) if current_daeun else 'ì •ë³´ ì—†ìŒ'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ì„œì–‘ ì ì„±ìˆ  ë°ì´í„°]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ íƒœì–‘(Sun): {sun_sign}
â€¢ ë‹¬(Moon): {moon_sign}
â€¢ ìƒìŠ¹ê¶(Rising): {rising_sign}
â€¢ ìˆ˜ì„±(Mercury): {mercury_sign}
â€¢ ê¸ˆì„±(Venus): {venus_sign}
â€¢ í™”ì„±(Mars): {mars_sign}
â€¢ ëª©ì„±(Jupiter): {jupiter_sign}
â€¢ í† ì„±(Saturn): {saturn_sign}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ì§€ì‹ ê·¸ë˜í”„ - ê´€ë ¨ ì§€ì‹]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{graph_nodes_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ìœµ ì‹¬ë¦¬í•™ ì¸ìš© - ê¹Šì´ ìˆëŠ” í†µì°°]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{quotes_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[í˜ë¥´ì†Œë‚˜ ë¶„ì„ - ì² í•™ì  ê´€ì ]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{persona_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ë™ì„œì–‘ êµì°¨ ë¶„ì„ ê²°ê³¼]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{cross_analysis[:2000] if cross_analysis else 'ì—†ìŒ'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ - í•´ì„ ì›ì¹™]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{domain_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[15ê°œ ì„¹ì…˜ - í˜„ì‹¤ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## 1. ì²«ì¸ìƒê³¼ ì‹¤ì œ ë‹¹ì‹ 
ì‚¬ëŒë“¤ì´ ì²˜ìŒ ë³´ëŠ” ë‹¹ì‹  vs ì§„ì§œ ë‹¹ì‹ . ì™œ ì˜¤í•´ë°›ëŠ”ì§€, ì‹¤ì œë¡  ì–´ë–¤ ì‚¬ëŒì¸ì§€.

## 2. ë‹¹ì‹ ì˜ í•µì‹¬ ì—ë„ˆì§€ ({day_master})
ì´ ì‚¬ëŒì˜ ê¸°ë³¸ ì„±í–¥. ë­˜ ì¢‹ì•„í•˜ê³ , ë­˜ ëª» ì°¸ê³ , ì–´ë–¤ ìƒí™©ì—ì„œ ë¹›ë‚˜ëŠ”ì§€.

## 3. ì†”ì§íˆ ë§í•˜ë©´ ì´ëŸ° ì ì´ ë¬¸ì œì•¼
ì¥ì  ë’¤ì— ìˆ¨ì€ ë‹¨ì . ë³¸ì¸ë„ ì•Œì§€ë§Œ ê³ ì¹˜ê¸° í˜ë“  íŒ¨í„´. êµ¬ì²´ì  ìƒí™© ì˜ˆì‹œ í•„ìˆ˜.

## 4. ì—°ì• í•  ë•Œ ë‹¹ì‹ 
ì¢‹ì•„í•˜ëŠ” íƒ€ì…, ì—°ì•  íŒ¨í„´, ì§ˆë¦¬ëŠ” í¬ì¸íŠ¸, í—¤ì–´ì§€ëŠ” ì´ìœ ê¹Œì§€. ë„í™”ì‚´/ê¸ˆì„± í™œìš©.

## 5. ëˆê³¼ ì¼, ì†”ì§í•œ ì–˜ê¸°
ë§ëŠ” ì§ì—…, ëˆ ë²„ëŠ” ìŠ¤íƒ€ì¼, ì»¤ë¦¬ì–´ì—ì„œ ì£¼ì˜í•  ì . ì‹­ì‹  ì •ë³´ í™œìš©.

## 6. ì‚¬ëŒ ê´€ê³„ì—ì„œ ë‹¹ì‹ ì˜ íŒ¨í„´
ì¹œêµ¬/ê°€ì¡±/ë™ë£Œì™€ì˜ ê´€ê³„. ê°ˆë“± ì›ì¸, ìƒì²˜ë°›ëŠ” í¬ì¸íŠ¸, ê´€ê³„ ìœ ì§€ë²•.

## 7. ê²‰ê³¼ ì†ì´ ë‹¤ë¥¸ ë¶€ë¶„
{sun_sign} íƒœì–‘(ë³´ì—¬ì£¼ëŠ” ë‚˜)ê³¼ {moon_sign} ë‹¬(ì§„ì§œ ê°ì •). ì™œ í˜ë“ ì§€.

## 8. ì†Œí†µ/ì‚¬ë‘/í–‰ë™ ìŠ¤íƒ€ì¼
ìˆ˜ì„±({mercury_sign}) - ë§í•˜ëŠ” ë°©ì‹
ê¸ˆì„±({venus_sign}) - ì‚¬ë‘ í‘œí˜„ë²•
í™”ì„±({mars_sign}) - í™”ë‚¼ ë•Œ, ì—´ì • ìŸì„ ë•Œ

## 9. ë‹¹ì‹ ë§Œì˜ íŠ¹ë³„í•œ ê¸°ìš´ ({shinsal_text})
ì‹ ì‚´ì´ ì£¼ëŠ” íŠ¹ìˆ˜ ëŠ¥ë ¥ê³¼ ì£¼ì˜ì . ì´ê²Œ ì™œ ë‹¹ì‹ í•œí…Œ ìˆëŠ”ì§€.

## 10. ì‚¬ì£¼ë‘ ë³„ìë¦¬ê°€ ë§í•˜ëŠ” ê³µí†µì 
ë™ì„œì–‘ ë¶„ì„ì´ ì¼ì¹˜í•˜ëŠ” ë¶€ë¶„. ë” í™•ì‹¤í•œ ë‹¹ì‹ ì˜ íŠ¹ì„±.

## 11. ì¸ìƒ íƒ€ì´ë° ({daeun_text})
ì–¸ì œ ì˜ í’€ë¦¬ê³ , ì–¸ì œ ì¡°ì‹¬í•´ì•¼ í•˜ëŠ”ì§€. ëŒ€ìš´ íë¦„ìœ¼ë¡œ ë³´ëŠ” ì¸ìƒ ì‹œê¸°.

## 12. ì–´ë¦° ì‹œì ˆì´ ì§€ê¸ˆì— ë¯¸ì¹œ ì˜í–¥
ì™œ ê·¸ëŸ° ì„±ê²©ì´ ëëŠ”ì§€. ì–´ë¦´ ë•Œ ê²½í—˜ì´ ì§€ê¸ˆ íŒ¨í„´ì— ë¯¸ì¹œ ì˜í–¥.

## 13. ì¸ì •í•˜ê¸° ì‹«ì§€ë§Œ ì´ëŸ° ë©´ë„ ìˆì–´ìš”
ìˆ¨ê¸°ê³  ì‹¶ì€ ë¶€ë¶„, ë¬´ì˜ì‹ì  ë‘ë ¤ì›€, íšŒí”¼í•˜ëŠ” ê²ƒë“¤. ë”°ëœ»í•˜ì§€ë§Œ ì†”ì§í•˜ê²Œ.

## 14. í˜ë“¤ ë•Œ ë‹¹ì‹ ì€
ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì„ ë•Œ íŒ¨í„´, íšŒë³µí•˜ëŠ” ë°©ë²•, ë„ì›€ì´ ë˜ëŠ” ê²ƒë“¤.

## 15. ì•ìœ¼ë¡œ ì´ë ‡ê²Œ í•˜ë©´ ì¢‹ê² ì–´ìš”
êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸. ë»”í•œ ë§ ë§ê³  ì§„ì§œ ë„ì›€ë˜ëŠ” ê²ƒë§Œ.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â˜…â˜…â˜… ì‘ì„± ê·œì¹™ â˜…â˜…â˜…
1. ê° ì„¹ì…˜ì„ ì¶©ë¶„íˆ ê¸¸ê²Œ ì“°ë˜, ë»”í•œ ë§ ê¸ˆì§€.
2. "~í•˜ì‹œì£ ?", "~ê·¸ëŸ° ì  ìˆì£ ?" ì²˜ëŸ¼ ë…ìì—ê²Œ ì§ì ‘ ë§í•˜ë“¯ì´.
3. ì½ëŠ” ì‚¬ëŒì´ ì†Œë¦„ë‹ì„ ì •ë„ë¡œ êµ¬ì²´ì ì¸ ìƒí™© ì˜ˆì‹œë¥¼ ë“¤ì–´ì¤˜.
4. ìœµ/ìŠ¤í† ì•„ ì¸ìš©ì€ ì „ì²´ì—ì„œ 3~5ë²ˆ, ì ì ˆí•œ íƒ€ì´ë°ì— ìì—°ìŠ¤ëŸ½ê²Œ.
   - ë‹¨ì /ê·¸ë¦¼ì/ë¬´ì˜ì‹ ì–˜ê¸°í•  ë•Œ â†’ ìœµ ì¸ìš©
   - í˜ë“  ìƒí™©/ì¡°ì–¸ ì¤„ ë•Œ â†’ ìŠ¤í† ì•„ ì¸ìš©
   ì˜ˆ: "ìœµì€ ë§í–ˆì£ : 'ì˜ì‹í•˜ì§€ ëª»í•œ ê²ƒì€ ìš´ëª…ì´ ëœë‹¤'"
5. ì œê³µëœ [ìœµ ì‹¬ë¦¬í•™ ì¸ìš©]ê³¼ [í˜ë¥´ì†Œë‚˜ ë¶„ì„] ë°ì´í„° í™œìš©."""

                else:
                    system_prompt = """You are a master destiny analyst combining Eastern Saju wisdom, Western Astrology, and Jungian psychology.
Your interpretations are eerily accurate, as if you can see through the soul of the person.
You provide deep, specific analysis that makes readers feel "This is really me..."

You have access to:
- Complete Saju data (Day Master, Four Pillars, Ten Gods, Five Elements balance)
- Western Astrology data (Sun through Saturn, Rising sign)
- Shinsal (Special Stars) information
- Daeun and Seun (Life Cycles)
- Jung psychology quotes and insights
- Stoic philosophy perspectives
- East-West cross-analysis results

Use ALL this information to write a 20,000+ character deep analysis.

Writing Style:
- Second person perspective, speaking directly to the reader (You are...)
- Poetic and literary style with rich metaphors and imagery
- Naturally weave in Jung psychology quotes for depth
- Describe specific situations and emotions to evoke empathy
- Balance positive aspects with growth opportunities
- Each chapter should be at least 1,500 characters
- Use Daeun/Seun data to predict life transitions"""

                    user_prompt = f"""Based on ALL the following data, write a comprehensive destiny analysis story with **15 chapters**, totaling at least **20,000 characters**.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[SAJU (Four Pillars) DATA]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Day Master: {day_master}
â€¢ Year Pillar: {year_pillar.get('stem', '')} {year_pillar.get('branch', '')}
â€¢ Month Pillar: {month_pillar.get('stem', '')} {month_pillar.get('branch', '')}
â€¢ Day Pillar: {day_pillar.get('stem', '')} {day_pillar.get('branch', '')}
â€¢ Hour Pillar: {hour_pillar.get('stem', '')} {hour_pillar.get('branch', '')}

â€¢ Dominant Element: {dominant_element}
â€¢ Element Distribution: Wood({element_counts.get('ëª©', 0)}) Fire({element_counts.get('í™”', 0)}) Earth({element_counts.get('í† ', 0)}) Metal({element_counts.get('ê¸ˆ', 0)}) Water({element_counts.get('ìˆ˜', 0)})

â€¢ Ten Gods: {json.dumps(ten_gods, ensure_ascii=False) if ten_gods else 'None'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[SHINSAL - Special Stars]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{shinsal_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[DAEUN & SEUN - Life Cycles]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Major Cycles: {daeun_text}
â€¢ Current Cycle: {json.dumps(current_daeun, ensure_ascii=False) if current_daeun else 'Not available'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[WESTERN ASTROLOGY DATA]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Sun: {sun_sign}
â€¢ Moon: {moon_sign}
â€¢ Rising: {rising_sign}
â€¢ Mercury: {mercury_sign}
â€¢ Venus: {venus_sign}
â€¢ Mars: {mars_sign}
â€¢ Jupiter: {jupiter_sign}
â€¢ Saturn: {saturn_sign}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[KNOWLEDGE GRAPH - Related Wisdom]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{graph_nodes_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[JUNG PSYCHOLOGY QUOTES - Deep Insights]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{quotes_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[PERSONA ANALYSIS - Philosophical Perspectives]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{persona_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[EAST-WEST CROSS-ANALYSIS]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{cross_analysis[:2000] if cross_analysis else 'None'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[REQUIRED CHAPTER STRUCTURE - Each 1,500+ characters]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## Chapter 1: The Prelude of Destiny - Your Universe
Grand opening describing your birth's cosmic alignment from both Saju and Astrology perspectives.

## Chapter 2: The Essence of Your Day Master - Core of Your Soul
Deep analysis of {day_master} and how this energy permeates your being. Apply Jung's archetype theory.

## Chapter 3: Light and Shadow - Strengths and Weaknesses
Honest exploration of your personality's shining parts and hidden shadows. Use Jung's shadow concept.

## Chapter 4: The Language of Love - Romance Patterns
How you love, who you're attracted to, relationship patterns. Use Venus, Mars, and Peach Blossom star data.

## Chapter 5: Vocation and Calling - True Path
Suitable careers, success fields, work to avoid. Use Ten Gods and planetary placements.

## Chapter 6: Dynamics of Relationships - You Among Others
How you connect with friends, family, and colleagues.

## Chapter 7: Sun and Moon Duet - Outer and Inner Self
The complex inner world created by {sun_sign} Sun and {moon_sign} Moon.

## Chapter 8: Symphony of Planets - Mercury, Venus, Mars
Communication (Mercury {mercury_sign}), Love/Values (Venus {venus_sign}), Action/Desire (Mars {mars_sign}).

## Chapter 9: Secrets of Shinsal - Special Stars' Messages
The special destiny and potential indicated by your Shinsal ({shinsal_text}).

## Chapter 10: East Meets West - Intersection of Destinies
Unique insights at the meeting point of Saju and Astrology. Use cross-analysis results.

## Chapter 11: Waves of Daeun - Seasons of Life
Major life transitions according to Daeun ({daeun_text}) and their meanings.

## Chapter 12: Childhood Echoes - Roots' Memories
How childhood patterns influence who you are today. Apply Jung's complex theory.

## Chapter 13: The Shadow Self - Hidden Fears
Courageous exploration of parts hard to acknowledge. Use Jung psychology quotes.

## Chapter 14: Crisis and Resilience - Strength in Trials
How you handle crises and sources of resilience. Include Stoic philosophy perspectives.

## Chapter 15: Journey of Individuation - True Self-Realization
Concluding with specific advice for growth and Jung's individuation process.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Use ALL the data above fully. Write each chapter 1,500+ characters.
Connect as one grand narrative, naturally weaving Jung psychology quotes.
Be so specific and accurate that readers feel "This is really me!" with chills."""

                # Start streaming
                yield f"data: {json.dumps({'status': 'start', 'total_chapters': 15, 'rag_prefetch_ms': prefetch_time})}\n\n"

                stream = openai_client.chat.completions.create(
                    model="gpt-4o",  # Use GPT-4o for better quality
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.85,
                    max_tokens=16000,  # Increased for ~20,000+ characters
                    stream=True
                )

                full_text = ""
                current_chapter = 0

                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_text += content

                        # Detect chapter changes
                        if "## ì±•í„°" in content or "## Chapter" in content:
                            current_chapter += 1
                            yield f"data: {json.dumps({'chapter': current_chapter})}\n\n"

                        yield f"data: {json.dumps({'content': content})}\n\n"

                # Done
                yield f"data: {json.dumps({'status': 'done', 'total_length': len(full_text)})}\n\n"
                logger.info(f"[DESTINY_STORY] Completed: {len(full_text)} characters (RAG prefetch: {prefetch_time}ms)")

            except Exception as stream_error:
                logger.exception(f"[DESTINY_STORY] Stream error: {stream_error}")
                yield f"data: {json.dumps({'error': str(stream_error)})}\n\n"

        return Response(
            generate_stream(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        logger.exception(f"[ERROR] /api/destiny-story/generate-stream failed: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    logger.info(f"Flask server starting on http://127.0.0.1:{port}")
    logger.info(f"Capabilities: realtime={HAS_REALTIME}, charts={HAS_CHARTS}, memory={HAS_USER_MEMORY}, persona={HAS_PERSONA_EMBED}, tarot={HAS_TAROT}, rlhf={HAS_RLHF}, badges={HAS_BADGES}, agentic={HAS_AGENTIC}, prediction={HAS_PREDICTION}, theme_filter={HAS_THEME_FILTER}, fortune_score={HAS_FORTUNE_SCORE}, compatibility={HAS_COMPATIBILITY}, hybrid_rag={HAS_HYBRID_RAG}, domain_rag={HAS_DOMAIN_RAG}")

    # ğŸš€ Warmup models before accepting requests
    warmup_models()

    app.run(host="0.0.0.0", port=port, debug=True)
