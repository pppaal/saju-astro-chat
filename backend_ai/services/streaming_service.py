"""
Streaming Service

Business logic for streaming AI responses with Server-Sent Events (SSE).
Handles fortune telling, dream interpretation, and counseling with real-time streaming.

Moved from app.py to separate business logic from routing.
"""
import logging
import os
import time
from datetime import datetime
from typing import Dict, Any, Optional, Generator, List
from flask import Response, stream_with_context, g

logger = logging.getLogger(__name__)


class StreamingService:
    """
    Streaming service for AI-powered fortune telling with SSE.

    Methods:
        stream_fortune: Streaming fortune calculation with RAG and context
    """

    def __init__(self):
        """Initialize Streaming Service."""
        pass

    def stream_fortune(
        self,
        saju_data: Dict[str, Any],
        astro_data: Dict[str, Any],
        advanced_astro: Dict[str, Any],
        birth_data: Dict[str, Any],
        theme: str = "chat",
        locale: str = "ko",
        prompt: str = "",
        session_id: Optional[str] = None,
        conversation_history: List[Dict[str, Any]] = None,
        user_context: Dict[str, Any] = None,
        cv_text: str = "",
        request_id: Optional[str] = None
    ) -> Response:
        """
        Stream fortune telling with SSE (Server-Sent Events).

        This is the core business logic moved from app.py ask_stream() function.
        Logic remains 100% identical to ensure compatibility.

        Args:
            saju_data: Saju birth chart data
            astro_data: Astrology birth chart data
            advanced_astro: Advanced astrology features
            birth_data: Normalized birth data
            theme: Fortune theme (chat, career, love, etc.)
            locale: Language locale (en, ko)
            prompt: User's question/prompt (may be structured from frontend)
            session_id: Optional session ID for pre-fetched RAG data
            conversation_history: Previous messages for context
            user_context: Premium user context (persona, sessions, personality type)
            cv_text: CV/Resume text for career consultations
            request_id: Optional request ID for logging

        Returns:
            Flask Response with SSE stream (text/event-stream)
        """
        try:
            # Import dependencies (lazy loaded in app.py)
            from backend_ai.app.sanitizer import (
                is_suspicious_input,
                sanitize_user_input,
                sanitize_messages,
            )
            from backend_ai.app.app import (
                normalize_day_master,
                _has_saju_payload,
                _has_astro_payload,
                _calculate_simple_saju,
                validate_birth_data,
                _build_missing_payload_message,
                _build_birth_format_message,
                _sse_error_response,
                _build_detailed_saju,
                _build_detailed_astro,
                _build_advanced_astro_context,
                get_cross_analysis_for_chart,
                get_session_rag_cache,
                get_lifespan_guidance,
                get_theme_fusion_rules,
                get_active_imagination_prompts,
                _is_truthy,
                _bool_env,
                _get_int_env,
                _add_months,
                _format_date_ymd,
                _ensure_ko_prefix,
                _build_missing_requirements_addendum,
                _insert_addendum,
                _build_rag_debug_addendum,
                _format_korean_spacing,
                _to_sse_event,
                _chunk_text,
                _get_stream_chunk_size,
                _select_model_and_temperature,
                _clamp_temperature,
                _coerce_float,
            )

            # Crisis detection (if available)
            HAS_COUNSELING = False
            CrisisDetector = None
            try:
                from backend_ai.counseling.crisis import CrisisDetector
                HAS_COUNSELING = True
            except ImportError:
                pass

            # Corpus RAG (Jung psychology quotes)
            HAS_CORPUS_RAG = False
            get_corpus_rag = None
            try:
                from backend_ai.app.app import get_corpus_rag
                HAS_CORPUS_RAG = True
            except ImportError:
                pass

            if conversation_history is None:
                conversation_history = []
            if user_context is None:
                user_context = {}

            logger.info(f"[StreamingService] request_id={request_id} theme={theme} locale={locale} session={session_id or 'none'} history_len={len(conversation_history)} has_user_ctx={bool(user_context)} cv_len={len(cv_text)}")

            # Input validation - check for suspicious patterns
            if is_suspicious_input(prompt):
                logger.warning(f"[StreamingService] Suspicious input detected: {prompt[:100]}...")

            # Detect if frontend already sent a fully structured prompt
            is_frontend_structured = (
                "ÎãπÏã†ÏùÄ Îî∞ÎúªÌïòÍ≥† Ï†ÑÎ¨∏Ï†ÅÏù∏ Ïö¥Î™Ö ÏÉÅÎã¥ÏÇ¨" in prompt or
                "You are a warm, professional destiny counselor" in prompt or
                "[ÏÇ¨Ï£º/Ï†êÏÑ± Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞]" in prompt or
                "‚òÖ‚òÖ‚òÖ ÌïµÏã¨ Í∑úÏπô ‚òÖ‚òÖ‚òÖ" in prompt
            )

            # Sanitize prompt
            sanitized_prompt = sanitize_user_input(prompt, max_length=8000 if is_frontend_structured else 1500, allow_newlines=True)

            # Debug flags
            debug_rag = _is_truthy(os.getenv("RAG_DEBUG_RESPONSE", ""))
            debug_log = _is_truthy(os.getenv("RAG_DEBUG_LOG", "")) or debug_rag

            # Extract current user question
            current_user_question = ""
            if "ÏßàÎ¨∏:" in sanitized_prompt:
                current_user_question = sanitized_prompt.split("ÏßàÎ¨∏:")[-1].strip()[:500]
            elif "Q:" in sanitized_prompt:
                current_user_question = sanitized_prompt.split("Q:")[-1].strip()[:500]
            else:
                current_user_question = sanitized_prompt[-500:] if sanitized_prompt else ""

            if is_frontend_structured:
                logger.info(f"[StreamingService] Detected STRUCTURED frontend prompt (len={len(prompt)})")

            # Normalize dayMaster structure (nested -> flat)
            saju_data = normalize_day_master(saju_data)

            logger.info(f"[StreamingService] saju dayMaster: {saju_data.get('dayMaster', {})}")

            # Check for pre-fetched RAG data from session
            session_cache = None
            session_rag_data = {}
            persona_context = {}
            rag_context = ""

            if session_id:
                session_cache = get_session_rag_cache(session_id)
                if session_cache:
                    logger.info(f"[StreamingService] Using pre-fetched session data for {session_id}")
                    # Use cached saju/astro if not provided in request
                    if not saju_data:
                        saju_data = session_cache.get("saju_data", {})
                    if not astro_data:
                        astro_data = session_cache.get("astro_data", {})

                    # Build rich RAG context from pre-fetched data
                    rag_data = session_cache.get("rag_data", {})
                    session_rag_data = rag_data

                    # GraphRAG context
                    if rag_data.get("graph_nodes"):
                        rag_context += "\n[üìä Í¥ÄÎ†® ÏßÄÏãù Í∑∏ÎûòÌîÑ]\n"
                        rag_context += "\n".join(rag_data["graph_nodes"][:8])

                    # Jung quotes
                    if rag_data.get("corpus_quotes"):
                        rag_context += "\n\n[üìö Í¥ÄÎ†® Ïúµ Ïã¨Î¶¨Ìïô Ïù∏Ïö©]\n"
                        for q in rag_data["corpus_quotes"][:3]:
                            rag_context += f"‚Ä¢ {q.get('text_ko', q.get('text_en', ''))} ({q.get('source', '')})\n"

                    # Persona insights
                    persona_context = rag_data.get("persona_context", {})
                    if persona_context.get("jung"):
                        rag_context += "\n[üß† Î∂ÑÏÑùÍ∞Ä Í¥ÄÏ†ê]\n"
                        rag_context += "\n".join(f"‚Ä¢ {i}" for i in persona_context["jung"][:3])
                    if persona_context.get("stoic"):
                        rag_context += "\n\n[‚öîÔ∏è Ïä§ÌÜ†ÏïÑ Ï≤†Ìïô Í¥ÄÏ†ê]\n"
                        rag_context += "\n".join(f"‚Ä¢ {i}" for i in persona_context["stoic"][:3])

                    logger.info(f"[StreamingService] RAG context from session: {len(rag_context)} chars")
                else:
                    logger.warning(f"[StreamingService] Session {session_id} not found or expired")

            # Allow birth-only computation if enabled
            allow_birth_compute = _bool_env("ALLOW_BIRTH_ONLY")
            if allow_birth_compute and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
                try:
                    saju_data = _calculate_simple_saju(
                        birth_data["date"],
                        birth_data["time"],
                    )
                    saju_data = normalize_day_master(saju_data)
                    logger.info(f"[StreamingService] Computed simple saju from birth: {saju_data.get('dayMaster', {})}")
                except Exception as e:
                    logger.warning(f"[StreamingService] Failed to compute simple saju: {e}")

            # Validate required payloads
            has_saju_payload = _has_saju_payload(saju_data)
            has_astro_payload = _has_astro_payload(astro_data)
            require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))

            if require_computed_payload and (not has_saju_payload or not has_astro_payload):
                if birth_data.get("date") or birth_data.get("time"):
                    valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                    if not valid_birth:
                        logger.warning("[StreamingService] Invalid birth format for missing payload")
                        return _sse_error_response(_build_birth_format_message(locale))

                missing_message = _build_missing_payload_message(
                    locale,
                    missing_saju=not has_saju_payload,
                    missing_astro=not has_astro_payload,
                )
                logger.warning("[StreamingService] Missing computed payload(s)")
                return _sse_error_response(missing_message)

            # Build DETAILED chart context (not just summary)
            saju_detail = _build_detailed_saju(saju_data)
            astro_detail = _build_detailed_astro(astro_data)
            advanced_astro_detail = _build_advanced_astro_context(advanced_astro)

            logger.info(f"[StreamingService] saju_detail length: {len(saju_detail)}")
            logger.info(f"[StreamingService] astro_detail length: {len(astro_detail)}")
            if advanced_astro_detail:
                logger.info(f"[StreamingService] advanced_astro_detail length: {len(advanced_astro_detail)}")

            # Get cross-analysis (from session or instant lookup)
            cross_rules = ""
            if session_cache and session_cache.get("rag_data", {}).get("cross_analysis"):
                cross_rules = session_cache["rag_data"]["cross_analysis"]
            else:
                try:
                    cross_rules = get_cross_analysis_for_chart(saju_data, astro_data, theme, locale)
                    if cross_rules:
                        logger.info(f"[StreamingService] Instant cross-analysis: {len(cross_rules)} chars, theme={theme}")
                except Exception as e:
                    logger.warning(f"[StreamingService] Cross-analysis lookup failed: {e}")

            # Get Jung/Stoic insights if not from session (instant lookup)
            instant_quotes = []
            if not rag_context and HAS_CORPUS_RAG and get_corpus_rag:
                try:
                    _corpus_rag_inst = get_corpus_rag()
                    if _corpus_rag_inst:
                        # Build query from user context
                        theme_concepts = {
                            "career": "vocation calling purpose ÏÜåÎ™Ö ÏßÅÏóÖ ÏûêÏïÑÏã§ÌòÑ",
                            "love": "anima animus relationship Í¥ÄÍ≥Ñ ÏÇ¨Îûë Í∑∏Î¶ºÏûê",
                            "health": "healing wholeness ÏπòÏú† ÌÜµÌï©",
                            "life_path": "individuation meaning Í∞úÏÑ±Ìôî ÏùòÎØ∏ ÏÑ±Ïû•",
                            "family": "complex archetype ÏΩ§ÌîåÎ†âÏä§ ÏõêÌòï Í∞ÄÏ°±",
                        }
                        jung_query = f"{theme_concepts.get(theme, theme)} {sanitized_prompt[:50] if sanitized_prompt else ''}"
                        quotes = _corpus_rag_inst.search(jung_query, top_k=3, min_score=0.15)
                        if quotes:
                            instant_quotes = quotes
                            rag_context += "\n\n[üìö Ïúµ Ïã¨Î¶¨Ìïô ÌÜµÏ∞∞]\n"
                            for q in quotes[:2]:
                                quote_text = q.get('quote_kr') or q.get('quote_en', '')
                                if quote_text:
                                    rag_context += f"‚Ä¢ \"{quote_text[:150]}...\" ‚Äî Ïπº Ïúµ, {q.get('source', '')}\n"
                            logger.info(f"[StreamingService] Instant Jung quotes: {len(quotes)} found")
                except Exception as e:
                    logger.debug(f"[StreamingService] Instant Jung quotes failed: {e}")

            # Build cross-analysis section
            cross_section = ""
            if cross_rules:
                cross_section = f"\n[ÏÇ¨Ï£º+Ï†êÏÑ± ÍµêÏ∞® Ìï¥ÏÑù Í∑úÏπô]\n{cross_rules}\n"

            # Current date for time-relevant advice
            now = datetime.now()
            today_date = now.date()
            six_month_date = _add_months(today_date, 6)
            weekdays_ko = ["ÏõîÏöîÏùº", "ÌôîÏöîÏùº", "ÏàòÏöîÏùº", "Î™©ÏöîÏùº", "Í∏àÏöîÏùº", "ÌÜ†ÏöîÏùº", "ÏùºÏöîÏùº"]
            current_date_str = f"Ïò§Îäò: {now.year}ÎÖÑ {now.month}Ïõî {now.day}Ïùº ({weekdays_ko[now.weekday()]})"
            timing_window_str = (
                f"ÌÉÄÏù¥Î∞ç Í∏∞Ï§Ä: {_format_date_ymd(today_date)} ~ {_format_date_ymd(six_month_date)}"
                if locale == "ko"
                else f"Timing window: {_format_date_ymd(today_date)} to {_format_date_ymd(six_month_date)}"
            )

            # Build user context section for returning users (premium feature)
            user_context_section = self._build_user_context_section(user_context, locale)

            # Build CV/Resume section
            cv_section = ""
            if cv_text:
                cv_section = f"""
[üìÑ ÏÇ¨Ïö©Ïûê Ïù¥Î†•ÏÑú/CV]
{cv_text}

‚Üí ÏúÑ Ïù¥Î†•ÏÑú ÎÇ¥Ïö©ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏùò Í≤ΩÎ†•, Í∏∞Ïà†, Í≤ΩÌóòÏóê ÎßûÎäî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï°∞Ïñ∏ÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.
‚Üí ÏÇ¨Ï£º/Ï†êÏÑ± Ìï¥ÏÑùÍ≥º Ïù¥Î†•ÏÑú ÎÇ¥Ïö©ÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Í∞úÏù∏ÌôîÎêú Ï°∞Ïñ∏ÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.
‚Üí Ïª§Î¶¨Ïñ¥, ÏßÅÏóÖ, Ï†ÅÏÑ± Í¥ÄÎ†® ÏßàÎ¨∏ÏóêÎäî Ïù¥Î†•ÏÑú Ï†ïÎ≥¥Î•º Ï†ÅÍ∑π ÌôúÏö©ÌïòÏÑ∏Ïöî.
"""
                logger.info(f"[StreamingService] CV section added: {len(cv_text)} chars, theme={theme}")

            # Lifespan guidance
            lifespan_section = self._build_lifespan_section(birth_data, saju_data)

            # Theme fusion rules
            theme_fusion_section = self._build_theme_fusion_section(saju_data, astro_data, theme, locale, birth_data)

            # Active imagination prompts
            imagination_section = self._build_imagination_section(sanitized_prompt)

            # Crisis detection
            crisis_response = None
            crisis_check = {"is_crisis": False, "max_severity": "none", "requires_immediate_action": False}
            if HAS_COUNSELING and CrisisDetector and current_user_question:
                crisis_check = CrisisDetector.detect_crisis(current_user_question)
                if crisis_check["is_crisis"]:
                    logger.warning(f"[StreamingService] Crisis detected! severity={crisis_check['max_severity']}")
                    crisis_response = CrisisDetector.get_crisis_response(
                        crisis_check["max_severity"],
                        locale=locale
                    )
                    if crisis_check["requires_immediate_action"]:
                        # Return safety response immediately via SSE
                        def crisis_generator():
                            msg = crisis_response.get("immediate_message", "")
                            if crisis_response.get("follow_up"):
                                msg += "\n\n" + crisis_response["follow_up"]
                            if crisis_response.get("closing"):
                                msg += "\n\n" + crisis_response["closing"]
                            yield f"data: {msg}\n\n"
                            yield "data: [DONE]\n\n"

                        return Response(
                            stream_with_context(crisis_generator()),
                            mimetype="text/event-stream",
                            headers={
                                "Cache-Control": "no-cache",
                                "Connection": "keep-alive",
                                "X-Accel-Buffering": "no",
                            }
                        )

            # Build crisis context for medium/medium_high severity
            crisis_context_section = self._build_crisis_context(crisis_response, crisis_check)

            # Build therapeutic context based on question type
            therapeutic_section = self._build_therapeutic_section(sanitized_prompt, locale, HAS_COUNSELING)

            # RAG metadata for debugging
            rag_meta = {}
            if debug_rag or debug_log:
                rag_meta = {
                    "enabled": True,
                    "theme": theme,
                    "question": current_user_question[:120],
                    "graph_nodes": len(session_rag_data.get("graph_nodes", [])),
                    "corpus_quotes": len(session_rag_data.get("corpus_quotes", [])) or len(instant_quotes),
                    "persona_jung": len(persona_context.get("jung", [])),
                    "persona_stoic": len(persona_context.get("stoic", [])),
                    "cross_analysis": bool(cross_rules),
                    "theme_fusion": bool(theme_fusion_section),
                    "lifespan": bool(lifespan_section),
                    "therapeutic": bool(therapeutic_section),
                    "session_rag": bool(session_cache),
                }
                if debug_log:
                    logger.info(
                        "[RAG-DEBUG] theme=%s q=%s graph=%s corpus=%s persona=%s cross=%s fusion=%s session=%s",
                        theme,
                        current_user_question[:80],
                        rag_meta["graph_nodes"],
                        rag_meta["corpus_quotes"],
                        rag_meta["persona_jung"] + rag_meta["persona_stoic"],
                        rag_meta["cross_analysis"],
                        rag_meta["theme_fusion"],
                        rag_meta["session_rag"],
                    )

            # Build system prompt
            system_prompt = self._build_system_prompt(
                is_frontend_structured=is_frontend_structured,
                locale=locale,
                theme=theme,
                timing_window_str=timing_window_str,
                current_date_str=current_date_str,
                rag_context=rag_context,
                cross_rules=cross_rules,
                saju_detail=saju_detail,
                astro_detail=astro_detail,
                advanced_astro_detail=advanced_astro_detail,
                cross_section=cross_section,
                user_context_section=user_context_section,
                cv_section=cv_section,
                lifespan_section=lifespan_section,
                theme_fusion_section=theme_fusion_section,
                imagination_section=imagination_section,
                crisis_context_section=crisis_context_section,
                therapeutic_section=therapeutic_section,
                sanitized_prompt=sanitized_prompt,
            )

            # Emotion tracking
            emotion_context = self._build_emotion_context(sanitized_prompt)
            if emotion_context:
                system_prompt = system_prompt.replace("[üìè ÏùëÎãµ Íµ¨Ï°∞]", f"{emotion_context}\n[üìè ÏùëÎãµ Íµ¨Ï°∞]")

            # Generate streaming response
            def generate():
                """SSE generator for streaming response."""
                try:
                    from openai import OpenAI
                    import httpx
                    client = OpenAI(
                        api_key=os.getenv("OPENAI_API_KEY"),
                        timeout=httpx.Timeout(60.0, connect=10.0)
                    )

                    # Build messages with conversation history
                    messages = [{"role": "system", "content": system_prompt}]

                    # Add conversation history - increased limit for better context
                    history_limit = 12  # 6 user + 6 assistant messages
                    recent_history = conversation_history[-history_limit:] if conversation_history else []

                    # Generate conversation summary for long sessions (>6 messages)
                    conversation_summary = ""
                    if len(conversation_history) > 6:
                        # Extract key topics from older messages
                        older_msgs = conversation_history[:-6]
                        topics = []
                        for m in older_msgs:
                            if m.get("role") == "user" and m.get("content"):
                                content = m["content"][:100]
                                if any(k in content for k in ["Ïó∞Ïï†", "ÏÇ¨Îûë", "Í≤∞Ìòº"]):
                                    topics.append("Ïó∞Ïï†/Í¥ÄÍ≥Ñ")
                                elif any(k in content for k in ["Ï∑®ÏóÖ", "Ïù¥ÏßÅ", "Ïª§Î¶¨Ïñ¥", "ÏßÑÎ°ú"]):
                                    topics.append("Ïª§Î¶¨Ïñ¥/ÏßÑÎ°ú")
                                elif any(k in content for k in ["ÌûòÎì§", "Ïö∞Ïö∏", "ÏßÄÏ≥ê"]):
                                    topics.append("Í∞êÏ†ïÏ†Å Ïñ¥Î†§ÏõÄ")
                                elif any(k in content for k in ["ÎÇòÎäî", "ÏÑ±Í≤©", "Ïñ¥Îñ§ ÏÇ¨Îûå"]):
                                    topics.append("ÏûêÍ∏∞ÌÉêÏÉâ")
                        if topics:
                            unique_topics = list(dict.fromkeys(topics))[:3]
                            conversation_summary = f"[üìã Ïù¥Ï†Ñ ÎåÄÌôî ÏöîÏïΩ: {', '.join(unique_topics)} Ï£ºÏ†úÎ°ú ÎåÄÌôîÌï®]\n"

                    # Add summary if available
                    if conversation_summary:
                        messages.append({
                            "role": "system",
                            "content": conversation_summary
                        })

                    # Smart truncation: recent messages get more space
                    for idx, msg in enumerate(recent_history):
                        if msg.get("role") in ("user", "assistant") and msg.get("content"):
                            # Older messages: shorter, Recent messages: longer
                            is_recent = idx >= len(recent_history) - 4
                            max_len = 800 if is_recent else 300
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"][:max_len]
                            })

                    # Add current user message
                    messages.append({"role": "user", "content": sanitized_prompt})

                    # Model selection
                    default_model = os.getenv("CHAT_MODEL") or os.getenv("FUSION_MODEL") or "gpt-4.1"
                    default_temp = _clamp_temperature(_coerce_float(os.getenv("CHAT_TEMPERATURE")), 0.75)
                    model_name, temperature, ab_variant = _select_model_and_temperature(
                        {"session_id": session_id},
                        default_model,
                        default_temp,
                        session_id,
                        request_id,
                    )

                    if debug_rag or debug_log:
                        rag_meta["model"] = model_name
                        rag_meta["temperature"] = temperature
                        rag_meta["ab_variant"] = ab_variant or ""

                    max_tokens = _get_int_env("ASK_STREAM_MAX_TOKENS", 1600, min_value=400, max_value=4000)

                    # Stream from OpenAI
                    stream = client.chat.completions.create(
                        model=model_name,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        stream=True
                    )

                    full_text = ""

                    for chunk in stream:
                        if not chunk.choices or not chunk.choices[0].delta.content:
                            continue
                        full_text += chunk.choices[0].delta.content

                    # Post-processing
                    full_text = _ensure_ko_prefix(full_text, locale)

                    if full_text.strip().startswith("[ERROR]") or not full_text.strip():
                        yield "data: [DONE]\n\n"
                        return

                    # Add missing requirements addendum
                    addendum = _build_missing_requirements_addendum(
                        full_text,
                        locale,
                        saju_data,
                        astro_data,
                        today_date,
                    )
                    if addendum:
                        full_text = _insert_addendum(full_text, addendum)

                    # Add RAG debug info
                    debug_addendum = _build_rag_debug_addendum(rag_meta, locale) if debug_rag else ""
                    if debug_addendum:
                        sep = "\n\n" if full_text else ""
                        full_text = f"{full_text}{sep}{debug_addendum}"

                    # Format Korean spacing
                    full_text = _format_korean_spacing(full_text)
                    if debug_rag and full_text:
                        full_text = full_text.rstrip() + "\n"

                    # Add follow-up question if missing
                    if locale == "ko" and not full_text.rstrip().endswith("?"):
                        followup = "ÌòπÏãú ÏßÄÍ∏à Í∞ÄÏû• Í∂ÅÍ∏àÌïú Ìè¨Ïù∏Ìä∏Í∞Ä Î≠êÏòàÏöî?"
                        separator = "" if (full_text.endswith((" ", "\n", "\t")) or not full_text) else " "
                        full_text += f"{separator}{followup}"

                    # Stream in chunks
                    chunk_size = _get_stream_chunk_size()
                    for piece in _chunk_text(full_text, chunk_size):
                        yield _to_sse_event(piece)

                    # Signal end of stream
                    yield "data: [DONE]\n\n"

                except Exception as e:
                    logger.error(f"[StreamingService] Streaming error: {e}")
                    yield f"data: [ERROR] {str(e)}\n\n"

            return Response(
                stream_with_context(generate()),
                mimetype="text/event-stream",
                headers={
                    "Content-Type": "text/event-stream; charset=utf-8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",  # Disable nginx buffering
                }
            )

        except Exception as e:
            logger.exception(f"[StreamingService] request_id={request_id} stream_fortune failed: {e}")
            from flask import jsonify
            return jsonify({"status": "error", "message": str(e)}), 500

    def _build_user_context_section(self, user_context: Dict[str, Any], locale: str) -> str:
        """Build user context section for returning users (premium feature)."""
        if not user_context:
            return ""

        user_context_section = ""
        persona = user_context.get("persona", {})
        recent_sessions = user_context.get("recentSessions", [])
        personality_type = user_context.get("personalityType", {})

        # Nova Personality Type (from personality quiz)
        if personality_type.get("typeCode"):
            type_code = personality_type["typeCode"]
            type_name = personality_type.get("personaName", "")
            user_context_section = f"\n[üé≠ ÏÇ¨Ïö©Ïûê ÏÑ±Í≤© Ïú†Ìòï: {type_code}]\n"
            if type_name:
                user_context_section += f"‚Ä¢ Ïú†ÌòïÎ™Ö: {type_name}\n"

            # Lookup archetype details for counseling approach
            archetype_hints = {
                "RVLA": "Ï†ÑÎûµÏ†Å Í¥ÄÏ†êÏóêÏÑú Ï°∞Ïñ∏. ÌÅ∞ Í∑∏Î¶ºÍ≥º Ïã§Ìñâ Í≥ÑÌöç Ï†úÏãú. ÏßÅÏ†ëÏ†Å ÏÜåÌÜµ ÏÑ†Ìò∏.",
                "RVLF": "Îã§ÏñëÌïú ÏòµÏÖòÍ≥º Í∞ÄÎä•ÏÑ± Ï†úÏãú. Ïã§ÌóòÏ†Å Ï†ëÍ∑º Í∂åÏû•. Ï∞ΩÏùòÏ†Å Ìï¥Í≤∞Ï±Ö ÌÉêÏÉâ.",
                "RVHA": "ÎπÑÏ†ÑÍ≥º ÏùòÎØ∏ Ïó∞Í≤∞. Ïä§ÌÜ†Î¶¨ÌÖîÎßÅ ÌôúÏö©. ÎèôÍ∏∞Î∂ÄÏó¨ Ï§ëÏã¨ Ï°∞Ïñ∏.",
                "RVHF": "ÏòÅÍ∞êÍ≥º ÏÉàÎ°úÏö¥ Í¥ÄÏ†ê Ï†úÏãú. ÏÇ¨ÌöåÏ†Å Ïó∞Í≤∞ Í∞ïÏ°∞. Ïó¥Ï†ïÏ†Å ÏÜåÌÜµ.",
                "RSLA": "Î™ÖÌôïÌïú Îã®Í≥ÑÎ≥Ñ Ïã§Ìñâ Í≥ÑÌöç Ï†úÏãú. Ï±ÖÏûÑÍ≥º Í≤∞Í≥º Í∞ïÏ°∞. Ìö®Ïú®Ï†Å Ï°∞Ïñ∏.",
                "RSLF": "Ï¶âÍ∞ÅÏ†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏù∏ Ìï¥Í≤∞Ï±Ö. ÌòÑÏû• Í∞êÍ∞Å ÌôúÏö©. ÏúÑÍ∏∞ ÎåÄÏùë Í¥ÄÏ†ê.",
                "RSHA": "Í¥ÄÍ≥ÑÏôÄ ÏÑ±Ïû• Ï§ëÏã¨. Îî∞ÎúªÌïòÎ©¥ÏÑúÎèÑ Ï≤¥Í≥ÑÏ†Å. ÏïàÏ†ïÏ†Å ÌôòÍ≤Ω Í∞ïÏ°∞.",
                "RSHF": "Ï∞∏Ïó¨ÏôÄ ÏÜåÌÜµ Ï§ëÏã¨. Îã§ÏñëÌïú Í¥ÄÏ†ê Ìè¨Ïö©. Ìï®ÍªòÌïòÎäî Ìï¥Í≤∞.",
                "GVLA": "Í∑ºÎ≥∏ ÏõêÏù∏ Î∂ÑÏÑù. Ïû•Í∏∞Ï†Å Í¥ÄÏ†ê. Ï≤¥Í≥ÑÏ†ÅÏù¥Í≥† ÍπäÏùÄ Ìï¥Í≤∞Ï±Ö.",
                "GVLF": "Îç∞Ïù¥ÌÑ∞ÏôÄ Ï¶ùÍ±∞ Í∏∞Î∞ò Ï†ëÍ∑º. Ï≤¥Í≥ÑÏ†Å Î∂ÑÏÑù. Ìå®ÌÑ¥ ÌôúÏö©.",
                "GVHA": "ÏÑ±Ïû•Í≥º Î∞úÏ†Ñ Ï§ëÏã¨. Ïû•Í∏∞Ï†Å Í¥ÄÍ≥Ñ. Î©òÌÜ†ÎßÅ Í¥ÄÏ†ê.",
                "GVHF": "ÏùòÎØ∏ÏôÄ Î™©Ï†Å Ïó∞Í≤∞. ÍπäÏùÄ ÏßàÎ¨∏. ÏßÑÏ†ïÏÑ± ÏûàÎäî ÎåÄÌôî.",
                "GSLA": "Îã®Í≥ÑÎ≥Ñ Í≤ÄÏ¶ùÎêú Î∞©Î≤ï Í∂åÏû•. ÏïàÏ†ïÏ†Å Ïã§Ìñâ ÏßÄÏõê. Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨.",
                "GSLF": "Î¨∏Ï†úÎ•º ÏûëÍ≤å Î∂ÑÌï¥. ÌïòÎÇòÏî© Ï≤¥Í≥ÑÏ†Å Ìï¥Í≤∞. Ï†ïÎ∞ÄÌïú Ï†ëÍ∑º.",
                "GSHA": "ÏïàÏ†ïÍ≥º Ïã†Î¢∞ Í∏∞Î∞ò. Ï†êÏßÑÏ†Å Î≥ÄÌôî Í∂åÏû•. Íæ∏Ï§ÄÌïú ÏßÄÏõê.",
                "GSHF": "Í∞àÎì± Ìï¥ÏÜåÏôÄ Ï°∞Ìôî Ï§ëÏã¨. Í≤ΩÏ≤≠Í≥º Ï§ëÏû¨. Î∂ÄÎìúÎü¨Ïö¥ Ï†ëÍ∑º.",
            }
            if type_code in archetype_hints:
                user_context_section += f"‚Ä¢ ÏÉÅÎã¥ Ïä§ÌÉÄÏùº: {archetype_hints[type_code]}\n"

            user_context_section += "\n‚Üí Ïù¥ ÏÇ¨Ïö©ÏûêÏùò ÏÑ±Í≤© Ïú†ÌòïÏóê ÎßûÍ≤å ÏÜåÌÜµ Ïä§ÌÉÄÏùºÏùÑ Ï°∞Ï†àÌïòÏÑ∏Ïöî.\n"
            logger.info(f"[StreamingService] Personality type: {type_code}")

        if persona.get("sessionCount", 0) > 0 or recent_sessions:
            if not user_context_section:
                user_context_section = "\n[üîÑ Ïù¥Ï†Ñ ÏÉÅÎã¥ Îß•ÎùΩ]\n"
            else:
                user_context_section += "\n[üîÑ Ïù¥Ï†Ñ ÏÉÅÎã¥ Îß•ÎùΩ]\n"

            # Persona memory
            if persona.get("sessionCount"):
                user_context_section += f"‚Ä¢ Ï¥ù {persona['sessionCount']}Ìöå ÏÉÅÎã¥Ìïú Ïû¨Î∞©Î¨∏ Í≥†Í∞ù\n"
            if persona.get("lastTopics"):
                topics = persona["lastTopics"][:3] if isinstance(persona["lastTopics"], list) else []
                if topics:
                    user_context_section += f"‚Ä¢ Ï£ºÏöî Í¥ÄÏã¨ÏÇ¨: {', '.join(topics)}\n"
            if persona.get("emotionalTone"):
                user_context_section += f"‚Ä¢ Í∞êÏ†ï ÏÉÅÌÉú: {persona['emotionalTone']}\n"
            if persona.get("recurringIssues"):
                issues = persona["recurringIssues"][:2] if isinstance(persona["recurringIssues"], list) else []
                if issues:
                    user_context_section += f"‚Ä¢ Î∞òÎ≥µ Ïù¥Ïäà: {', '.join(issues)}\n"

            # Recent session summaries
            if recent_sessions:
                user_context_section += "\n[ÏµúÍ∑º ÎåÄÌôî]\n"
                for sess in recent_sessions[:2]:  # Last 2 sessions
                    if sess.get("summary"):
                        user_context_section += f"‚Ä¢ {sess['summary']}\n"
                    elif sess.get("keyTopics"):
                        topics_str = ", ".join(sess["keyTopics"][:3]) if isinstance(sess["keyTopics"], list) else ""
                        if topics_str:
                            user_context_section += f"‚Ä¢ Ï£ºÏ†ú: {topics_str}\n"

            user_context_section += "\n‚Üí Ïû¨Î∞©Î¨∏ Í≥†Í∞ùÏù¥Îãà 'Îòê Ïò§ÏÖ®ÎÑ§Ïöî' Í∞ôÏùÄ ÏπúÍ∑ºÌïú Ïù∏ÏÇ¨Î°ú ÏãúÏûëÌïòÍ≥†, Ïù¥Ï†Ñ ÏÉÅÎã¥ ÎÇ¥Ïö©ÏùÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.\n"
            logger.info(f"[StreamingService] User context section: {len(user_context_section)} chars")

        return user_context_section

    def _build_lifespan_section(self, birth_data: Dict[str, Any], saju_data: Dict[str, Any]) -> str:
        """Build lifespan guidance section (age-appropriate psychological tasks)."""
        from backend_ai.app.app import get_lifespan_guidance

        lifespan_section = ""
        birth_year = None
        try:
            # Extract birth year from birth_data or saju_data
            if birth_data.get("date"):
                birth_year = int(birth_data["date"].split("-")[0])
            elif saju_data.get("birthYear"):
                birth_year = int(saju_data["birthYear"])
        except (ValueError, KeyError, TypeError, AttributeError):
            pass

        if birth_year:
            lifespan_guidance = get_lifespan_guidance(birth_year)
            if lifespan_guidance and lifespan_guidance.get("stage_name"):
                stage = lifespan_guidance
                lifespan_section = f"""
[üå± ÏÉùÏï†Ï£ºÍ∏∞Î≥Ñ Ïã¨Î¶¨ Í≥ºÏ†ú: {stage['stage_name']} ({stage['age']}ÏÑ∏)]
‚Ä¢ Î∞úÎã¨ Í≥ºÏ†ú: {', '.join(stage.get('psychological_tasks', [])[:3])}
‚Ä¢ ÌïµÏã¨ ÏõêÌòï: {stage.get('archetypal_themes', {}).get('primary', [''])[0] if isinstance(stage.get('archetypal_themes', {}).get('primary'), list) else ''}
‚Ä¢ ÌùîÌïú ÏúÑÍ∏∞: {', '.join(stage.get('developmental_crises', stage.get('shadow_challenges', []))[:2])}
‚Ä¢ ÏÇ¨Ï£º Ïó∞Í≤∞: {stage.get('saju_parallel', {}).get('theme', '')}
‚Ä¢ Ï†êÏÑ± Ïó∞Í≤∞: {stage.get('astro_parallel', {}).get('theme', '')}

‚Üí Ïù¥ ÏÉùÏï† Îã®Í≥ÑÏóê ÎßûÎäî Ï°∞Ïñ∏ÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî. ÎÇòÏù¥Ïóê ÎßûÏßÄ ÏïäÎäî Ï°∞Ïñ∏(Ïòà: 20ÎåÄÏóêÍ≤å 'ÏùÄÌá¥ Ï§ÄÎπÑ')ÏùÄ ÌîºÌïòÏÑ∏Ïöî.
"""
                logger.info(f"[StreamingService] Lifespan guidance: {stage['stage_name']} (age {stage['age']})")

        return lifespan_section

    def _build_theme_fusion_section(self, saju_data: Dict[str, Any], astro_data: Dict[str, Any],
                                   theme: str, locale: str, birth_data: Dict[str, Any]) -> str:
        """Build theme fusion rules section (daily/monthly/yearly guidance)."""
        from backend_ai.app.app import get_theme_fusion_rules

        theme_fusion_section = ""
        birth_year = None
        try:
            if birth_data.get("date"):
                birth_year = int(birth_data["date"].split("-")[0])
            elif saju_data.get("birthYear"):
                birth_year = int(saju_data["birthYear"])
        except (ValueError, KeyError, TypeError, AttributeError):
            pass

        try:
            theme_fusion = get_theme_fusion_rules(saju_data, astro_data, theme, locale, birth_year)
            if theme_fusion:
                theme_fusion_section = f"""
[üéØ ÌÖåÎßàÎ≥Ñ ÏúµÌï© Ìï¥ÏÑù]
{theme_fusion}

‚Üí ÏúÑ ÌÖåÎßàÎ≥Ñ Ìï¥ÏÑùÏùÑ ÏÉÅÎã¥ ÎÇ¥Ïö©Ïóê ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎÖπÏó¨ÏÑú Ï†ÑÎã¨ÌïòÏÑ∏Ïöî.
"""
                logger.info(f"[StreamingService] Theme fusion rules added: {len(theme_fusion)} chars, theme={theme}")
        except Exception as e:
            logger.warning(f"[StreamingService] Theme fusion rules failed: {e}")

        return theme_fusion_section

    def _build_imagination_section(self, prompt: str) -> str:
        """Build active imagination prompts section (deep therapeutic work)."""
        from backend_ai.app.app import get_active_imagination_prompts

        imagination_section = ""
        if prompt and any(k in prompt.lower() for k in ["ÍπäÏù¥", "ÎÇ¥Î©¥", "Î¨¥ÏùòÏãù", "Í∑∏Î¶ºÏûê", "Î™ÖÏÉÅ", "ÏÉÅÏÉÅ"]):
            ai_prompts = get_active_imagination_prompts(prompt)
            if ai_prompts:
                imagination_section = f"""
[üé® Ï†ÅÍ∑πÏ†Å ÏÉÅÏÉÅ Í∏∞Î≤ï - Ïã¨Ï∏µ ÏûëÏóÖÏö©]
‚Ä¢ ÏãúÏûë ÏßàÎ¨∏: {ai_prompts.get('opening', [''])[0] if ai_prompts.get('opening') else ''}
‚Ä¢ Ïã¨Ìôî ÏßàÎ¨∏: {ai_prompts.get('deepening', [''])[0] if ai_prompts.get('deepening') else ''}
‚Ä¢ ÌÜµÌï© ÏßàÎ¨∏: {ai_prompts.get('integration', [''])[0] if ai_prompts.get('integration') else ''}

‚Üí ÏÇ¨Ïö©ÏûêÍ∞Ä ÍπäÏùÄ ÎÇ¥Î©¥ ÏûëÏóÖÏùÑ ÏõêÌï† ÎïåÎßå Ïù¥ ÏßàÎ¨∏Îì§ÏùÑ ÌôúÏö©ÌïòÏÑ∏Ïöî. Í∞ïÏöîÌïòÏßÄ ÎßàÏÑ∏Ïöî.
"""
                logger.info(f"[StreamingService] Active imagination prompts added")

        return imagination_section

    def _build_crisis_context(self, crisis_response: Optional[Dict], crisis_check: Dict) -> str:
        """Build crisis context section for medium/medium_high severity."""
        crisis_context_section = ""
        if crisis_response and not crisis_check.get("requires_immediate_action"):
            severity = crisis_check.get("max_severity", "")
            if severity == "medium_high":
                crisis_context_section = """
[‚ö†Ô∏è ÏÇ¨Ïö©Ïûê Í∞êÏ†ï ÏÉÅÌÉú: ÎÜíÏùÄ Ïä§Ìä∏Î†àÏä§]
- Í≥µÍ∞êÍ≥º ÏïàÏ†ïÍ∞êÏùÑ Ï£ºÎäî ÌÜ§ÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî
- Î®ºÏ†Ä Í∞êÏ†ïÏùÑ Ïù∏Ï†ïÌïòÍ≥† Ìò∏Ìù°/Í∑∏ÎùºÏö¥Îî© Í∏∞Î≤ïÏùÑ ÏïàÎÇ¥ÌïòÏÑ∏Ïöî
- Ï†êÏà† Ìï¥ÏÑùÏùÄ Ìù¨ÎßùÏ†ÅÏù∏ Í¥ÄÏ†êÏúºÎ°ú Î∂ÄÎìúÎüΩÍ≤å Ï†ÑÎã¨ÌïòÏÑ∏Ïöî
- ÌïÑÏöîÏãú Ï†ÑÎ¨∏ ÏÉÅÎã¥ Í∂åÏú†: Ï†ïÏã†Í±¥Í∞ïÏúÑÍ∏∞ÏÉÅÎã¥Ï†ÑÌôî 1577-0199
"""
            elif severity == "medium":
                crisis_context_section = """
[‚ö†Ô∏è ÏÇ¨Ïö©Ïûê Í∞êÏ†ï ÏÉÅÌÉú: Ìù¨Îßù Ï†ÄÌïò]
- Í≥µÍ∞êÍ≥º Îî∞ÎúªÌï®ÏùÑ Îã¥ÏïÑ ÏùëÎãµÌïòÏÑ∏Ïöî
- ÏûëÏùÄ Ìù¨ÎßùÏù¥ÎùºÎèÑ Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù ÎèÑÏôÄÏ£ºÏÑ∏Ïöî
- Ï†êÏà† Ìï¥ÏÑùÏóêÏÑú Í∏çÏ†ïÏ†Å Í∞ÄÎä•ÏÑ±ÏùÑ Í∞ïÏ°∞ÌïòÏÑ∏Ïöî
- "ÌòºÏûêÍ∞Ä ÏïÑÎãàÏóêÏöî"ÎùºÎäî Î©îÏãúÏßÄÎ•º ÏûêÏó∞Ïä§ÎüΩÍ≤å Ï†ÑÎã¨ÌïòÏÑ∏Ïöî
"""
            logger.info(f"[StreamingService] Added crisis context for severity={severity}")

        return crisis_context_section

    def _build_therapeutic_section(self, prompt: str, locale: str, has_counseling: bool) -> str:
        """Build therapeutic guidance based on question type (Jung psychology enhanced)."""
        therapeutic_section = ""
        if not (has_counseling and prompt):
            return therapeutic_section

        prompt_lower = prompt.lower()

        # Detect question themes and add therapeutic guidance
        if any(k in prompt_lower for k in ["ÌûòÎì§", "Ïö∞Ïö∏", "ÏßÄÏ≥ê", "Ìè¨Í∏∞", "ÏùòÎØ∏ÏóÜ", "ÌóàÎ¨¥"]):
            therapeutic_section = """
[üß† Ïã¨Î¶¨ÏÉÅÎã¥ Í∞ÄÏù¥Îìú: ÏùòÎØ∏/Ï†ïÏÑú ÏßÄÏßÄ]
- Î®ºÏ†Ä Í∞êÏ†ïÏùÑ Ï∂©Î∂ÑÌûà Ïù∏Ï†ï: "Ï†ïÎßê ÌûòÎìúÏÖ®Í≤†Ïñ¥Ïöî... Í∑∏ Î¨¥Í≤åÎ•º ÌòºÏûê ÏßÄÍ≥† Í≥ÑÏÖ®Íµ∞Ïöî"
- Ïúµ Í¥ÄÏ†ê: "ÏòÅÌòºÏùò Ïñ¥ÎëêÏö¥ Î∞§(dark night of soul)"ÏùÄ Î≥ÄÌôîÏùò Ï†ÑÏ°∞
- ÏÇ¨Ï£º/Ï†êÏÑ±ÏóêÏÑú 'Ï†ÑÌôòÏ†ê'Ïù¥ÎÇò 'ÏÑ±Ïû•Í∏∞'Î•º Ï∞æÏïÑ Ìù¨Îßù Ïó∞Í≤∞
- Í∑∏Î¶ºÏûê ÏûëÏóÖ: "Ïù¥ ÌûòÎì¶Ïù¥ ÎãπÏã†ÏóêÍ≤å Í∞ÄÎ•¥ÏπòÎ†§Îäî Í≤å ÏûàÎã§Î©¥?"
- ÏûëÏùÄ Ïï°ÏÖò Ï†úÏïà: "Ïò§Îäò ÌïòÎÇòÎßå ÏûêÏã†ÏùÑ ÏúÑÌï¥ ÌïúÎã§Î©¥ Î≠ò ÌïòÍ≥† Ïã∂ÏúºÏÑ∏Ïöî?"
"""
        elif any(k in prompt_lower for k in ["Ïó∞Ïï†", "ÏÇ¨Îûë", "Í≤∞Ìòº", "Ïù¥Î≥Ñ", "ÏßùÏÇ¨Îûë", "Ïç∏"]):
            therapeutic_section = """
[üß† Ïã¨Î¶¨ÏÉÅÎã¥ Í∞ÄÏù¥Îìú: Í¥ÄÍ≥Ñ/ÏÇ¨Îûë]
- Í∞êÏ†ïÏùò ÍπäÏù¥Î•º Ïù∏Ï†ï: "ÎßàÏùåÏù¥ ÎßéÏù¥ Ïì∞Ïù¥ÏãúÎÑ§Ïöî"
- Ïúµ Í¥ÄÏ†ê - ÏïÑÎãàÎßà/ÏïÑÎãàÎ¨¥Ïä§ Ìà¨ÏÇ¨: "ÎÅåÎ¶¨Îäî Í∑∏ ÌäπÏÑ±Ïù¥ ÌòπÏãú ÎÇ¥ ÏïàÏóêÎèÑ ÏûàÎã§Î©¥?"
- Í∑∏Î¶ºÏûê Ìà¨ÏÇ¨: "Ïã´ÏùÄ Í∑∏ Ï†ê... ÎÇ¥ Í∑∏Î¶ºÏûêÎäî ÏïÑÎãêÍπåÏöî?"
- ÏÇ¨Ï£º Í¥ÄÏÑ±(ÂÆòÊòü)/Ï†êÏÑ± Í∏àÏÑ±-7ÌïòÏö∞Ïä§ Ìï¥ÏÑùÏùÑ Ïã¨Î¶¨Ï†Å Ìå®ÌÑ¥Í≥º Ïó∞Í≤∞
- ÏßàÎ¨∏ÏúºÎ°ú ÎßàÎ¨¥Î¶¨: "ÏÉÅÎåÄÏóêÍ≤å ÏßÑÏßú ÏõêÌïòÎäî Í±¥ Î≠òÍπåÏöî?" / "ÏôÑÎ≤ΩÌïú Í¥ÄÍ≥ÑÎûÄ Ïñ¥Îñ§ Î™®ÏäµÏù¥ÏóêÏöî?"
"""
        elif any(k in prompt_lower for k in ["Ï∑®ÏóÖ", "Ïù¥ÏßÅ", "ÏßÑÎ°ú", "ÏÇ¨ÏóÖ", "Ìá¥ÏÇ¨", "Ïª§Î¶¨Ïñ¥"]):
            therapeutic_section = """
[üß† Ïã¨Î¶¨ÏÉÅÎã¥ Í∞ÄÏù¥Îìú: Ïª§Î¶¨Ïñ¥/Ï†ïÏ≤¥ÏÑ±]
- Î∂àÏïàÍ∞ê Ïù∏Ï†ï: "Ï§ëÏöîÌïú Í≤∞Ï†ï ÏïûÏóêÏÑú Í≥†ÎØºÏù¥ ÍπäÏúºÏãúÎÑ§Ïöî"
- Ïúµ Í¥ÄÏ†ê - ÏÜåÎ™Ö(calling): "ÎèàÏùÑ Îñ†ÎÇòÏÑú, ÏßÑÏßú ÌïòÍ≥† Ïã∂ÏùÄ ÏùºÏùÄ Î≠êÏòàÏöî?"
- ÌéòÎ•¥ÏÜåÎÇò vs ÏûêÍ∏∞(Self): "ÏùºÌïòÎäî ÎÇò vs ÏßÑÏßú ÎÇò, ÏñºÎßàÎÇò Îã§Î•∏Í∞ÄÏöî?"
- ÏÇ¨Ï£º ÏãùÏÉÅ/Ïû¨ÏÑ±Í≥º Ï†êÏÑ± 10ÌïòÏö∞Ïä§/MC Ïó∞Í≤∞ÌïòÏó¨ Ï†ÅÏÑ± Î∂ÑÏÑù
- Íµ¨Ï≤¥Ï†Å ÏãúÍ∏∞ Ï†úÏãú: "2025ÎÖÑ ÏÉÅÎ∞òÍ∏∞Í∞Ä Ï†ÑÌôòÏ†ê" ÏãùÏúºÎ°ú
- ÏßàÎ¨∏: "Îèà vs Î≥¥Îûå, ÏßÄÍ∏à Îçî Ï§ëÏöîÌïú Í±¥?" / "5ÎÖÑ Îí§ Ïñ¥Îñ§ Î™®ÏäµÏù¥Í≥† Ïã∂ÏúºÏÑ∏Ïöî?"
"""
        elif any(k in prompt_lower for k in ["Ïñ∏Ï†ú", "ÏãúÍ∏∞", "ÌÉÄÏù¥Î∞ç", "Î™á Ïõî", "Ïò¨Ìï¥", "ÎÇ¥ÎÖÑ"]):
            therapeutic_section = """
[üß† Ïã¨Î¶¨ÏÉÅÎã¥ Í∞ÄÏù¥Îìú: ÏãúÍ∏∞/ÌÉÄÏù¥Î∞ç]
- Íµ¨Ï≤¥Ï†Å ÏãúÍ∏∞ Ï†úÏãú ÌïÑÏàò: ÏÇ¨Ï£º ÎåÄÏö¥/ÏÑ∏Ïö¥ + Ï†êÏÑ± Ìä∏ÎûúÏßì Î∂ÑÏÑù
- Ïõî/Î∂ÑÍ∏∞ Îã®ÏúÑÎ°ú Î™ÖÌôïÌïòÍ≤å: "2025ÎÖÑ 3-4ÏõîÏù¥ Ï¢ãÏïÑÏöî"
- Ïôú Í∑∏ ÏãúÍ∏∞Ïù∏ÏßÄ ÏÑ§Î™Ö: "Î™©ÏÑ±Ïù¥ ~Ïóê Îì§Ïñ¥Ïò§Î©¥ÏÑú..."
- Í∑∏ ÏãúÍ∏∞Ïóê Ìï† Ïùº Ï†úÏïà: "Ïù¥ ÏãúÍ∏∞Ïóê [Íµ¨Ï≤¥Ï†Å ÌñâÎèô]ÏùÑ ÏãúÏûëÌïòÎ©¥ Ï¢ãÍ≤†Ïñ¥Ïöî"
- Ï£ºÏùòÌï† ÏãúÍ∏∞ÎèÑ Ìï®Íªò: "Îã§Îßå ~ÏõîÏùÄ Ïã†Ï§ëÌïòÍ≤å"
"""

        return therapeutic_section

    def _build_system_prompt(self, **kwargs) -> str:
        """Build system prompt for OpenAI (full structured prompt)."""
        # Extract all kwargs
        is_frontend_structured = kwargs.get("is_frontend_structured", False)
        locale = kwargs.get("locale", "ko")
        theme = kwargs.get("theme", "chat")
        timing_window_str = kwargs.get("timing_window_str", "")
        current_date_str = kwargs.get("current_date_str", "")
        rag_context = kwargs.get("rag_context", "")
        cross_rules = kwargs.get("cross_rules", "")
        saju_detail = kwargs.get("saju_detail", "")
        astro_detail = kwargs.get("astro_detail", "")
        advanced_astro_detail = kwargs.get("advanced_astro_detail", "")
        cross_section = kwargs.get("cross_section", "")
        user_context_section = kwargs.get("user_context_section", "")
        cv_section = kwargs.get("cv_section", "")
        lifespan_section = kwargs.get("lifespan_section", "")
        theme_fusion_section = kwargs.get("theme_fusion_section", "")
        imagination_section = kwargs.get("imagination_section", "")
        crisis_context_section = kwargs.get("crisis_context_section", "")
        therapeutic_section = kwargs.get("therapeutic_section", "")

        # FRONTEND STRUCTURED PROMPT - Use simplified backend system prompt
        if is_frontend_structured:
            # Build RAG-only enrichment section
            rag_enrichment_parts = []

            # 1. Cross-analysis rules
            if cross_rules:
                rag_enrichment_parts.append(f"[üîó ÏÇ¨Ï£º+Ï†êÏÑ± ÍµêÏ∞® Ìï¥ÏÑù Í∑úÏπô]\n{cross_rules[:1500]}")

            # 2. Jung/Stoic quotes from RAG
            if rag_context:
                rag_enrichment_parts.append(rag_context)

            # 3. Lifespan guidance
            if lifespan_section:
                rag_enrichment_parts.append(lifespan_section)

            # 4. Theme fusion rules
            if theme_fusion_section:
                rag_enrichment_parts.append(theme_fusion_section)

            # 5. Therapeutic guidance
            if therapeutic_section:
                rag_enrichment_parts.append(therapeutic_section)

            # 6. Crisis context
            if crisis_context_section:
                rag_enrichment_parts.append(crisis_context_section)

            # 7. User context
            if user_context_section:
                rag_enrichment_parts.append(user_context_section)

            # 8. CV section
            if cv_section:
                rag_enrichment_parts.append(cv_section)

            rag_enrichment = "\n\n".join(rag_enrichment_parts) if rag_enrichment_parts else ""

            # Simplified system prompt for frontend-structured requests
            if locale == "en":
                system_prompt = f"""You are a Saju+Astrology integrated counselor. Speak naturally and weave the data into your sentences. Start the first sentence directly with an answer (e.g., "So," or "Right now,").

ABSOLUTELY AVOID:
- Formal greetings ("Hello", "Nice to meet you")
- Self-introductions
- Bullet lists or numbered lists
- Bold text

STYLE:
- Conversational and warm, but concise
- Use 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- End with exactly one follow-up question

EVIDENCE REQUIRED (inline, not as a list):
- At least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- At least one Astrology reference (Sun/Moon/ASC plus a planet+house if possible)
- Give 2-3 timing windows within 6 months using month+week phrasing, and include one caution point
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.

{timing_window_str}

Additional knowledge:
{rag_enrichment if rag_enrichment else "(none)"}

Response length: 400-600 words, {locale}, natural spoken tone."""
            else:
                system_prompt = f"""ÏÇ¨Ï£º+Ï†êÏÑ± ÌÜµÌï© ÏÉÅÎã¥ÏÇ¨. ÏπúÍµ¨ÏóêÍ≤å ÎßêÌïòÎìØ ÏûêÏó∞Ïä§ÎüΩÍ≤å, Îç∞Ïù¥ÌÑ∞Î•º ÎÖπÏó¨ÏÑú Ìï¥ÏÑùÌï¥. Ï≤´ Î¨∏Ïû•ÏùÄ 'Ïù¥Ïïº'Î°ú ÏãúÏûëÌï¥(ÎßêÏ§ÑÏûÑÌëú Í∞ÄÎä•).

üö´ Ï†àÎåÄ Í∏àÏßÄ:
- "ÏùºÍ∞ÑÏù¥ XÏûÖÎãàÎã§" ÎÇòÏó¥Ïãù ÏÑ§Î™Ö (ÏÇ¨Ïö©ÏûêÎäî Ïù¥ÎØ∏ ÏûêÍ∏∞ Ï∞®Ìä∏ ÏïåÍ≥† ÏûàÏùå)
- "ÏïàÎÖïÌïòÏÑ∏Ïöî" Ïù∏ÏÇ¨
- "Ï°∞Ïã¨ÌïòÏÑ∏Ïöî" "Ï¢ãÏïÑÏßà Í±∞ÏòàÏöî" Îú¨Íµ¨Î¶Ñ Îßê
- **Î≥ºÎìúÏ≤¥**, Î≤àÌò∏ Îß§Í∏∞Í∏∞, Î™©Î°ù ÎÇòÏó¥

‚úÖ Ïò¨Î∞îÎ•∏ Ïä§ÌÉÄÏùº:
- Ïπ¥ÌéòÏóêÏÑú ÏπúÍµ¨ÌïúÌÖå ÏñòÍ∏∞ÌïòÎìØ ÏûêÏó∞Ïä§ÎüΩÍ≤å
- Îç∞Ïù¥ÌÑ∞Î•º Î¨∏Ïû• ÏÜçÏóê ÎÖπÏó¨ÏÑú (ÎÇòÏó¥ X)
- Ïã§ÏÉùÌôúÍ≥º Ïó∞Í≤∞Ìï¥ÏÑú ÏÑ§Î™Ö
- Ìï¥ÏöîÏ≤¥Î°ú ÏπúÍ∑ºÌïòÍ≤å (ÎÑàÎ¨¥ Îî±Îî±Ìïú Î¨∏Ïñ¥Ï≤¥ Í∏àÏßÄ)
- ÎßêÌà¨Îäî Î∂ÄÎìúÎüΩÍ≥† Îã§Ï†ïÌïòÍ≤å, Îã®Ï†ï ÎåÄÏã† '~Í∞ôÏïÑ/Í∞ÄÎä•ÏÑ±' ÌëúÌòÑ ÏÇ¨Ïö©
- Î¨∏Îã® 3Í∞ú ÎÇ¥Ïô∏ (ÌïµÏã¨ ÏöîÏïΩ ‚Üí Í∑ºÍ±∞/Ìå®ÌÑ¥ ‚Üí ÌÉÄÏù¥Î∞ç/ÌñâÎèô + ÏßàÎ¨∏)

‚úÖ Í∑ºÍ±∞ ÌïÑÏàò:
- ÏÇ¨Ï£º Í∑ºÍ±∞ 1Í∞ú Ïù¥ÏÉÅ(ÏùºÍ∞Ñ/ÎåÄÏö¥/ÏÑ∏Ïö¥ Ï§ë 1Í∞ú) + Ïò§Ìñâ/Ïã≠ÏÑ± Î∞òÎìúÏãú Ïñ∏Í∏â
- Ï†êÏÑ± Í∑ºÍ±∞ 1Í∞ú Ïù¥ÏÉÅ(ÌÉúÏñë/Îã¨/ASC Ï§ë 1Í∞ú) + ÌñâÏÑ±/ÌïòÏö∞Ïä§ Î∞òÎìúÏãú Ïñ∏Í∏â(Í∞ÄÎä•ÌïòÎ©¥ Í∞Å 1Í∞ú)
- Í∑ºÍ±∞Îäî Î¨∏Ïû• ÏÜçÏóê ÏûêÏó∞Ïä§ÎüΩÍ≤å Ìè¨Ìï® (ÎÇòÏó¥ Í∏àÏßÄ)
- 6Í∞úÏõî ÌÉÄÏù¥Î∞ç 2~3Í∞úÎ•º Ïõî+Ï£º Îã®ÏúÑÎ°ú Ï†úÏãú(Ïòà: 3Ïõî 2~3Ï£ºÏ∞®)
- ÌÉÄÏù¥Î∞ç Ï§ë 1Í∞úÎäî Ï£ºÏùòÏ†ê/ÌîºÌï¥Ïïº Ìï† Ìè¨Ïù∏Ìä∏ Ìè¨Ìï®
- ÌÖåÎßà Í≥†Ï†ï: theme="{theme}"Îßå Îã§Î£®Í≥† Îã§Î•∏ ÌÖåÎßàÎ°ú ÌùêÎ•¥ÏßÄ Îßê Í≤É.
- ÎßàÏßÄÎßâÏóê ÌõÑÏÜç ÏßàÎ¨∏ 1Í∞ú

üìÖ {timing_window_str}

üìö Ï∂îÍ∞Ä ÏßÄÏãù:
{rag_enrichment if rag_enrichment else "(ÏóÜÏùå)"}

üìå 500-800Ïûê, {locale}, ÏûêÏó∞Ïä§Îü¨Ïö¥ Íµ¨Ïñ¥Ï≤¥"""

            logger.info(f"[StreamingService] Using SIMPLIFIED system prompt for frontend-structured request (RAG enrichment: {len(rag_enrichment)} chars)")

        else:
            # LEGACY PATH - Build full system prompt (for non-frontend requests)
            counselor_persona = """ÎãπÏã†ÏùÄ ÏÇ¨Ï£º+Ï†êÏÑ±Ïà† ÌÜµÌï© ÏÉÅÎã¥ÏÇ¨ÏûÖÎãàÎã§.

‚ö†Ô∏è Ï†àÎåÄ Í∑úÏπô:
1. Ïù∏ÏÇ¨ Í∏àÏßÄ - "ÏïàÎÖïÌïòÏÑ∏Ïöî", "Î∞òÍ∞ÄÏõåÏöî" Îì± Ïù∏ÏÇ¨ Ï†àÎåÄ Í∏àÏßÄ
2. Ïã†ÏÉÅ ÏÜåÍ∞ú Í∏àÏßÄ - "ÏùºÍ∞ÑÏù¥ XÏûÖÎãàÎã§", "ÎãπÏã†ÏùÄ Y ÏÑ±Ìñ•" Í∞ôÏùÄ Í∏∞Î≥∏ ÏÑ§Î™Ö Í∏àÏßÄ. ÏÇ¨Ïö©ÏûêÎäî Ïù¥ÎØ∏ ÏûêÍ∏∞ ÏÇ¨Ï£ºÎ•º ÏïàÎã§. Î∞îÎ°ú ÏßàÎ¨∏Ïóê ÎãµÌï¥.
3. Ï†úÍ≥µÎêú Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö© - ÎåÄÏö¥/ÏÑ∏Ïö¥ÏùÑ ÏßÄÏñ¥ÎÇ¥ÏßÄ ÎßàÏÑ∏Ïöî. ÏïÑÎûò [ÏÇ¨Ï£º Î∂ÑÏÑù]Ïóê ÏûàÎäî Í∑∏ÎåÄÎ°úÎßå Ïù∏Ïö©
4. Ï≤´ Î¨∏Ïû•Î∂ÄÌÑ∞ ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥ÄÏúºÎ°ú ÏãúÏûë

üí¨ ÏÉÅÎã¥ Ïä§ÌÉÄÏùº:
‚Ä¢ ÏÉÅÏÑ∏ÌïòÍ≥† ÍπäÏù¥ ÏûàÎäî Î∂ÑÏÑù (400-600Îã®Ïñ¥)
‚Ä¢ ÏÇ¨Ï£ºÏôÄ Ï†êÏÑ±Ïà† Í∑†ÌòïÏûàÍ≤å ÌôúÏö©ÌïòÎêò ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎÖπÏó¨ÎÇ¥
‚Ä¢ Íµ¨Ï≤¥Ï†Å ÎÇ†Ïßú/ÏãúÍ∏∞ Ï†úÏãú
‚Ä¢ 'Ïôú Í∑∏Îü∞ÏßÄ' Ïù¥Ïú†Î•º Ï∂©Î∂ÑÌûà ÏÑ§Î™Ö
‚Ä¢ Ïúµ Ïã¨Î¶¨Ìïô Ïù∏Ïö©Ïù¥ ÏûàÏúºÎ©¥ Ìï¥ÏÑùÏóê ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎÖπÏó¨ÏÑú ÍπäÏù¥ ÎçîÌïòÍ∏∞"""

            if locale == "en":
                counselor_persona = """You are an integrated Saju + Astrology counselor.

ABSOLUTE RULES:
1. No greetings or self-introductions.
2. Answer the user's question from the first sentence.
3. Use only provided data; do not invent Ïö¥ or placements.

STYLE:
- 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- Provide concrete timing windows within 6 months, including one caution
- Keep the tone warm and practical
"""

            # Build advanced astrology section
            advanced_astro_section = ""
            if advanced_astro_detail:
                advanced_astro_section = f"""

[üî≠ Ïã¨Ï∏µ Ï†êÏÑ± Î∂ÑÏÑù]
{advanced_astro_detail}
"""

            if locale == "en":
                system_prompt = f"""{counselor_persona}

{timing_window_str}

[SAJU ANALYSIS]
{saju_detail}

[ASTROLOGY ANALYSIS]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[RESPONSE RULES]
- Include at least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- Include at least one Astrology reference (Sun/Moon/ASC + planet+house if possible)
- 2-3 timing windows within 6 months (month+week phrasing), include one caution
- End with exactly one follow-up question
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.
- Respond in English only
"""
            else:
                system_prompt = f"""{counselor_persona}

‚ö†Ô∏è {current_date_str} - Í≥ºÍ±∞ ÎÇ†ÏßúÎ•º ÎØ∏ÎûòÏ≤òÎüº ÎßêÌïòÏßÄ ÎßàÏÑ∏Ïöî
‚ö†Ô∏è {timing_window_str} - Ïù¥ Î≤îÏúÑ ÏïàÏóêÏÑú 2~3Í∞ú ÏãúÍ∏∞Î•º Ï†úÏãúÌïòÏÑ∏Ïöî

[üìä ÏÇ¨Ï£º Î∂ÑÏÑù]
{saju_detail}

[üåü Ï†êÏÑ± Î∂ÑÏÑù]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[üéØ ÏùëÎãµ Ïä§ÌÉÄÏùº]
‚Ä¢ Ï≤´ Î¨∏Ïû•Î∂ÄÌÑ∞ ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Ïóê ÏßÅÏ†ë ÎãµÎ≥Ä - Ïã†ÏÉÅ ÏÜåÍ∞ú NO
‚Ä¢ ÏÇ¨Ï£ºÏôÄ Ï†êÏÑ±Ïà† ÌÜµÏ∞∞ÏùÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎÖπÏó¨ÏÑú ÏÑ§Î™Ö
‚Ä¢ 'Ïôú Í∑∏Îü∞ÏßÄ' Ïù¥Ïú†Î•º ÏÉÅÏÑ∏Ìûà ÌíÄÏñ¥ÏÑú ÏÑ§Î™Ö
‚Ä¢ Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ†Ïßú/ÏãúÍ∏∞ Î∞òÎìúÏãú Ìè¨Ìï®
‚Ä¢ Ïã§Ï≤ú Í∞ÄÎä•Ìïú Íµ¨Ï≤¥Ï†Å Ï°∞Ïñ∏ Ï†úÍ≥µ
‚Ä¢ Ïúµ Ïã¨Î¶¨Ìïô Ïù∏Ïö©Ïù¥ ÏûàÏúºÎ©¥ 1-2Î¨∏Ïû• ÏûêÏó∞Ïä§ÎüΩÍ≤å ÌôúÏö© (Îî±Îî±ÌïòÍ≤å Ïù∏Ïö© X)

‚ùå Ï†àÎåÄ Í∏àÏßÄ:
‚Ä¢ Ïù∏ÏÇ¨/ÌôòÏòÅ Î©òÌä∏ ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Îã§Ïãú Ï∞æÏïÑÏ£ºÏÖ®ÎÑ§Ïöî")
‚Ä¢ Ïã†ÏÉÅ ÏÜåÍ∞ú ("ÏùºÍ∞ÑÏù¥ XÏûÖÎãàÎã§", "ÎãπÏã†ÏùÄ Y ÏÑ±Ìñ•" Îì±)
‚Ä¢ ÎåÄÏö¥/ÏÑ∏Ïö¥ ÏßÄÏñ¥ÎÇ¥Í∏∞ (ÏúÑ Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÎäî Í≤É Ïñ∏Í∏â)
‚Ä¢ Ï∂îÏÉÅÏ†Å ÎßêÎßå ÎÇòÏó¥ (Íµ¨Ï≤¥Ï†Å ÏãúÍ∏∞ ÏóÜÏù¥)
‚Ä¢ ÌîºÏÉÅÏ†ÅÏù¥Í≥† ÏßßÏùÄ ÎãµÎ≥Ä

üìå ÏùëÎãµ Í∏∏Ïù¥: 400-600Îã®Ïñ¥Î°ú Ï∂©Î∂ÑÌûà ÏÉÅÏÑ∏ÌïòÍ≤å ({locale})"""

        return system_prompt

    def _build_emotion_context(self, prompt: str) -> str:
        """Build emotion tracking context."""
        emotion_context = ""
        if not prompt:
            return emotion_context

        prompt_lower = prompt.lower()

        # Detect emotional indicators
        emotions_detected = []
        if any(k in prompt_lower for k in ["ÌûòÎì§", "ÏßÄÏ≥ê", "ÌîºÍ≥§", "ÏßÄÏπ®"]):
            emotions_detected.append("exhausted")
        if any(k in prompt_lower for k in ["Ïö∞Ïö∏", "Ïä¨Ìçº", "ÎààÎ¨º", "Ïö∏Í≥†"]):
            emotions_detected.append("sad")
        if any(k in prompt_lower for k in ["Î∂àÏïà", "Í±±Ï†ï", "ÎëêÎ†§", "Î¨¥ÏÑú"]):
            emotions_detected.append("anxious")
        if any(k in prompt_lower for k in ["ÌôîÎÇò", "ÏßúÏ¶ù", "ÏñµÏö∏", "Î∂ÑÎÖ∏"]):
            emotions_detected.append("angry")
        if any(k in prompt_lower for k in ["Ïô∏Î°ú", "ÌòºÏûê", "Í≥†ÎèÖ"]):
            emotions_detected.append("lonely")
        if any(k in prompt_lower for k in ["ÏÑ§Î†à", "Í∏∞ÎåÄ", "ÌñâÎ≥µ", "Ï¢ãÏïÑ"]):
            emotions_detected.append("hopeful")
        if any(k in prompt_lower for k in ["ÌòºÎûÄ", "Î™®Î•¥Í≤†", "Ïñ¥ÎñªÍ≤å", "Î≠ò Ìï¥Ïïº"]):
            emotions_detected.append("confused")

        if emotions_detected:
            emotion_map = {
                "exhausted": "ÏßÄÏπ®/ÌîºÎ°ú",
                "sad": "Ïä¨Ìîî/Ïö∞Ïö∏",
                "anxious": "Î∂àÏïà/Í±±Ï†ï",
                "angry": "Î∂ÑÎÖ∏/ÎãµÎãµ",
                "lonely": "Ïô∏Î°úÏõÄ",
                "hopeful": "Ìù¨Îßù/ÏÑ§Î†ò",
                "confused": "ÌòºÎûÄ/Î∞©Ìñ•ÏÉÅÏã§"
            }
            detected_ko = [emotion_map.get(e, e) for e in emotions_detected]
            emotion_context = f"\n[üí≠ Í∞êÏßÄÎêú Í∞êÏ†ï ÏÉÅÌÉú: {', '.join(detected_ko)}]\n‚Üí Ïù¥ Í∞êÏ†ïÏùÑ Î®ºÏ†Ä Ïù∏Ï†ïÌïòÍ≥† Í≥µÍ∞êÌïòÏÑ∏Ïöî. ÏÑ±Í∏âÌûà Ìï¥Í≤∞Ï±ÖÏúºÎ°ú ÎÑòÏñ¥Í∞ÄÏßÄ ÎßàÏÑ∏Ïöî.\n"
            logger.info(f"[StreamingService] Emotion detected: {emotions_detected}")

        return emotion_context
