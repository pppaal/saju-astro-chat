"""
Streaming Service

Business logic for streaming AI responses with Server-Sent Events (SSE).
Handles fortune telling, dream interpretation, and counseling with real-time streaming.

Moved from app.py to separate business logic from routing.
"""
import logging
import os
import time
from datetime import datetime
from typing import Dict, Any, Optional, Generator, List
from flask import Response, stream_with_context, g

logger = logging.getLogger(__name__)


class StreamingService:
    """
    Streaming service for AI-powered fortune telling with SSE.

    Methods:
        stream_fortune: Streaming fortune calculation with RAG and context
    """

    def __init__(self):
        """Initialize Streaming Service."""
        pass

    def stream_fortune(
        self,
        saju_data: Dict[str, Any],
        astro_data: Dict[str, Any],
        advanced_astro: Dict[str, Any],
        birth_data: Dict[str, Any],
        theme: str = "chat",
        locale: str = "ko",
        prompt: str = "",
        session_id: Optional[str] = None,
        conversation_history: List[Dict[str, Any]] = None,
        user_context: Dict[str, Any] = None,
        cv_text: str = "",
        request_id: Optional[str] = None
    ) -> Response:
        """
        Stream fortune telling with SSE (Server-Sent Events).

        This is the core business logic moved from app.py ask_stream() function.
        Logic remains 100% identical to ensure compatibility.

        Args:
            saju_data: Saju birth chart data
            astro_data: Astrology birth chart data
            advanced_astro: Advanced astrology features
            birth_data: Normalized birth data
            theme: Fortune theme (chat, career, love, etc.)
            locale: Language locale (en, ko)
            prompt: User's question/prompt (may be structured from frontend)
            session_id: Optional session ID for pre-fetched RAG data
            conversation_history: Previous messages for context
            user_context: Premium user context (persona, sessions, personality type)
            cv_text: CV/Resume text for career consultations
            request_id: Optional request ID for logging

        Returns:
            Flask Response with SSE stream (text/event-stream)
        """
        try:
            # Import dependencies (lazy loaded in app.py)
            from backend_ai.app.sanitizer import (
                is_suspicious_input,
                sanitize_user_input,
                sanitize_messages,
            )
            from backend_ai.app.app import (
                normalize_day_master,
                _has_saju_payload,
                _has_astro_payload,
                _calculate_simple_saju,
                validate_birth_data,
                _build_missing_payload_message,
                _build_birth_format_message,
                _sse_error_response,
                _build_detailed_saju,
                _build_detailed_astro,
                _build_advanced_astro_context,
                get_cross_analysis_for_chart,
                get_session_rag_cache,
                get_lifespan_guidance,
                get_theme_fusion_rules,
                get_active_imagination_prompts,
                _is_truthy,
                _bool_env,
                _get_int_env,
                _add_months,
                _format_date_ymd,
                _ensure_ko_prefix,
                _build_missing_requirements_addendum,
                _insert_addendum,
                _build_rag_debug_addendum,
                _format_korean_spacing,
                _to_sse_event,
                _chunk_text,
                _get_stream_chunk_size,
                _select_model_and_temperature,
                _clamp_temperature,
                _coerce_float,
            )

            # Crisis detection (if available)
            HAS_COUNSELING = False
            CrisisDetector = None
            try:
                from backend_ai.counseling.crisis import CrisisDetector
                HAS_COUNSELING = True
            except ImportError:
                pass

            # Corpus RAG (Jung psychology quotes)
            HAS_CORPUS_RAG = False
            get_corpus_rag = None
            try:
                from backend_ai.app.app import get_corpus_rag
                HAS_CORPUS_RAG = True
            except ImportError:
                pass

            if conversation_history is None:
                conversation_history = []
            if user_context is None:
                user_context = {}

            logger.info(f"[StreamingService] request_id={request_id} theme={theme} locale={locale} session={session_id or 'none'} history_len={len(conversation_history)} has_user_ctx={bool(user_context)} cv_len={len(cv_text)}")

            # Input validation - check for suspicious patterns
            if is_suspicious_input(prompt):
                logger.warning(f"[StreamingService] Suspicious input detected: {prompt[:100]}...")

            # Detect if frontend already sent a fully structured prompt
            is_frontend_structured = (
                "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ìš´ëª… ìƒë‹´ì‚¬" in prompt or
                "You are a warm, professional destiny counselor" in prompt or
                "[ì‚¬ì£¼/ì ì„± ê¸°ë³¸ ë°ì´í„°]" in prompt or
                "â˜…â˜…â˜… í•µì‹¬ ê·œì¹™ â˜…â˜…â˜…" in prompt
            )

            # Sanitize prompt
            sanitized_prompt = sanitize_user_input(prompt, max_length=8000 if is_frontend_structured else 1500, allow_newlines=True)

            # Debug flags
            debug_rag = _is_truthy(os.getenv("RAG_DEBUG_RESPONSE", ""))
            debug_log = _is_truthy(os.getenv("RAG_DEBUG_LOG", "")) or debug_rag

            # Extract current user question
            current_user_question = ""
            if "ì§ˆë¬¸:" in sanitized_prompt:
                current_user_question = sanitized_prompt.split("ì§ˆë¬¸:")[-1].strip()[:500]
            elif "Q:" in sanitized_prompt:
                current_user_question = sanitized_prompt.split("Q:")[-1].strip()[:500]
            else:
                current_user_question = sanitized_prompt[-500:] if sanitized_prompt else ""

            if is_frontend_structured:
                logger.info(f"[StreamingService] Detected STRUCTURED frontend prompt (len={len(prompt)})")

            # Normalize dayMaster structure (nested -> flat)
            saju_data = normalize_day_master(saju_data)

            logger.info(f"[StreamingService] saju dayMaster: {saju_data.get('dayMaster', {})}")

            # Check for pre-fetched RAG data from session
            session_cache = None
            session_rag_data = {}
            persona_context = {}
            rag_context = ""

            if session_id:
                session_cache = get_session_rag_cache(session_id)
                if session_cache:
                    logger.info(f"[StreamingService] Using pre-fetched session data for {session_id}")
                    # Use cached saju/astro if not provided in request
                    if not saju_data:
                        saju_data = session_cache.get("saju_data", {})
                    if not astro_data:
                        astro_data = session_cache.get("astro_data", {})

                    # Build rich RAG context from pre-fetched data
                    rag_data = session_cache.get("rag_data", {})
                    session_rag_data = rag_data

                    # GraphRAG context
                    if rag_data.get("graph_nodes"):
                        rag_context += "\n[ğŸ“Š ê´€ë ¨ ì§€ì‹ ê·¸ë˜í”„]\n"
                        rag_context += "\n".join(rag_data["graph_nodes"][:8])

                    # Jung quotes
                    if rag_data.get("corpus_quotes"):
                        rag_context += "\n\n[ğŸ“š ê´€ë ¨ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©]\n"
                        for q in rag_data["corpus_quotes"][:3]:
                            rag_context += f"â€¢ {q.get('text_ko', q.get('text_en', ''))} ({q.get('source', '')})\n"

                    # Persona insights
                    persona_context = rag_data.get("persona_context", {})
                    if persona_context.get("jung"):
                        rag_context += "\n[ğŸ§  ë¶„ì„ê°€ ê´€ì ]\n"
                        rag_context += "\n".join(f"â€¢ {i}" for i in persona_context["jung"][:3])
                    if persona_context.get("stoic"):
                        rag_context += "\n\n[âš”ï¸ ìŠ¤í† ì•„ ì² í•™ ê´€ì ]\n"
                        rag_context += "\n".join(f"â€¢ {i}" for i in persona_context["stoic"][:3])

                    logger.info(f"[StreamingService] RAG context from session: {len(rag_context)} chars")
                else:
                    logger.warning(f"[StreamingService] Session {session_id} not found or expired")

            # Allow birth-only computation if enabled
            allow_birth_compute = _bool_env("ALLOW_BIRTH_ONLY")
            if allow_birth_compute and (not _has_saju_payload(saju_data)) and birth_data.get("date") and birth_data.get("time"):
                try:
                    saju_data = _calculate_simple_saju(
                        birth_data["date"],
                        birth_data["time"],
                    )
                    saju_data = normalize_day_master(saju_data)
                    logger.info(f"[StreamingService] Computed simple saju from birth: {saju_data.get('dayMaster', {})}")
                except Exception as e:
                    logger.warning(f"[StreamingService] Failed to compute simple saju: {e}")

            # Validate required payloads
            has_saju_payload = _has_saju_payload(saju_data)
            has_astro_payload = _has_astro_payload(astro_data)
            require_computed_payload = _is_truthy(os.getenv("REQUIRE_COMPUTED_PAYLOAD", "1"))

            if require_computed_payload and (not has_saju_payload or not has_astro_payload):
                if birth_data.get("date") or birth_data.get("time"):
                    valid_birth, _err = validate_birth_data(birth_data.get("date"), birth_data.get("time"))
                    if not valid_birth:
                        logger.warning("[StreamingService] Invalid birth format for missing payload")
                        return _sse_error_response(_build_birth_format_message(locale))

                missing_message = _build_missing_payload_message(
                    locale,
                    missing_saju=not has_saju_payload,
                    missing_astro=not has_astro_payload,
                )
                logger.warning("[StreamingService] Missing computed payload(s)")
                return _sse_error_response(missing_message)

            # Build DETAILED chart context (not just summary)
            saju_detail = _build_detailed_saju(saju_data)
            astro_detail = _build_detailed_astro(astro_data)
            advanced_astro_detail = _build_advanced_astro_context(advanced_astro)

            logger.info(f"[StreamingService] saju_detail length: {len(saju_detail)}")
            logger.info(f"[StreamingService] astro_detail length: {len(astro_detail)}")
            if advanced_astro_detail:
                logger.info(f"[StreamingService] advanced_astro_detail length: {len(advanced_astro_detail)}")

            # Get cross-analysis (from session or instant lookup)
            cross_rules = ""
            if session_cache and session_cache.get("rag_data", {}).get("cross_analysis"):
                cross_rules = session_cache["rag_data"]["cross_analysis"]
            else:
                try:
                    cross_rules = get_cross_analysis_for_chart(saju_data, astro_data, theme, locale)
                    if cross_rules:
                        logger.info(f"[StreamingService] Instant cross-analysis: {len(cross_rules)} chars, theme={theme}")
                except Exception as e:
                    logger.warning(f"[StreamingService] Cross-analysis lookup failed: {e}")

            # Get Jung/Stoic insights if not from session (instant lookup)
            instant_quotes = []
            if not rag_context and HAS_CORPUS_RAG and get_corpus_rag:
                try:
                    _corpus_rag_inst = get_corpus_rag()
                    if _corpus_rag_inst:
                        # Build query from user context
                        theme_concepts = {
                            "career": "vocation calling purpose ì†Œëª… ì§ì—… ìì•„ì‹¤í˜„",
                            "love": "anima animus relationship ê´€ê³„ ì‚¬ë‘ ê·¸ë¦¼ì",
                            "health": "healing wholeness ì¹˜ìœ  í†µí•©",
                            "life_path": "individuation meaning ê°œì„±í™” ì˜ë¯¸ ì„±ì¥",
                            "family": "complex archetype ì½¤í”Œë ‰ìŠ¤ ì›í˜• ê°€ì¡±",
                        }
                        jung_query = f"{theme_concepts.get(theme, theme)} {sanitized_prompt[:50] if sanitized_prompt else ''}"
                        quotes = _corpus_rag_inst.search(jung_query, top_k=3, min_score=0.15)
                        if quotes:
                            instant_quotes = quotes
                            rag_context += "\n\n[ğŸ“š ìœµ ì‹¬ë¦¬í•™ í†µì°°]\n"
                            for q in quotes[:2]:
                                quote_text = q.get('quote_kr') or q.get('quote_en', '')
                                if quote_text:
                                    rag_context += f"â€¢ \"{quote_text[:150]}...\" â€” ì¹¼ ìœµ, {q.get('source', '')}\n"
                            logger.info(f"[StreamingService] Instant Jung quotes: {len(quotes)} found")
                except Exception as e:
                    logger.debug(f"[StreamingService] Instant Jung quotes failed: {e}")

            # Build cross-analysis section
            cross_section = ""
            if cross_rules:
                cross_section = f"\n[ì‚¬ì£¼+ì ì„± êµì°¨ í•´ì„ ê·œì¹™]\n{cross_rules}\n"

            # Current date for time-relevant advice
            now = datetime.now()
            today_date = now.date()
            six_month_date = _add_months(today_date, 6)
            weekdays_ko = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
            current_date_str = f"ì˜¤ëŠ˜: {now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekdays_ko[now.weekday()]})"
            timing_window_str = (
                f"íƒ€ì´ë° ê¸°ì¤€: {_format_date_ymd(today_date)} ~ {_format_date_ymd(six_month_date)}"
                if locale == "ko"
                else f"Timing window: {_format_date_ymd(today_date)} to {_format_date_ymd(six_month_date)}"
            )

            # Build user context section for returning users (premium feature)
            user_context_section = self._build_user_context_section(user_context, locale)

            # Build CV/Resume section
            cv_section = ""
            if cv_text:
                cv_section = f"""
[ğŸ“„ ì‚¬ìš©ì ì´ë ¥ì„œ/CV]
{cv_text}

â†’ ìœ„ ì´ë ¥ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ê²½ë ¥, ê¸°ìˆ , ê²½í—˜ì— ë§ëŠ” êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
â†’ ì‚¬ì£¼/ì ì„± í•´ì„ê³¼ ì´ë ¥ì„œ ë‚´ìš©ì„ ì—°ê²°í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.
â†’ ì»¤ë¦¬ì–´, ì§ì—…, ì ì„± ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ì´ë ¥ì„œ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
"""
                logger.info(f"[StreamingService] CV section added: {len(cv_text)} chars, theme={theme}")

            # Lifespan guidance
            lifespan_section = self._build_lifespan_section(birth_data, saju_data)

            # Theme fusion rules
            theme_fusion_section = self._build_theme_fusion_section(saju_data, astro_data, theme, locale, birth_data)

            # Active imagination prompts
            imagination_section = self._build_imagination_section(sanitized_prompt)

            # Crisis detection
            crisis_response = None
            crisis_check = {"is_crisis": False, "max_severity": "none", "requires_immediate_action": False}
            if HAS_COUNSELING and CrisisDetector and current_user_question:
                crisis_check = CrisisDetector.detect_crisis(current_user_question)
                if crisis_check["is_crisis"]:
                    logger.warning(f"[StreamingService] Crisis detected! severity={crisis_check['max_severity']}")
                    crisis_response = CrisisDetector.get_crisis_response(
                        crisis_check["max_severity"],
                        locale=locale
                    )
                    if crisis_check["requires_immediate_action"]:
                        # Return safety response immediately via SSE
                        def crisis_generator():
                            msg = crisis_response.get("immediate_message", "")
                            if crisis_response.get("follow_up"):
                                msg += "\n\n" + crisis_response["follow_up"]
                            if crisis_response.get("closing"):
                                msg += "\n\n" + crisis_response["closing"]
                            yield f"data: {msg}\n\n"
                            yield "data: [DONE]\n\n"

                        return Response(
                            stream_with_context(crisis_generator()),
                            mimetype="text/event-stream",
                            headers={
                                "Cache-Control": "no-cache",
                                "Connection": "keep-alive",
                                "X-Accel-Buffering": "no",
                            }
                        )

            # Build crisis context for medium/medium_high severity
            crisis_context_section = self._build_crisis_context(crisis_response, crisis_check)

            # Build therapeutic context based on question type
            therapeutic_section = self._build_therapeutic_section(sanitized_prompt, locale, HAS_COUNSELING)

            # RAG metadata for debugging
            rag_meta = {}
            if debug_rag or debug_log:
                rag_meta = {
                    "enabled": True,
                    "theme": theme,
                    "question": current_user_question[:120],
                    "graph_nodes": len(session_rag_data.get("graph_nodes", [])),
                    "corpus_quotes": len(session_rag_data.get("corpus_quotes", [])) or len(instant_quotes),
                    "persona_jung": len(persona_context.get("jung", [])),
                    "persona_stoic": len(persona_context.get("stoic", [])),
                    "cross_analysis": bool(cross_rules),
                    "theme_fusion": bool(theme_fusion_section),
                    "lifespan": bool(lifespan_section),
                    "therapeutic": bool(therapeutic_section),
                    "session_rag": bool(session_cache),
                }
                if debug_log:
                    logger.info(
                        "[RAG-DEBUG] theme=%s q=%s graph=%s corpus=%s persona=%s cross=%s fusion=%s session=%s",
                        theme,
                        current_user_question[:80],
                        rag_meta["graph_nodes"],
                        rag_meta["corpus_quotes"],
                        rag_meta["persona_jung"] + rag_meta["persona_stoic"],
                        rag_meta["cross_analysis"],
                        rag_meta["theme_fusion"],
                        rag_meta["session_rag"],
                    )

            # Build system prompt
            system_prompt = self._build_system_prompt(
                is_frontend_structured=is_frontend_structured,
                locale=locale,
                theme=theme,
                timing_window_str=timing_window_str,
                current_date_str=current_date_str,
                rag_context=rag_context,
                cross_rules=cross_rules,
                saju_detail=saju_detail,
                astro_detail=astro_detail,
                advanced_astro_detail=advanced_astro_detail,
                cross_section=cross_section,
                user_context_section=user_context_section,
                cv_section=cv_section,
                lifespan_section=lifespan_section,
                theme_fusion_section=theme_fusion_section,
                imagination_section=imagination_section,
                crisis_context_section=crisis_context_section,
                therapeutic_section=therapeutic_section,
                sanitized_prompt=sanitized_prompt,
            )

            # Emotion tracking
            emotion_context = self._build_emotion_context(sanitized_prompt)
            if emotion_context:
                system_prompt = system_prompt.replace("[ğŸ“ ì‘ë‹µ êµ¬ì¡°]", f"{emotion_context}\n[ğŸ“ ì‘ë‹µ êµ¬ì¡°]")

            # Generate streaming response
            def generate():
                """SSE generator for streaming response."""
                try:
                    from openai import OpenAI
                    import httpx
                    client = OpenAI(
                        api_key=os.getenv("OPENAI_API_KEY"),
                        timeout=httpx.Timeout(60.0, connect=10.0)
                    )

                    # Build messages with conversation history
                    messages = [{"role": "system", "content": system_prompt}]

                    # Add conversation history - increased limit for better context
                    history_limit = 12  # 6 user + 6 assistant messages
                    recent_history = conversation_history[-history_limit:] if conversation_history else []

                    # Generate conversation summary for long sessions (>6 messages)
                    conversation_summary = ""
                    if len(conversation_history) > 6:
                        # Extract key topics from older messages
                        older_msgs = conversation_history[:-6]
                        topics = []
                        for m in older_msgs:
                            if m.get("role") == "user" and m.get("content"):
                                content = m["content"][:100]
                                if any(k in content for k in ["ì—°ì• ", "ì‚¬ë‘", "ê²°í˜¼"]):
                                    topics.append("ì—°ì• /ê´€ê³„")
                                elif any(k in content for k in ["ì·¨ì—…", "ì´ì§", "ì»¤ë¦¬ì–´", "ì§„ë¡œ"]):
                                    topics.append("ì»¤ë¦¬ì–´/ì§„ë¡œ")
                                elif any(k in content for k in ["í˜ë“¤", "ìš°ìš¸", "ì§€ì³"]):
                                    topics.append("ê°ì •ì  ì–´ë ¤ì›€")
                                elif any(k in content for k in ["ë‚˜ëŠ”", "ì„±ê²©", "ì–´ë–¤ ì‚¬ëŒ"]):
                                    topics.append("ìê¸°íƒìƒ‰")
                        if topics:
                            unique_topics = list(dict.fromkeys(topics))[:3]
                            conversation_summary = f"[ğŸ“‹ ì´ì „ ëŒ€í™” ìš”ì•½: {', '.join(unique_topics)} ì£¼ì œë¡œ ëŒ€í™”í•¨]\n"

                    # Add summary if available
                    if conversation_summary:
                        messages.append({
                            "role": "system",
                            "content": conversation_summary
                        })

                    # Smart truncation: recent messages get more space
                    for idx, msg in enumerate(recent_history):
                        if msg.get("role") in ("user", "assistant") and msg.get("content"):
                            # Older messages: shorter, Recent messages: longer
                            is_recent = idx >= len(recent_history) - 4
                            max_len = 800 if is_recent else 300
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"][:max_len]
                            })

                    # Add current user message
                    messages.append({"role": "user", "content": sanitized_prompt})

                    # Model selection
                    default_model = os.getenv("CHAT_MODEL") or os.getenv("FUSION_MODEL") or "gpt-4.1"
                    default_temp = _clamp_temperature(_coerce_float(os.getenv("CHAT_TEMPERATURE")), 0.75)
                    model_name, temperature, ab_variant = _select_model_and_temperature(
                        {"session_id": session_id},
                        default_model,
                        default_temp,
                        session_id,
                        request_id,
                    )

                    if debug_rag or debug_log:
                        rag_meta["model"] = model_name
                        rag_meta["temperature"] = temperature
                        rag_meta["ab_variant"] = ab_variant or ""

                    max_tokens = _get_int_env("ASK_STREAM_MAX_TOKENS", 1600, min_value=400, max_value=4000)

                    # Stream from OpenAI
                    stream = client.chat.completions.create(
                        model=model_name,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        stream=True
                    )

                    full_text = ""

                    for chunk in stream:
                        if not chunk.choices or not chunk.choices[0].delta.content:
                            continue
                        token = chunk.choices[0].delta.content
                        full_text += token
                        yield _to_sse_event(token)

                    # Post-processing: send as final events
                    full_text = _ensure_ko_prefix(full_text, locale)

                    if full_text.strip().startswith("[ERROR]") or not full_text.strip():
                        yield "data: [DONE]\n\n"
                        return

                    # Build addendum
                    addendum = _build_missing_requirements_addendum(
                        full_text,
                        locale,
                        saju_data,
                        astro_data,
                        today_date,
                    )
                    if addendum:
                        yield _to_sse_event(f"\n\n{addendum}")

                    # Add RAG debug info
                    if debug_rag:
                        debug_addendum = _build_rag_debug_addendum(rag_meta, locale)
                        if debug_addendum:
                            yield _to_sse_event(f"\n\n{debug_addendum}")

                    # Add follow-up question if missing
                    if locale == "ko" and not full_text.rstrip().endswith("?"):
                        followup = " í˜¹ì‹œ ì§€ê¸ˆ ê°€ì¥ ê¶ê¸ˆí•œ í¬ì¸íŠ¸ê°€ ë­ì˜ˆìš”?"
                        yield _to_sse_event(followup)

                    # Signal end of stream
                    yield "data: [DONE]\n\n"

                except Exception as e:
                    logger.error(f"[StreamingService] Streaming error: {e}")
                    yield "data: [ERROR] ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"

            return Response(
                stream_with_context(generate()),
                mimetype="text/event-stream",
                headers={
                    "Content-Type": "text/event-stream; charset=utf-8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",  # Disable nginx buffering
                }
            )

        except Exception as e:
            logger.exception(f"[StreamingService] request_id={request_id} stream_fortune failed: {e}")
            from flask import jsonify
            return jsonify({"status": "error", "message": str(e)}), 500

    def _build_user_context_section(self, user_context: Dict[str, Any], locale: str) -> str:
        """Build user context section for returning users (premium feature)."""
        if not user_context:
            return ""

        user_context_section = ""
        persona = user_context.get("persona", {})
        recent_sessions = user_context.get("recentSessions", [])
        personality_type = user_context.get("personalityType", {})

        # Nova Personality Type (from personality quiz)
        if personality_type.get("typeCode"):
            type_code = personality_type["typeCode"]
            type_name = personality_type.get("personaName", "")
            user_context_section = f"\n[ğŸ­ ì‚¬ìš©ì ì„±ê²© ìœ í˜•: {type_code}]\n"
            if type_name:
                user_context_section += f"â€¢ ìœ í˜•ëª…: {type_name}\n"

            # Lookup archetype details for counseling approach
            archetype_hints = {
                "RVLA": "ì „ëµì  ê´€ì ì—ì„œ ì¡°ì–¸. í° ê·¸ë¦¼ê³¼ ì‹¤í–‰ ê³„íš ì œì‹œ. ì§ì ‘ì  ì†Œí†µ ì„ í˜¸.",
                "RVLF": "ë‹¤ì–‘í•œ ì˜µì…˜ê³¼ ê°€ëŠ¥ì„± ì œì‹œ. ì‹¤í—˜ì  ì ‘ê·¼ ê¶Œì¥. ì°½ì˜ì  í•´ê²°ì±… íƒìƒ‰.",
                "RVHA": "ë¹„ì „ê³¼ ì˜ë¯¸ ì—°ê²°. ìŠ¤í† ë¦¬í…”ë§ í™œìš©. ë™ê¸°ë¶€ì—¬ ì¤‘ì‹¬ ì¡°ì–¸.",
                "RVHF": "ì˜ê°ê³¼ ìƒˆë¡œìš´ ê´€ì  ì œì‹œ. ì‚¬íšŒì  ì—°ê²° ê°•ì¡°. ì—´ì •ì  ì†Œí†µ.",
                "RSLA": "ëª…í™•í•œ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš ì œì‹œ. ì±…ì„ê³¼ ê²°ê³¼ ê°•ì¡°. íš¨ìœ¨ì  ì¡°ì–¸.",
                "RSLF": "ì¦‰ê°ì ì´ê³  ì‹¤ìš©ì ì¸ í•´ê²°ì±…. í˜„ì¥ ê°ê° í™œìš©. ìœ„ê¸° ëŒ€ì‘ ê´€ì .",
                "RSHA": "ê´€ê³„ì™€ ì„±ì¥ ì¤‘ì‹¬. ë”°ëœ»í•˜ë©´ì„œë„ ì²´ê³„ì . ì•ˆì •ì  í™˜ê²½ ê°•ì¡°.",
                "RSHF": "ì°¸ì—¬ì™€ ì†Œí†µ ì¤‘ì‹¬. ë‹¤ì–‘í•œ ê´€ì  í¬ìš©. í•¨ê»˜í•˜ëŠ” í•´ê²°.",
                "GVLA": "ê·¼ë³¸ ì›ì¸ ë¶„ì„. ì¥ê¸°ì  ê´€ì . ì²´ê³„ì ì´ê³  ê¹Šì€ í•´ê²°ì±….",
                "GVLF": "ë°ì´í„°ì™€ ì¦ê±° ê¸°ë°˜ ì ‘ê·¼. ì²´ê³„ì  ë¶„ì„. íŒ¨í„´ í™œìš©.",
                "GVHA": "ì„±ì¥ê³¼ ë°œì „ ì¤‘ì‹¬. ì¥ê¸°ì  ê´€ê³„. ë©˜í† ë§ ê´€ì .",
                "GVHF": "ì˜ë¯¸ì™€ ëª©ì  ì—°ê²°. ê¹Šì€ ì§ˆë¬¸. ì§„ì •ì„± ìˆëŠ” ëŒ€í™”.",
                "GSLA": "ë‹¨ê³„ë³„ ê²€ì¦ëœ ë°©ë²• ê¶Œì¥. ì•ˆì •ì  ì‹¤í–‰ ì§€ì›. ë¦¬ìŠ¤í¬ ê´€ë¦¬.",
                "GSLF": "ë¬¸ì œë¥¼ ì‘ê²Œ ë¶„í•´. í•˜ë‚˜ì”© ì²´ê³„ì  í•´ê²°. ì •ë°€í•œ ì ‘ê·¼.",
                "GSHA": "ì•ˆì •ê³¼ ì‹ ë¢° ê¸°ë°˜. ì ì§„ì  ë³€í™” ê¶Œì¥. ê¾¸ì¤€í•œ ì§€ì›.",
                "GSHF": "ê°ˆë“± í•´ì†Œì™€ ì¡°í™” ì¤‘ì‹¬. ê²½ì²­ê³¼ ì¤‘ì¬. ë¶€ë“œëŸ¬ìš´ ì ‘ê·¼.",
            }
            if type_code in archetype_hints:
                user_context_section += f"â€¢ ìƒë‹´ ìŠ¤íƒ€ì¼: {archetype_hints[type_code]}\n"

            user_context_section += "\nâ†’ ì´ ì‚¬ìš©ìì˜ ì„±ê²© ìœ í˜•ì— ë§ê²Œ ì†Œí†µ ìŠ¤íƒ€ì¼ì„ ì¡°ì ˆí•˜ì„¸ìš”.\n"
            logger.info(f"[StreamingService] Personality type: {type_code}")

        if persona.get("sessionCount", 0) > 0 or recent_sessions:
            if not user_context_section:
                user_context_section = "\n[ğŸ”„ ì´ì „ ìƒë‹´ ë§¥ë½]\n"
            else:
                user_context_section += "\n[ğŸ”„ ì´ì „ ìƒë‹´ ë§¥ë½]\n"

            # Persona memory
            if persona.get("sessionCount"):
                user_context_section += f"â€¢ ì´ {persona['sessionCount']}íšŒ ìƒë‹´í•œ ì¬ë°©ë¬¸ ê³ ê°\n"
            if persona.get("lastTopics"):
                topics = persona["lastTopics"][:3] if isinstance(persona["lastTopics"], list) else []
                if topics:
                    user_context_section += f"â€¢ ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(topics)}\n"
            if persona.get("emotionalTone"):
                user_context_section += f"â€¢ ê°ì • ìƒíƒœ: {persona['emotionalTone']}\n"
            if persona.get("recurringIssues"):
                issues = persona["recurringIssues"][:2] if isinstance(persona["recurringIssues"], list) else []
                if issues:
                    user_context_section += f"â€¢ ë°˜ë³µ ì´ìŠˆ: {', '.join(issues)}\n"

            # Recent session summaries
            if recent_sessions:
                user_context_section += "\n[ìµœê·¼ ëŒ€í™”]\n"
                for sess in recent_sessions[:2]:  # Last 2 sessions
                    if sess.get("summary"):
                        user_context_section += f"â€¢ {sess['summary']}\n"
                    elif sess.get("keyTopics"):
                        topics_str = ", ".join(sess["keyTopics"][:3]) if isinstance(sess["keyTopics"], list) else ""
                        if topics_str:
                            user_context_section += f"â€¢ ì£¼ì œ: {topics_str}\n"

            user_context_section += "\nâ†’ ì¬ë°©ë¬¸ ê³ ê°ì´ë‹ˆ 'ë˜ ì˜¤ì…¨ë„¤ìš”' ê°™ì€ ì¹œê·¼í•œ ì¸ì‚¬ë¡œ ì‹œì‘í•˜ê³ , ì´ì „ ìƒë‹´ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì°¸ì¡°í•˜ì„¸ìš”.\n"
            logger.info(f"[StreamingService] User context section: {len(user_context_section)} chars")

        return user_context_section

    def _build_lifespan_section(self, birth_data: Dict[str, Any], saju_data: Dict[str, Any]) -> str:
        """Build lifespan guidance section (age-appropriate psychological tasks)."""
        from backend_ai.app.app import get_lifespan_guidance

        lifespan_section = ""
        birth_year = None
        try:
            # Extract birth year from birth_data or saju_data
            if birth_data.get("date"):
                birth_year = int(birth_data["date"].split("-")[0])
            elif saju_data.get("birthYear"):
                birth_year = int(saju_data["birthYear"])
        except (ValueError, KeyError, TypeError, AttributeError):
            pass

        if birth_year:
            lifespan_guidance = get_lifespan_guidance(birth_year)
            if lifespan_guidance and lifespan_guidance.get("stage_name"):
                stage = lifespan_guidance
                lifespan_section = f"""
[ğŸŒ± ìƒì• ì£¼ê¸°ë³„ ì‹¬ë¦¬ ê³¼ì œ: {stage['stage_name']} ({stage['age']}ì„¸)]
â€¢ ë°œë‹¬ ê³¼ì œ: {', '.join(stage.get('psychological_tasks', [])[:3])}
â€¢ í•µì‹¬ ì›í˜•: {stage.get('archetypal_themes', {}).get('primary', [''])[0] if isinstance(stage.get('archetypal_themes', {}).get('primary'), list) else ''}
â€¢ í”í•œ ìœ„ê¸°: {', '.join(stage.get('developmental_crises', stage.get('shadow_challenges', []))[:2])}
â€¢ ì‚¬ì£¼ ì—°ê²°: {stage.get('saju_parallel', {}).get('theme', '')}
â€¢ ì ì„± ì—°ê²°: {stage.get('astro_parallel', {}).get('theme', '')}

â†’ ì´ ìƒì•  ë‹¨ê³„ì— ë§ëŠ” ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”. ë‚˜ì´ì— ë§ì§€ ì•ŠëŠ” ì¡°ì–¸(ì˜ˆ: 20ëŒ€ì—ê²Œ 'ì€í‡´ ì¤€ë¹„')ì€ í”¼í•˜ì„¸ìš”.
"""
                logger.info(f"[StreamingService] Lifespan guidance: {stage['stage_name']} (age {stage['age']})")

        return lifespan_section

    def _build_theme_fusion_section(self, saju_data: Dict[str, Any], astro_data: Dict[str, Any],
                                   theme: str, locale: str, birth_data: Dict[str, Any]) -> str:
        """Build theme fusion rules section (daily/monthly/yearly guidance)."""
        from backend_ai.app.app import get_theme_fusion_rules

        theme_fusion_section = ""
        birth_year = None
        try:
            if birth_data.get("date"):
                birth_year = int(birth_data["date"].split("-")[0])
            elif saju_data.get("birthYear"):
                birth_year = int(saju_data["birthYear"])
        except (ValueError, KeyError, TypeError, AttributeError):
            pass

        try:
            theme_fusion = get_theme_fusion_rules(saju_data, astro_data, theme, locale, birth_year)
            if theme_fusion:
                theme_fusion_section = f"""
[ğŸ¯ í…Œë§ˆë³„ ìœµí•© í•´ì„]
{theme_fusion}

â†’ ìœ„ í…Œë§ˆë³„ í•´ì„ì„ ìƒë‹´ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì „ë‹¬í•˜ì„¸ìš”.
"""
                logger.info(f"[StreamingService] Theme fusion rules added: {len(theme_fusion)} chars, theme={theme}")
        except Exception as e:
            logger.warning(f"[StreamingService] Theme fusion rules failed: {e}")

        return theme_fusion_section

    def _build_imagination_section(self, prompt: str) -> str:
        """Build active imagination prompts section (deep therapeutic work)."""
        from backend_ai.app.app import get_active_imagination_prompts

        imagination_section = ""
        if prompt and any(k in prompt.lower() for k in ["ê¹Šì´", "ë‚´ë©´", "ë¬´ì˜ì‹", "ê·¸ë¦¼ì", "ëª…ìƒ", "ìƒìƒ"]):
            ai_prompts = get_active_imagination_prompts(prompt)
            if ai_prompts:
                imagination_section = f"""
[ğŸ¨ ì ê·¹ì  ìƒìƒ ê¸°ë²• - ì‹¬ì¸µ ì‘ì—…ìš©]
â€¢ ì‹œì‘ ì§ˆë¬¸: {ai_prompts.get('opening', [''])[0] if ai_prompts.get('opening') else ''}
â€¢ ì‹¬í™” ì§ˆë¬¸: {ai_prompts.get('deepening', [''])[0] if ai_prompts.get('deepening') else ''}
â€¢ í†µí•© ì§ˆë¬¸: {ai_prompts.get('integration', [''])[0] if ai_prompts.get('integration') else ''}

â†’ ì‚¬ìš©ìê°€ ê¹Šì€ ë‚´ë©´ ì‘ì—…ì„ ì›í•  ë•Œë§Œ ì´ ì§ˆë¬¸ë“¤ì„ í™œìš©í•˜ì„¸ìš”. ê°•ìš”í•˜ì§€ ë§ˆì„¸ìš”.
"""
                logger.info(f"[StreamingService] Active imagination prompts added")

        return imagination_section

    def _build_crisis_context(self, crisis_response: Optional[Dict], crisis_check: Dict) -> str:
        """Build crisis context section for medium/medium_high severity."""
        crisis_context_section = ""
        if crisis_response and not crisis_check.get("requires_immediate_action"):
            severity = crisis_check.get("max_severity", "")
            if severity == "medium_high":
                crisis_context_section = """
[âš ï¸ ì‚¬ìš©ì ê°ì • ìƒíƒœ: ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤]
- ê³µê°ê³¼ ì•ˆì •ê°ì„ ì£¼ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- ë¨¼ì € ê°ì •ì„ ì¸ì •í•˜ê³  í˜¸í¡/ê·¸ë¼ìš´ë”© ê¸°ë²•ì„ ì•ˆë‚´í•˜ì„¸ìš”
- ì ìˆ  í•´ì„ì€ í¬ë§ì ì¸ ê´€ì ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- í•„ìš”ì‹œ ì „ë¬¸ ìƒë‹´ ê¶Œìœ : ì •ì‹ ê±´ê°•ìœ„ê¸°ìƒë‹´ì „í™” 1577-0199
"""
            elif severity == "medium":
                crisis_context_section = """
[âš ï¸ ì‚¬ìš©ì ê°ì • ìƒíƒœ: í¬ë§ ì €í•˜]
- ê³µê°ê³¼ ë”°ëœ»í•¨ì„ ë‹´ì•„ ì‘ë‹µí•˜ì„¸ìš”
- ì‘ì€ í¬ë§ì´ë¼ë„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”
- ì ìˆ  í•´ì„ì—ì„œ ê¸ì •ì  ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•˜ì„¸ìš”
- "í˜¼ìê°€ ì•„ë‹ˆì—ìš”"ë¼ëŠ” ë©”ì‹œì§€ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ì„¸ìš”
"""
            logger.info(f"[StreamingService] Added crisis context for severity={severity}")

        return crisis_context_section

    def _build_therapeutic_section(self, prompt: str, locale: str, has_counseling: bool) -> str:
        """Build therapeutic guidance based on question type (Jung psychology enhanced)."""
        therapeutic_section = ""
        if not (has_counseling and prompt):
            return therapeutic_section

        prompt_lower = prompt.lower()

        # Detect question themes and add therapeutic guidance
        if any(k in prompt_lower for k in ["í˜ë“¤", "ìš°ìš¸", "ì§€ì³", "í¬ê¸°", "ì˜ë¯¸ì—†", "í—ˆë¬´"]):
            therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì˜ë¯¸/ì •ì„œ ì§€ì§€]
- ë¨¼ì € ê°ì •ì„ ì¶©ë¶„íˆ ì¸ì •: "ì •ë§ í˜ë“œì…¨ê² ì–´ìš”... ê·¸ ë¬´ê²Œë¥¼ í˜¼ì ì§€ê³  ê³„ì…¨êµ°ìš”"
- ìœµ ê´€ì : "ì˜í˜¼ì˜ ì–´ë‘ìš´ ë°¤(dark night of soul)"ì€ ë³€í™”ì˜ ì „ì¡°
- ì‚¬ì£¼/ì ì„±ì—ì„œ 'ì „í™˜ì 'ì´ë‚˜ 'ì„±ì¥ê¸°'ë¥¼ ì°¾ì•„ í¬ë§ ì—°ê²°
- ê·¸ë¦¼ì ì‘ì—…: "ì´ í˜ë“¦ì´ ë‹¹ì‹ ì—ê²Œ ê°€ë¥´ì¹˜ë ¤ëŠ” ê²Œ ìˆë‹¤ë©´?"
- ì‘ì€ ì•¡ì…˜ ì œì•ˆ: "ì˜¤ëŠ˜ í•˜ë‚˜ë§Œ ìì‹ ì„ ìœ„í•´ í•œë‹¤ë©´ ë­˜ í•˜ê³  ì‹¶ìœ¼ì„¸ìš”?"
"""
        elif any(k in prompt_lower for k in ["ì—°ì• ", "ì‚¬ë‘", "ê²°í˜¼", "ì´ë³„", "ì§ì‚¬ë‘", "ì¸"]):
            therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ê´€ê³„/ì‚¬ë‘]
- ê°ì •ì˜ ê¹Šì´ë¥¼ ì¸ì •: "ë§ˆìŒì´ ë§ì´ ì“°ì´ì‹œë„¤ìš”"
- ìœµ ê´€ì  - ì•„ë‹ˆë§ˆ/ì•„ë‹ˆë¬´ìŠ¤ íˆ¬ì‚¬: "ëŒë¦¬ëŠ” ê·¸ íŠ¹ì„±ì´ í˜¹ì‹œ ë‚´ ì•ˆì—ë„ ìˆë‹¤ë©´?"
- ê·¸ë¦¼ì íˆ¬ì‚¬: "ì‹«ì€ ê·¸ ì ... ë‚´ ê·¸ë¦¼ìëŠ” ì•„ë‹ê¹Œìš”?"
- ì‚¬ì£¼ ê´€ì„±(å®˜æ˜Ÿ)/ì ì„± ê¸ˆì„±-7í•˜ìš°ìŠ¤ í•´ì„ì„ ì‹¬ë¦¬ì  íŒ¨í„´ê³¼ ì—°ê²°
- ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬: "ìƒëŒ€ì—ê²Œ ì§„ì§œ ì›í•˜ëŠ” ê±´ ë­˜ê¹Œìš”?" / "ì™„ë²½í•œ ê´€ê³„ë€ ì–´ë–¤ ëª¨ìŠµì´ì—ìš”?"
"""
        elif any(k in prompt_lower for k in ["ì·¨ì—…", "ì´ì§", "ì§„ë¡œ", "ì‚¬ì—…", "í‡´ì‚¬", "ì»¤ë¦¬ì–´"]):
            therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì»¤ë¦¬ì–´/ì •ì²´ì„±]
- ë¶ˆì•ˆê° ì¸ì •: "ì¤‘ìš”í•œ ê²°ì • ì•ì—ì„œ ê³ ë¯¼ì´ ê¹Šìœ¼ì‹œë„¤ìš”"
- ìœµ ê´€ì  - ì†Œëª…(calling): "ëˆì„ ë– ë‚˜ì„œ, ì§„ì§œ í•˜ê³  ì‹¶ì€ ì¼ì€ ë­ì˜ˆìš”?"
- í˜ë¥´ì†Œë‚˜ vs ìê¸°(Self): "ì¼í•˜ëŠ” ë‚˜ vs ì§„ì§œ ë‚˜, ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€ìš”?"
- ì‚¬ì£¼ ì‹ìƒ/ì¬ì„±ê³¼ ì ì„± 10í•˜ìš°ìŠ¤/MC ì—°ê²°í•˜ì—¬ ì ì„± ë¶„ì„
- êµ¬ì²´ì  ì‹œê¸° ì œì‹œ: "2025ë…„ ìƒë°˜ê¸°ê°€ ì „í™˜ì " ì‹ìœ¼ë¡œ
- ì§ˆë¬¸: "ëˆ vs ë³´ëŒ, ì§€ê¸ˆ ë” ì¤‘ìš”í•œ ê±´?" / "5ë…„ ë’¤ ì–´ë–¤ ëª¨ìŠµì´ê³  ì‹¶ìœ¼ì„¸ìš”?"
"""
        elif any(k in prompt_lower for k in ["ì–¸ì œ", "ì‹œê¸°", "íƒ€ì´ë°", "ëª‡ ì›”", "ì˜¬í•´", "ë‚´ë…„"]):
            therapeutic_section = """
[ğŸ§  ì‹¬ë¦¬ìƒë‹´ ê°€ì´ë“œ: ì‹œê¸°/íƒ€ì´ë°]
- êµ¬ì²´ì  ì‹œê¸° ì œì‹œ í•„ìˆ˜: ì‚¬ì£¼ ëŒ€ìš´/ì„¸ìš´ + ì ì„± íŠ¸ëœì§“ ë¶„ì„
- ì›”/ë¶„ê¸° ë‹¨ìœ„ë¡œ ëª…í™•í•˜ê²Œ: "2025ë…„ 3-4ì›”ì´ ì¢‹ì•„ìš”"
- ì™œ ê·¸ ì‹œê¸°ì¸ì§€ ì„¤ëª…: "ëª©ì„±ì´ ~ì— ë“¤ì–´ì˜¤ë©´ì„œ..."
- ê·¸ ì‹œê¸°ì— í•  ì¼ ì œì•ˆ: "ì´ ì‹œê¸°ì— [êµ¬ì²´ì  í–‰ë™]ì„ ì‹œì‘í•˜ë©´ ì¢‹ê² ì–´ìš”"
- ì£¼ì˜í•  ì‹œê¸°ë„ í•¨ê»˜: "ë‹¤ë§Œ ~ì›”ì€ ì‹ ì¤‘í•˜ê²Œ"
"""

        return therapeutic_section

    def _build_system_prompt(self, **kwargs) -> str:
        """Build system prompt for OpenAI (full structured prompt)."""
        # Extract all kwargs
        is_frontend_structured = kwargs.get("is_frontend_structured", False)
        locale = kwargs.get("locale", "ko")
        theme = kwargs.get("theme", "chat")
        timing_window_str = kwargs.get("timing_window_str", "")
        current_date_str = kwargs.get("current_date_str", "")
        rag_context = kwargs.get("rag_context", "")
        cross_rules = kwargs.get("cross_rules", "")
        saju_detail = kwargs.get("saju_detail", "")
        astro_detail = kwargs.get("astro_detail", "")
        advanced_astro_detail = kwargs.get("advanced_astro_detail", "")
        cross_section = kwargs.get("cross_section", "")
        user_context_section = kwargs.get("user_context_section", "")
        cv_section = kwargs.get("cv_section", "")
        lifespan_section = kwargs.get("lifespan_section", "")
        theme_fusion_section = kwargs.get("theme_fusion_section", "")
        imagination_section = kwargs.get("imagination_section", "")
        crisis_context_section = kwargs.get("crisis_context_section", "")
        therapeutic_section = kwargs.get("therapeutic_section", "")

        # FRONTEND STRUCTURED PROMPT - Use simplified backend system prompt
        if is_frontend_structured:
            # Build RAG-only enrichment section
            rag_enrichment_parts = []

            # 1. Cross-analysis rules
            if cross_rules:
                rag_enrichment_parts.append(f"[ğŸ”— ì‚¬ì£¼+ì ì„± êµì°¨ í•´ì„ ê·œì¹™]\n{cross_rules[:1500]}")

            # 2. Jung/Stoic quotes from RAG
            if rag_context:
                rag_enrichment_parts.append(rag_context)

            # 3. Lifespan guidance
            if lifespan_section:
                rag_enrichment_parts.append(lifespan_section)

            # 4. Theme fusion rules
            if theme_fusion_section:
                rag_enrichment_parts.append(theme_fusion_section)

            # 5. Therapeutic guidance
            if therapeutic_section:
                rag_enrichment_parts.append(therapeutic_section)

            # 6. Crisis context
            if crisis_context_section:
                rag_enrichment_parts.append(crisis_context_section)

            # 7. User context
            if user_context_section:
                rag_enrichment_parts.append(user_context_section)

            # 8. CV section
            if cv_section:
                rag_enrichment_parts.append(cv_section)

            rag_enrichment = "\n\n".join(rag_enrichment_parts) if rag_enrichment_parts else ""

            # Simplified system prompt for frontend-structured requests
            if locale == "en":
                system_prompt = f"""You are a Saju+Astrology integrated counselor. Speak naturally and weave the data into your sentences. Start the first sentence directly with an answer (e.g., "So," or "Right now,").

ABSOLUTELY AVOID:
- Formal greetings ("Hello", "Nice to meet you")
- Self-introductions
- Bullet lists or numbered lists
- Bold text

STYLE:
- Conversational and warm, but concise
- Use 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- End with exactly one follow-up question

EVIDENCE REQUIRED (inline, not as a list):
- At least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- At least one Astrology reference (Sun/Moon/ASC plus a planet+house if possible)
- Give 2-3 timing windows within 6 months using month+week phrasing, and include one caution point
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.

{timing_window_str}

Additional knowledge:
{rag_enrichment if rag_enrichment else "(none)"}

Response length: 400-600 words, {locale}, natural spoken tone."""
            else:
                system_prompt = f"""ì‚¬ì£¼+ì ì„± í†µí•© ìƒë‹´ì‚¬. ì¹œêµ¬ì—ê²Œ ë§í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ, ë°ì´í„°ë¥¼ ë…¹ì—¬ì„œ í•´ì„í•´. ì²« ë¬¸ì¥ì€ 'ì´ì•¼'ë¡œ ì‹œì‘í•´(ë§ì¤„ì„í‘œ ê°€ëŠ¥).

ğŸš« ì ˆëŒ€ ê¸ˆì§€:
- "ì¼ê°„ì´ Xì…ë‹ˆë‹¤" ë‚˜ì—´ì‹ ì„¤ëª… (ì‚¬ìš©ìëŠ” ì´ë¯¸ ìê¸° ì°¨íŠ¸ ì•Œê³  ìˆìŒ)
- "ì•ˆë…•í•˜ì„¸ìš”" ì¸ì‚¬
- "ì¡°ì‹¬í•˜ì„¸ìš”" "ì¢‹ì•„ì§ˆ ê±°ì˜ˆìš”" ëœ¬êµ¬ë¦„ ë§
- **ë³¼ë“œì²´**, ë²ˆí˜¸ ë§¤ê¸°ê¸°, ëª©ë¡ ë‚˜ì—´

âœ… ì˜¬ë°”ë¥¸ ìŠ¤íƒ€ì¼:
- ì¹´í˜ì—ì„œ ì¹œêµ¬í•œí…Œ ì–˜ê¸°í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ
- ë°ì´í„°ë¥¼ ë¬¸ì¥ ì†ì— ë…¹ì—¬ì„œ (ë‚˜ì—´ X)
- ì‹¤ìƒí™œê³¼ ì—°ê²°í•´ì„œ ì„¤ëª…
- í•´ìš”ì²´ë¡œ ì¹œê·¼í•˜ê²Œ (ë„ˆë¬´ ë”±ë”±í•œ ë¬¸ì–´ì²´ ê¸ˆì§€)
- ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  ë‹¤ì •í•˜ê²Œ, ë‹¨ì • ëŒ€ì‹  '~ê°™ì•„/ê°€ëŠ¥ì„±' í‘œí˜„ ì‚¬ìš©
- ë¬¸ë‹¨ 3ê°œ ë‚´ì™¸ (í•µì‹¬ ìš”ì•½ â†’ ê·¼ê±°/íŒ¨í„´ â†’ íƒ€ì´ë°/í–‰ë™ + ì§ˆë¬¸)

âœ… ê·¼ê±° í•„ìˆ˜:
- ì‚¬ì£¼ ê·¼ê±° 1ê°œ ì´ìƒ(ì¼ê°„/ëŒ€ìš´/ì„¸ìš´ ì¤‘ 1ê°œ) + ì˜¤í–‰/ì‹­ì„± ë°˜ë“œì‹œ ì–¸ê¸‰
- ì ì„± ê·¼ê±° 1ê°œ ì´ìƒ(íƒœì–‘/ë‹¬/ASC ì¤‘ 1ê°œ) + í–‰ì„±/í•˜ìš°ìŠ¤ ë°˜ë“œì‹œ ì–¸ê¸‰(ê°€ëŠ¥í•˜ë©´ ê° 1ê°œ)
- ê·¼ê±°ëŠ” ë¬¸ì¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ (ë‚˜ì—´ ê¸ˆì§€)
- 6ê°œì›” íƒ€ì´ë° 2~3ê°œë¥¼ ì›”+ì£¼ ë‹¨ìœ„ë¡œ ì œì‹œ(ì˜ˆ: 3ì›” 2~3ì£¼ì°¨)
- íƒ€ì´ë° ì¤‘ 1ê°œëŠ” ì£¼ì˜ì /í”¼í•´ì•¼ í•  í¬ì¸íŠ¸ í¬í•¨
- í…Œë§ˆ ê³ ì •: theme="{theme}"ë§Œ ë‹¤ë£¨ê³  ë‹¤ë¥¸ í…Œë§ˆë¡œ íë¥´ì§€ ë§ ê²ƒ.
- ë§ˆì§€ë§‰ì— í›„ì† ì§ˆë¬¸ 1ê°œ

ğŸ“… {timing_window_str}

ğŸ“š ì¶”ê°€ ì§€ì‹:
{rag_enrichment if rag_enrichment else "(ì—†ìŒ)"}

ğŸ“Œ 500-800ì, {locale}, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´"""

            logger.info(f"[StreamingService] Using SIMPLIFIED system prompt for frontend-structured request (RAG enrichment: {len(rag_enrichment)} chars)")

        else:
            # LEGACY PATH - Build full system prompt (for non-frontend requests)
            counselor_persona = """ë‹¹ì‹ ì€ ì‚¬ì£¼+ì ì„±ìˆ  í†µí•© ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

âš ï¸ ì ˆëŒ€ ê·œì¹™:
1. ì¸ì‚¬ ê¸ˆì§€ - "ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°€ì›Œìš”" ë“± ì¸ì‚¬ ì ˆëŒ€ ê¸ˆì§€
2. ì‹ ìƒ ì†Œê°œ ê¸ˆì§€ - "ì¼ê°„ì´ Xì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ Y ì„±í–¥" ê°™ì€ ê¸°ë³¸ ì„¤ëª… ê¸ˆì§€. ì‚¬ìš©ìëŠ” ì´ë¯¸ ìê¸° ì‚¬ì£¼ë¥¼ ì•ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸ì— ë‹µí•´.
3. ì œê³µëœ ë°ì´í„°ë§Œ ì‚¬ìš© - ëŒ€ìš´/ì„¸ìš´ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ì•„ë˜ [ì‚¬ì£¼ ë¶„ì„]ì— ìˆëŠ” ê·¸ëŒ€ë¡œë§Œ ì¸ìš©
4. ì²« ë¬¸ì¥ë¶€í„° ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì‹œì‘

ğŸ’¬ ìƒë‹´ ìŠ¤íƒ€ì¼:
â€¢ ìƒì„¸í•˜ê³  ê¹Šì´ ìˆëŠ” ë¶„ì„ (400-600ë‹¨ì–´)
â€¢ ì‚¬ì£¼ì™€ ì ì„±ìˆ  ê· í˜•ìˆê²Œ í™œìš©í•˜ë˜ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´
â€¢ êµ¬ì²´ì  ë‚ ì§œ/ì‹œê¸° ì œì‹œ
â€¢ 'ì™œ ê·¸ëŸ°ì§€' ì´ìœ ë¥¼ ì¶©ë¶„íˆ ì„¤ëª…
â€¢ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©ì´ ìˆìœ¼ë©´ í•´ì„ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ê¹Šì´ ë”í•˜ê¸°"""

            if locale == "en":
                counselor_persona = """You are an integrated Saju + Astrology counselor.

ABSOLUTE RULES:
1. No greetings or self-introductions.
2. Answer the user's question from the first sentence.
3. Use only provided data; do not invent ìš´ or placements.

STYLE:
- 3 short paragraphs (summary -> evidence/patterns -> timing/action + question)
- Provide concrete timing windows within 6 months, including one caution
- Keep the tone warm and practical
"""

            # Build advanced astrology section
            advanced_astro_section = ""
            if advanced_astro_detail:
                advanced_astro_section = f"""

[ğŸ”­ ì‹¬ì¸µ ì ì„± ë¶„ì„]
{advanced_astro_detail}
"""

            if locale == "en":
                system_prompt = f"""{counselor_persona}

{timing_window_str}

[SAJU ANALYSIS]
{saju_detail}

[ASTROLOGY ANALYSIS]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[RESPONSE RULES]
- Include at least one Saju reference (day master / ten gods / five elements / daeun or annual fortune)
- Include at least one Astrology reference (Sun/Moon/ASC + planet+house if possible)
- 2-3 timing windows within 6 months (month+week phrasing), include one caution
- End with exactly one follow-up question
- Theme lock: focus strictly on theme="{theme}". Do not drift to other domains.
- Respond in English only
"""
            else:
                system_prompt = f"""{counselor_persona}

âš ï¸ {current_date_str} - ê³¼ê±° ë‚ ì§œë¥¼ ë¯¸ë˜ì²˜ëŸ¼ ë§í•˜ì§€ ë§ˆì„¸ìš”
âš ï¸ {timing_window_str} - ì´ ë²”ìœ„ ì•ˆì—ì„œ 2~3ê°œ ì‹œê¸°ë¥¼ ì œì‹œí•˜ì„¸ìš”

[ğŸ“Š ì‚¬ì£¼ ë¶„ì„]
{saju_detail}

[ğŸŒŸ ì ì„± ë¶„ì„]
{astro_detail}
{advanced_astro_section}{cross_section}
{rag_context}
{user_context_section}{cv_section}{lifespan_section}{theme_fusion_section}{imagination_section}{crisis_context_section}{therapeutic_section}

[ğŸ¯ ì‘ë‹µ ìŠ¤íƒ€ì¼]
â€¢ ì²« ë¬¸ì¥ë¶€í„° ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€ - ì‹ ìƒ ì†Œê°œ NO
â€¢ ì‚¬ì£¼ì™€ ì ì„±ìˆ  í†µì°°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì„¤ëª…
â€¢ 'ì™œ ê·¸ëŸ°ì§€' ì´ìœ ë¥¼ ìƒì„¸íˆ í’€ì–´ì„œ ì„¤ëª…
â€¢ êµ¬ì²´ì ì¸ ë‚ ì§œ/ì‹œê¸° ë°˜ë“œì‹œ í¬í•¨
â€¢ ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì–¸ ì œê³µ
â€¢ ìœµ ì‹¬ë¦¬í•™ ì¸ìš©ì´ ìˆìœ¼ë©´ 1-2ë¬¸ì¥ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš© (ë”±ë”±í•˜ê²Œ ì¸ìš© X)

âŒ ì ˆëŒ€ ê¸ˆì§€:
â€¢ ì¸ì‚¬/í™˜ì˜ ë©˜íŠ¸ ("ì•ˆë…•í•˜ì„¸ìš”", "ë‹¤ì‹œ ì°¾ì•„ì£¼ì…¨ë„¤ìš”")
â€¢ ì‹ ìƒ ì†Œê°œ ("ì¼ê°„ì´ Xì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ Y ì„±í–¥" ë“±)
â€¢ ëŒ€ìš´/ì„¸ìš´ ì§€ì–´ë‚´ê¸° (ìœ„ ë°ì´í„°ì— ì—†ëŠ” ê²ƒ ì–¸ê¸‰)
â€¢ ì¶”ìƒì  ë§ë§Œ ë‚˜ì—´ (êµ¬ì²´ì  ì‹œê¸° ì—†ì´)
â€¢ í”¼ìƒì ì´ê³  ì§§ì€ ë‹µë³€

ğŸ“Œ ì‘ë‹µ ê¸¸ì´: 400-600ë‹¨ì–´ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ({locale})"""

        return system_prompt

    def _build_emotion_context(self, prompt: str) -> str:
        """Build emotion tracking context."""
        emotion_context = ""
        if not prompt:
            return emotion_context

        prompt_lower = prompt.lower()

        # Detect emotional indicators
        emotions_detected = []
        if any(k in prompt_lower for k in ["í˜ë“¤", "ì§€ì³", "í”¼ê³¤", "ì§€ì¹¨"]):
            emotions_detected.append("exhausted")
        if any(k in prompt_lower for k in ["ìš°ìš¸", "ìŠ¬í¼", "ëˆˆë¬¼", "ìš¸ê³ "]):
            emotions_detected.append("sad")
        if any(k in prompt_lower for k in ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤", "ë¬´ì„œ"]):
            emotions_detected.append("anxious")
        if any(k in prompt_lower for k in ["í™”ë‚˜", "ì§œì¦", "ì–µìš¸", "ë¶„ë…¸"]):
            emotions_detected.append("angry")
        if any(k in prompt_lower for k in ["ì™¸ë¡œ", "í˜¼ì", "ê³ ë…"]):
            emotions_detected.append("lonely")
        if any(k in prompt_lower for k in ["ì„¤ë ˆ", "ê¸°ëŒ€", "í–‰ë³µ", "ì¢‹ì•„"]):
            emotions_detected.append("hopeful")
        if any(k in prompt_lower for k in ["í˜¼ë€", "ëª¨ë¥´ê² ", "ì–´ë–»ê²Œ", "ë­˜ í•´ì•¼"]):
            emotions_detected.append("confused")

        if emotions_detected:
            emotion_map = {
                "exhausted": "ì§€ì¹¨/í”¼ë¡œ",
                "sad": "ìŠ¬í””/ìš°ìš¸",
                "anxious": "ë¶ˆì•ˆ/ê±±ì •",
                "angry": "ë¶„ë…¸/ë‹µë‹µ",
                "lonely": "ì™¸ë¡œì›€",
                "hopeful": "í¬ë§/ì„¤ë ˜",
                "confused": "í˜¼ë€/ë°©í–¥ìƒì‹¤"
            }
            detected_ko = [emotion_map.get(e, e) for e in emotions_detected]
            emotion_context = f"\n[ğŸ’­ ê°ì§€ëœ ê°ì • ìƒíƒœ: {', '.join(detected_ko)}]\nâ†’ ì´ ê°ì •ì„ ë¨¼ì € ì¸ì •í•˜ê³  ê³µê°í•˜ì„¸ìš”. ì„±ê¸‰íˆ í•´ê²°ì±…ìœ¼ë¡œ ë„˜ì–´ê°€ì§€ ë§ˆì„¸ìš”.\n"
            logger.info(f"[StreamingService] Emotion detected: {emotions_detected}")

        return emotion_context
