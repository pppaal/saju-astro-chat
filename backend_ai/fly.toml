# fly.toml app configuration file for Backend AI
# See https://fly.io/docs/reference/configuration/ for documentation

app = "backend-ai"
primary_region = "nrt" # Tokyo - closest to Korea

[build]
  dockerfile = "Dockerfile"

[env]
  PORT = "8080"
  OMP_NUM_THREADS = "2"
  MKL_NUM_THREADS = "2"
  RAG_CPU_THREADS = "2"
  PYTHONUNBUFFERED = "1"
  APP_MODULE = "main:app"
  # Gunicorn settings optimized for AI workloads
  GUNICORN_WORKERS = "1"
  GUNICORN_THREADS = "4"
  GUNICORN_TIMEOUT = "120"
  GUNICORN_GRACEFUL_TIMEOUT = "30"
  GUNICORN_KEEPALIVE = "5"
  GUNICORN_WORKER_CLASS = "gthread"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ["app"]

  [[http_service.checks]]
    grace_period = "60s"
    interval = "15s"
    method = "GET"
    timeout = "5s"
    path = "/health"

[[services]]
  protocol = "tcp"
  internal_port = 8080

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]

  [services.concurrency]
    type = "requests"
    hard_limit = 50
    soft_limit = 40

[[vm]]
  size = "shared-cpu-2x"
  memory = "4096mb"

[deploy]
  strategy = "rolling"

[metrics]
  port = 9091
  path = "/metrics"
