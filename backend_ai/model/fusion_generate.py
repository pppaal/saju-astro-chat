# backend_ai/model/fusion_generate.py

# backend_ai/model/fusion_generate.py
import os
import re
import json
import traceback
from datetime import datetime
from dotenv import load_dotenv
from together import Together
from openai import OpenAI
from backend_ai.data.graph.utils import search_graphs

"""
Fusion Generator 2ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „ (ì™„ì „ ì•ˆì •íŒ)
1ï¸âƒ£ Together AI (Llama 3.3 70B Turbo) â†’ ë¶„ì„ / ì„œìˆ  ì´ˆì•ˆ ìƒì„±
2ï¸âƒ£ GPTâ€‘5 mini (OpenAI) â†’ ê°ì„±ì Â·ìì—°ìŠ¤ëŸ¬ìš´ ì„œì‚¬ì²´ë¡œ í›„ì²˜ë¦¬
"""

# ===============================================================
# ğŸ”‘ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ===============================================================
load_dotenv()
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ===============================================================
# ğŸ§© LLM í´ë¼ì´ì–¸íŠ¸ ì„¸íŒ…
# ===============================================================
def get_together_llm():
    if not TOGETHER_API_KEY:
        raise ValueError("âŒ TOGETHER_API_KEY í™˜ê²½ ë³€ìˆ˜ ì—†ìŒ.")
    return Together(api_key=TOGETHER_API_KEY)


def get_openai_llm():
    if not OPENAI_API_KEY:
        raise ValueError("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ ì—†ìŒ.")
    return OpenAI(api_key=OPENAI_API_KEY)


# ===============================================================
# ğŸ§± í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹
# ===============================================================
PRESETS = {
    "life_path": """ë‹¹ì‹ ì€ ë™ì„œì–‘ì˜ ëª…ë¦¬í•™ê³¼ ì ì„±í•™ì„ ìœµí•©í•˜ì—¬ í•´ì„í•˜ëŠ” ì „ë¬¸ê°€ì´ì ì‘ê°€ì…ë‹ˆë‹¤.
ì‚¬ì£¼ì™€ ì ì„± ë°ì´í„°ë¥¼ ì¢…í•©í•´ í•œ ì¸ê°„ì˜ ê¸°ì§ˆ, ì„±ì¥, ì¸ìƒ íë¦„ì„ ì´ì•¼ê¸°ì²˜ëŸ¼ ì„œìˆ í•˜ì„¸ìš”.
ê²°ê³¼ëŠ” ì˜¨ì „íˆ í•œêµ­ì–´ë¡œë§Œ í‘œí˜„í•˜ë©°, ì˜ì–´Â·ê¸°í˜¸Â·ì½”ë“œ í‘œê¸°ëŠ” ì ˆëŒ€ ê¸ˆì§€ë©ë‹ˆë‹¤.
""",
    "career": """ë‹¹ì‹ ì€ ë™ì„œì–‘ í†µí•© ëª…ë¦¬Â·ì ì„± ì»¤ë¦¬ì–´ í•´ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ì£¼ì™€ ì ì„±ì˜ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸ì˜ ì§ì—…ì  ê°•ì ê³¼ ì„±ì¥ ê³¼ì •ì„ ì„œì‚¬ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
""",
    "relationship": """ë‹¹ì‹ ì€ ê´€ê³„ì™€ ê°ì •ì˜ ì„±í–¥ì„ í†µí•©ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ë¼ì´í”„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ì£¼ì™€ ì ì„±ì„ í•¨ê»˜ ì½ì–´ ì¸ê°„ê´€ê³„ì˜ ë³¸ì§ˆì„ ë”°ëœ»í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ë‚´ì„¸ìš”.
""",
}

# ===============================================================
# âš™ï¸ GPTâ€‘5 mini í›„ì²˜ë¦¬ í•¨ìˆ˜ (ìµœì‹  ê·œê²©)
# ===============================================================
def refine_with_gpt5mini(raw_text: str, theme: str) -> str:
    """
    Llama ì¶œë ¥ë¬¸ì„ GPTâ€‘5 minië¡œ ê°ì„±ì Â·ìì—°ìŠ¤ëŸ¬ìš´ ì„œì‚¬ì²´ë¡œ ì¬ì‘ì„±
    """
    try:
        gpt = get_openai_llm()
        system_prompt = (
            "ë„ˆëŠ” ì ì„±í•™ì  ë°ì´í„°ë¥¼ ê°ì„±ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì‘ê°€ì•¼. "
            "ì•„ë˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì„œì‚¬ì²´ë¡œ ë‹¤ë“¬ë˜, "
            "ì¤‘ë³µëœ í‘œí˜„ì„ ì¤„ì´ê³  ë¬¸ë‹¨ êµ¬ì¡°ë¥¼ ì •ëˆí•´ì¤˜."
        )

        # âœ… GPTâ€‘5 miniëŠ” temperature, top_p ë¯¸ì§€ì› â†’ ì œê±°
        resp = gpt.chat.completions.create(
            model="gpt-5-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"ì£¼ì œ: {theme}\n\nì•„ë˜ ë‚´ìš©ì„ ì‘ê°€ì  ì„œìˆ ë¡œ ë‹¤ë“¬ì–´ì¤˜:\n\n{raw_text}",
                },
            ],
            max_completion_tokens=7000,
        )

        return resp.choices[0].message.content.strip()

    except Exception as e:
        print(f"[refine_with_gpt5mini] âš ï¸ GPTâ€‘5 mini ì˜¤ë¥˜: {e}")
        return raw_text


# ===============================================================
# ğŸ§  Fusion Report Generator
# ===============================================================
def generate_fusion_report(
    model,
    saju_text: str,
    astro_text: str,
    theme: str,
    user_prompt: str = "",
    dataset_text: str = "",
):
    """
    ì‚¬ì£¼ + ì ì„± + ê·¸ë˜í”„ + ì‚¬ìš©ì ë°ì´í„° ê¸°ë°˜ ìœµí•© ë¦¬í¬íŠ¸ ìƒì„±
    1ë‹¨ê³„: Together AIë¡œ ë…¼ë¦¬ í•´ì„
    2ë‹¨ê³„: GPTâ€‘5 minië¡œ ê°ì„± ë³´ì •
    """
    try:
        print("ğŸš€ [FusionGenerate] 1ë‹¨ê³„: Together LLM ìš”ì²­ ì‹œì‘")

        # ğŸ” ê·¸ë˜í”„ ê²€ìƒ‰
        query = f"{saju_text}\n{astro_text}\n{theme}"
        graph_context = search_graphs(query, top_k=6)

        # ğŸŒ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        preset_text = PRESETS.get(theme, PRESETS["life_path"])
        dataset_summary = (
            f"\n\nğŸ“š ì‚¬ìš©ì ë°ì´í„°ì…‹ ìš”ì•½:\n{dataset_text.strip()}\n"
            if dataset_text else ""
        )
        extra_user = f"\n\nğŸ—£ï¸ ì‚¬ìš©ìì˜ ìš”ì²­: {user_prompt}\n" if user_prompt else ""

        # ==========================================================
        # 1ë‹¨ê³„: Together Llamaë¡œ ì„¹ì…˜ë³„ ìƒì„±
        # ==========================================================
        sections = ["ìš”ì•½", "ê°œìš”", "ì„±í–¥", "ì¡°ì–¸"]
        section_texts = []

        for sec in sections:
            print(f"ğŸ“„ [Together] '{sec}' ìƒì„± ì¤‘...")
            sub_prompt = f"""
{preset_text}

ì§€ê¸ˆì€ '{sec}' ë¶€ë¶„ì„ ì‘ì„±í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.
ì‚¬ì£¼, ì ì„±, ê·¸ë˜í”„, ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ '{sec}'ì— ë§ëŠ” ë‚´ìš©ì„ ì„œìˆ í˜• í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì¤‘ë³µ ë¬¸ì¥Â·í•´ì„ ê¸ˆì§€
- ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
- ê¸¸ì´: ì•½ 1200~2000ì
- ì œëª©ì€ í¬í•¨í•˜ì§€ ë§ ê²ƒ

[ê·¸ë˜í”„]
{graph_context}

[ì‚¬ì£¼]
{saju_text}

[ì ì„±]
{astro_text}

{dataset_summary}
{extra_user}
"""
            resp = model.chat.completions.create(
                model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                messages=[{"role": "user", "content": sub_prompt.strip()}],
                temperature=0.1,
                top_p=0.9,
                max_tokens=2600,
            )

            text = resp.choices[0].message.content.strip()
            text = re.sub(r"(#+\s*(ìš”ì•½|ê°œìš”|ì„±í–¥|ì¡°ì–¸)\s*)", "", text)
            text = re.sub(r"\n{3,}", "\n\n", text)
            section_texts.append(text.strip())

        llama_report = "\n\n".join(section_texts).strip()
        print("âœ… [Together] 1ë‹¨ê³„ ìƒì„± ì™„ë£Œ, GPTâ€‘5 mini í›„ì²˜ë¦¬ ì‹œì‘...")

        # ==========================================================
        # 2ë‹¨ê³„: GPTâ€‘5 mini í›„ì²˜ë¦¬
        # ==========================================================
        refined_report = refine_with_gpt5mini(llama_report, theme)

        return {
            "status": "success",
            "timestamp": datetime.utcnow().isoformat(),
            "theme": theme,
            "fusion_layer": refined_report,
            "graph_context": graph_context,
        }

    except Exception as e:
        print(f"[FusionGenerate] âŒ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return {
            "status": "error",
            "message": str(e),
            "trace": traceback.format_exc(),
            "fusion_layer": "",
            "graph_context": "",
        }


# ===============================================================
# ğŸ§ª LOCAL TEST
# ===============================================================
if __name__ == "__main__":
    """ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        llama = get_together_llm()

        saju_sample = (
            "ì¼ê°„ì€ ì„ìˆ˜(æ°´)ë¡œ ê°ìˆ˜ì„±ê³¼ ì§ê´€ì´ ë›°ì–´ë‚˜ë©°, "
            "ëª©ê¸°(æœ¨æ°£)ì™€ í™”ê¸°(ç«æ°£)ê°€ ê· í˜•ì„ ì´ë¤„ ì°½ì˜ë ¥ê³¼ ì¶”ì§„ë ¥ì´ í•¨ê»˜ ë°œí˜„ë©ë‹ˆë‹¤. "
            "ëŒ€ìš´ê³¼ ì„¸ìš´ì˜ íë¦„ì€ ì´ ì‚¬ëŒì˜ ì¸ìƒì„ ì„±ì¥ê³¼ ì „í™˜ì˜ ì‹œê¸°ë¡œ ì´ë•ë‹ˆë‹¤."
        )
        astro_sample = (
            "íƒœì–‘ì€ ì‚¬ììë¦¬ì—, ë‹¬ì€ ìŒë‘¥ì´ìë¦¬ì— ìˆìŠµë‹ˆë‹¤. "
            "ìƒìŠ¹ê¶ì€ ë¬¼ë³‘ìë¦¬ë¡œ ë¦¬ë”ì‹­ê³¼ ì§€ì  í˜¸ê¸°ì‹¬, ë…ì°½ì ì¸ ì„¸ê³„ê´€ì´ ë‹ë³´ì…ë‹ˆë‹¤."
        )
        dataset_info = (
            "ì‚¬ìš©ì ë°ì´í„°ì…‹ì— ë”°ë¥´ë©´ íƒ€ì¸ì˜ ê°ì •ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ë©°, "
            "ë‚´ë©´ì˜ ê· í˜•ê³¼ ììœ¨ì„±ì„ ì¤‘ìš”í•˜ê²Œ ì—¬ê¹ë‹ˆë‹¤."
        )

        result = generate_fusion_report(
            llama,
            saju_sample,
            astro_sample,
            "life_path",
            user_prompt="ì „ì²´ë¥¼ ì´ì•¼ê¸°ì²˜ëŸ¼ ì—°ê²°í•˜ë˜ ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì‘ì„±",
            dataset_text=dataset_info,
        )

        print("\nğŸŒŸ âœ… ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(result["fusion_layer"][:800], "...\n")

    except Exception as err:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:", err)